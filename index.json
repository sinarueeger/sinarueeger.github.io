[{"authors":["admin"],"categories":null,"content":" My interests include statistical modelling, statistical genetics, programming using R and Python, reproducible analysis workflows, and topics touching on health, medicine and technology.\nDuring my PhD in Life Sciences at University of Lausanne, I worked on the imputation of GWAS summary statistics supervised by Zoltán Kutalik.\nFor me the most enjoyable aspects of applied statistics are interdisciplinary collaborations that allow a glimpse into different domains.\nIn my spare time, I enjoy exploring nature by bike and contributing to open-source projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://sinarueeger.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"My interests include statistical modelling, statistical genetics, programming using R and Python, reproducible analysis workflows, and topics touching on health, medicine and technology.\nDuring my PhD in Life Sciences at University of Lausanne, I worked on the imputation of GWAS summary statistics supervised by Zoltán Kutalik.\nFor me the most enjoyable aspects of applied statistics are interdisciplinary collaborations that allow a glimpse into different domains.\nIn my spare time, I enjoy exploring nature by bike and contributing to open-source projects.","tags":null,"title":"Sina Rüeger","type":"author"},{"authors":null,"categories":[],"content":" This blogpost is about applying a mental-wellbeing coping to constant “data analysis worrying”. From https://xkcd.com/1222/.\nLet’s say you get a new project handed over; some kind of data analysis. What do you feel:\nexcitement of getting your hands onto new data, or anxiety that this project may fail, stumbling upon all the data science mistakes that one can possibly make? I feel both of these things. I am always delighted about a. because it means that I still like my work. But b. bothers me to no end, and so I decided to do something about it. Here is my write-up of my coping approach, split into two parts.\nThis Part 1 will discuss a mental coping approach, while Part 2 will show technical solutions that are available in R.\nData analytic challenges There are tons of factors that will influence an analysis - whether controllable or not. Keeping these factors in check is often a challenge.\nTo give an example: Before actually seeing a dataset and without experience with that kind of data, we have little idea about the effort required to deliver an analysis.\nOf course, with experience, analysts will develop a knack for foreseeing some of these troublesome factors and start to master them.\nIn the example above, we may ask for a sample dataset first before giving any delivery date; ask for more background information; any analysis previously done; that the dataset be sent in a specific data format, or for the power analysis that was done before the data was generated.\nSuch challenges are also highlighted in this RStudio Conf talk by Caitlin Hudon. Here, Caitlin Hudon talked about different categories of Data Science mistakes, one of them being “Technical / Analysis”, and then goes on to give a list of potential misunderstandings and mistakes. It’s a brilliant talk and I recommend watching it and taking notes of your mistakes (I do that with a simple text file).\nNow, a little challenge is different from constant worrying. Constant worrying can come form real problems, say, “data QC will be a challenge”. But it can range up to a cognitive distortion, for example “this project will never end and be a total failure”.\nDoes experience really help? Making mistakes and developing experience takes time and may also depend on work culture. Most importantly though, your self-confidence, “worry baseline” and core beliefs play a role1.\nMy point is, that instead of relying on years and years of experience, it may be quicker to apply what I call the “worry battling technique”.\n“Worry battling technique” This is borrowed from CBT - cognitive behavioral therapy - an intervention technique used to improve mental health. CBT techniques re-direct emotions and the resulting behaviour.\nSo that the cycle of Thoughts \u0026gt; Feelings \u0026gt; Behaviour \u0026gt; Thoughts \u0026gt; … becomes less vicious.\nCBT also gives coping strategies to change cognitive distortions. You could say that worrying about the outcome of a data analysis after years of data analytic experience is a cognitive distortion.\nDe-catastrophizing For myself, I have created a mix of several existing techniques around catastrophizing and anxious thoughts (borrowed from worksheets here and here) that work for me.\nWhenever I feel anxiety creeping up, I go through the seven steps. It goes like this:\nQuestion to ask yourself Example 1: Modelling Example 2: Data QC 1) What is something you are worried about? I won’t find the right model for a given dataset. I will overlook data cleaning traps (e.g. incorrect identification of missing values) 2) What are some clues that your worry will not come true? I have a solid education and it is not possible to find “the best model” anyway - it will always be an approximation and also depends on the time I have at hand. I have modeling validation techniques in place. I dedicate lots of time to talk to subject matter experts and collect background information. I have data cleaning techniques in place. I have experienced lots of problems already - I am well prepared. Data cleaning is an iterative process; it’s ok to go back, correct the data cleaning process and rerun it. 3) Worst possible outcome Nothing fits, or the model does not converge. Forth and back, running out of time to do any actual analysis. 4) Best possible outcome Perfect model. Clean dataset. 5) Likely outcome I’ll find an ok model after some modelling iteration. I’ll overlook a data cleaning aspect, but will fix it and rerun the data QC. 6) If your worry does come true, how will you handle it? Will you eventually be okay? I stick to a simpler model and suggest that someone else looks at it with fresh eyes. I will be ok. I anticipate my worry and keep data cleaning separate from any analysis. This way I can rerun the data QC at any time again. I will be ok. Core beliefs Another exercise is around core beliefs:\nQuestions to ask yourself Example 1 Example 2 What is one of your negative core beliefs? No data analysis of mine was ever correct or has made a difference to science. I am a slow data analyst. List pieces of evidence contrary to your negative core belief. I) Several of my data analyses have made it to manuscript or pipelines, II) People often tell me that they find my work helpful. I am curious, quick to pick up new things and work hard. Why does it work? I am not a psychologist, so I don’t know.\nI guess for me it helps to know that there is a midway between the best and worst scenario, and that even if the worst happens, the world won’t fall apart.\nSummary Some people have a tendency to anxiety. Such anxiety may cause unrealistic catastrophizing of a data analysis. This in turn can create an unhealthy nervousness and simply exhaust.\nWhenever faced with a “ will go totally wrong”, I use a six step questionnaire for myself. Likewise, when I have a negative core belief creeping up, I use a two step questionnaire.\nThese sets of questions help me and I hope they help you as well.\nPart 2 In part 2 I will discuss more technical solutions, e.g. project set up, QC tooling in R or how important it is to take breaks.\nOne could also argue that this is simply impostor syndrome.↩\n","date":1619913600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619913600,"objectID":"d96330d41d414865fcee76f27d2abab4","permalink":"https://sinarueeger.github.io/post/cbt-data-analysis-part1/","publishdate":"2021-05-02T00:00:00Z","relpermalink":"/post/cbt-data-analysis-part1/","section":"post","summary":"This blogpost is about applying a mental-wellbeing coping to constant “data analysis worrying”. From https://xkcd.com/1222/.\nLet’s say you get a new project handed over; some kind of data analysis. What do you feel:\nexcitement of getting your hands onto new data, or anxiety that this project may fail, stumbling upon all the data science mistakes that one can possibly make? I feel both of these things. I am always delighted about a.","tags":["R","wellbeing","data-analysis"],"title":"Data Analysis Worrying (Part 1)","type":"post"},{"authors":null,"categories":[],"content":" I attended the last rOpenSci call on data repositories1 (recording) and below are my two cents on why storing on, and using data from the web may still be such a headache.\nThe hour-long community call was - as always - inspiring and presented itself in a relaxed setting; with a set of experts voicing their experiences and challenges around data repositories from different angles, and a Q\u0026amp;A session towards the end. It was, however, over way too quickly and when I left the Zoom call I still had these loose ends in my head.\nI am by no means a data repository expert, but I have faced challenges in both using data repositories and storing data in repositories while working as a genomic data scientist.\nThe big picture Purpose The advantages of open science have been discussed elsewhere2.\nThree reasons why researchers need (to share) data come to my mind:\nScientific findings; to do research we need data. But even if we have collected and analysed our own data, we often need external data to replicate our results. Reproducibility; we may want to rerun results shown in a paper (i.e. check reproducibility of published results). Standards; repositories may also guarantee certain data storage standards. If we don’t have governed data repositories, everyone will develop their own standards. Options The scientific community has already access to a wide range of data repositories out there: Dryad, Zenodo and figshare. They are fairly easy to use, integrate well with other tools and have free plans.\nThe enigma The question is then:\nThere are these great (and free!) data repository solutions out there AND we clearly need them - how come they are not used more often? The experts The experts on the call were the following3:\nDaniella Lowenberg, representing Dryad Matt Jones, representing dataONE Kara Woo, representing Sage Bionetworks and guiding through the call Carl Boettiger, a researcher Karthik Ram, a researcher Culture change \u0026amp; storage as an afterthought I work in academic research, and two comments resonated most with the problems I am facing, hence I will focus on those:\nKarthik Ram made the point that although there is infrastructure to host code, data, and computing, a) they lag behind in user experience and b) users lack incentives to share their data. Carl Boettiger mentioned that analysis and storage are still too far apart, with public data storage being a mere afterthought. Let’s discuss these two points in turn.\nWhat’s needed for culture change? To illustrate where data repositories lack compared to code and computing, Karthik Ram showed the pyramid for culture change introduced by Brian Nosek (shown as an annotated screenshot below).\nOn the left hand side (A) are the culture change mechanism (along with their incentives B):\nInfrastructure UI/UX Communities Incentives Policy On the right-hand side are the three pillars of (computational) research: data, code and the computing environment.\nGreen highlighting demonstrates where the mechanism has been accomplished.\nAll three pillars have the necessary infrastructure, which is the basis of the pyramid of culture change.\nCode is almost fully accomplished, with only policy left out (very few journals require code to be supplied).\nComputing environment can be shared, at times easily, e.g. through docker containers, but it lacks a community, incentives and policy enforcements.\nNow data HAS actual policies. Often journals require data alongside the publication, but for a proper culture change, we would need UI/UX, communities and incentives4 to be present.\ngit as a model Karthik Ram mentioned also that git wasn’t always as widely used in research, but now it is. Probably mostly thanks to infrastructures such as GitHub.\nSo hopefully data repositories will eventually become as popular as the use of git!\nData repositories as an afterthought The second comment that got me thinking was by Carl Boettiger: the discrepancy between the daily workflow and publishing the work.\nPublished data often comes at the end of a lengthy research project, when this is actually quite counter-intuitive (why being at the end when it’s the foundation of a project?).\nWhat’s meant by this? First we collect the data, analyse it using scripts, we get results (which are again data), maybe we write a paper (all displayed on the LHS). After all of that we then think about getting the data (input + results + code) onto a data repository (RHS) - because we remember that this is what we should do.\nFrom Karthik Ram we have learned that some mechanisms for an easy use of data repositories are missing, but that we do have infrastructure to do it and policies that demand it (hence the last-minute panic).\nCarl Boettiger also made the point that both - the daily workflow and published data - want similar things5, but that the tools to achieve them are different. In daily workflows we use R, git and data buckets; while published data are dealt with REST APIs and other tools.\nComputing environments that can access code and data at the same time Might the best solution not be to store data + code + computing environment all together somewhere publicly from the very beginning of an analysis?6\nThis has been proposed by binder, and wrapped into the R package holepunch by Karthik Ram.\nA similar solution is to use a cloud computing service combined with a pipelining tool (e.g. cromwell or nextflow). I started to use this setting over a year ago and find it so far the best approach - if the budget allows (cloud computing environments come at a price). I do wonder though, if universities will continue to invest in computing clusters or shift to cloud computing with managed access.\nThe tricky bits about data \u0026amp; privacy Let’s now put this all into context with real work and real data: I work mostly with human genomic and health (records) data, with sample sizes ranging from a couple of hundred to hundreds of thousands.\nEmbarrassingly, it was only in 2018 when I first heard about Zenodo. The reason is, I believe, that free sharing of human data is rather complicated (and in fact prohibited in many situations), as privacy laws apply (Kara Woo mentioned this too). Additionally, as an analyst I am not involved with data collection and therefore other entities decide on whether to store the data in a repository (e.g. the use of dbGAP + EBI) or keep it elsewhere.\nTo illustrate the extreme measures taken in terms of privacy; the 100,000 Genomes Project from Genomics England has a data center, where remote desktop computers are provided (it used to be a physical data center). And only a selected group of people can access the data.\nWhat can often be shared though is summarized data, called summary statistics. For genome-wide association studies (GWAS) there is an organisation called GWAS catalog, which provides a domain specific data repository for summary statistics only. They do a good job in making it easy to upload summary statistics; they standardize it, they educate people - and yet it may take a while till summary statistics show up there and lots of published summary statistic data are on university servers, google drives or websites (using different data storage standards - which causes lots of extra work for people that want to use the data).\nSome ideas 1. Think carefully ahead of time about incentives for analysts What’s worse? Going through the process of depositing the data in a repository (which means harmonizing the data, adding a data dictionary, sorting out permission, etc.) or dealing with the cumbersome process of finding data again after years of not working on it?\n2. Depositing the data online when starting to analyse it This results in an additional incentive: ensures backup of original base data (if you protect it from write access).\n3. Learning from studying other peoples work Initiatives such as reprohack make participants aware of necessary standards and subsequently enables them to lead by example.\n4. Use a hash in file names for identifiability of files Proposed by Carl Boettiger. This would resolve a notorious problem in my setup. For example, using a standard filename, say dat.txt, may result in overwriting the file with different data, but the same filename. This won’t happen with hashes. As a downside they may be a bit clunky to use.\nYes, it did take me two months to write this up…↩\nE.g. https://okfn.org/opendata/why-open-data/ or https://www.nature.com/articles/nchem.1149↩\nThey have diverse affiliations, but I added the one that seemed to me most fitting in the context of their presentation.↩\nI think that there are incentives for storing data in repositories - if people only knew about them! For example I’d say that looking up your own work is much easier with a properly organised online archive - instead of fumbling with outdated server logins and looking through backups.↩\nAs mentioned above, “find previous work” is one of my prime incentives to use data storage.↩\nThis was also asked by one of the participants; Michael Summner asked if combining code and data in a public computing environment would solve the problem.↩\n","date":1614470400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614470400,"objectID":"ceed0bea7e0607d52b5c73ddb2ae8df5","permalink":"https://sinarueeger.github.io/post/the-headache-with-data-repositories/","publishdate":"2021-02-28T00:00:00Z","relpermalink":"/post/the-headache-with-data-repositories/","section":"post","summary":"I attended the last rOpenSci call on data repositories1 (recording) and below are my two cents on why storing on, and using data from the web may still be such a headache.\nThe hour-long community call was - as always - inspiring and presented itself in a relaxed setting; with a set of experts voicing their experiences and challenges around data repositories from different angles, and a Q\u0026amp;A session towards the end.","tags":["reproducibility","R","rOpenSci","data"],"title":"The headache with data repositories","type":"post"},{"authors":["**R\u0026uuml;eger, S**","Hammer, C","Loetscher, A","McLaren, PJ","Lawless, D","Naret, O","Depledge, DP","Morfopoulou, S","Breuer, J","Zdobnov, E","Fellay, J","the Swiss HIV Cohort Study"],"categories":null,"content":"","date":1614211200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1614211200,"objectID":"6cdeb66130401cdf70f485a13b8be1c0","permalink":"https://sinarueeger.github.io/publication/scientific-reports-2021-ebv-g2g/","publishdate":"2021-02-25T00:00:00Z","relpermalink":"/publication/scientific-reports-2021-ebv-g2g/","section":"publication","summary":"","tags":[""],"title":"The influence of human genetic variation on Epstein–Barr virus sequence diversity","type":"publication"},{"authors":null,"categories":null,"content":"Listening to a podcast keeps me focussed like nothing else, hence perfect for reflection and learning. Usually, I do some unsophisticated knitting next to it or I plug in my earphones while walking to work.\nHere is a list of my preferred podcasts.\nR \u0026amp; stats \u0026amp; data science Podcasts that touch on R, statistics or data science:\nNot So Standard Deviations: Entertaining chit chat - uplifting and enlightening. See below why this is my favorite.\nCredibly Curious by Saskia Freytag and Nicholas Tierney is all about R: They manage to get these technical bits into a podcast!\nThe Corresponding Author by Stephanie Hicks and John Muschelli: About academic data science. The podcast I\u0026rsquo;d wish I\u0026rsquo;d had during my PhD.\nMy absolute favorite is Not So Standard Deviations by Hilary Parker and Roger Peng. Both born to talk. They can turn something seemingly banal into an exciting topic. Every time you think \u0026ldquo;now they will run out of topics\u0026rdquo; they come up with an existential question. Mostly, their hour-long by-weekly episodes feel like you are listening in to a conversation between them. Their coffee discussions are in a way pointless but make 100% sense to me. Through their podcast, I got to know one of my other favorite podcasts (Sticky Notes). The best thing - they giggle so much that if you feel a little down, you will be cheered up in no time!\nThere are a bunch of other podcasts I have not started listening to but should:\nCasual inference by Ellie Murray and Lucy D\u0026rsquo;Agostino McGowan. R podcast by Eric Nantz. Tidy Tuesday Podcast by Jon Harmon. See also this list for a comprehensive overview. There are tons of analytics podcasts out there. Best is to listen in and see if you like them.\nScience podcasts Most of us data scientists or statisticians have some domain we work in. Mines are health and academia. So here are the podcasts that touch on that:\nMosaic Science Podcast from the Wellcome Trust (basically articles about their research read by someone). Everything Hertz by Dan Quintana and James Heathers (I particularly like their interviews, loved this one with Kristin Sainani). I used to listen to Science Disrupt too.\nWork-related podcast The Broad Experience by Ashley Milne-Tyte. Aiming at working women, but many topics apply to anyone. My favorite: Episodes 128 and 129 on Negotiation.\nPodcast wishlist I am still waiting that someone starts a podcast about statistical genetics or genetic epidemiology. Nothing hard-core, just diving into methods.\nThanks to the podcasters for the entertainment and Maëlle Salmon for the gentle push to write up this blog post.\n","date":1578700800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1578700800,"objectID":"96ccd782ae519631ec69f70b123733e2","permalink":"https://sinarueeger.github.io/post/podcasts/","publishdate":"2020-01-11T00:00:00Z","relpermalink":"/post/podcasts/","section":"post","summary":"Listening to a podcast keeps me focussed like nothing else, hence perfect for reflection and learning. Usually, I do some unsophisticated knitting next to it or I plug in my earphones while walking to work.\nHere is a list of my preferred podcasts.\nR \u0026amp; stats \u0026amp; data science Podcasts that touch on R, statistics or data science:\nNot So Standard Deviations: Entertaining chit chat - uplifting and enlightening. See below why this is my favorite.","tags":["podcast","R","science communication"],"title":"Favourite Podcasts","type":"post"},{"authors":[],"categories":null,"content":" ","date":1567692000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567692000,"objectID":"9a7dc40750fb0851cdfa1304bab0c88d","permalink":"https://sinarueeger.github.io/talk/robust-data-analysis/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/robust-data-analysis/","section":"talk","summary":" ","tags":["best practices","R"],"title":"Robust data analysis - an introduction to R","type":"talk"},{"authors":null,"categories":null,"content":"This is the course material to the workshop Robust data analysis: an introduction to R held at the Open Science in Practice Summer School 2019 at EPFL, Switzerland, and taught with the help of Allie Burns.\nHere are the slides and the workshop handbook.\nI drew inspiration for the workshop from the following material:\nLet them have cake first by Mine CetinkayaRundel. Ten quick tips for teaching programming by Neil Brown and Greg Wilson. The beautiful illustrations by Allison Horst. Keynote at #useR2019 + mindset by Julia Stewart Lowndes. ","date":1567555200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567555200,"objectID":"ef1a3aedaf0659e9447e76e403cdcecf","permalink":"https://sinarueeger.github.io/project/robust-data-analysis/","publishdate":"2019-09-04T00:00:00Z","relpermalink":"/project/robust-data-analysis/","section":"project","summary":"Course material for R introduction workshop held at [Open science in practice](http://osip2019.epfl.ch/) summer school.","tags":["R"],"title":"Robust data analysis - an introduction to R","type":"project"},{"authors":null,"categories":null,"content":" The 3+ days at useR!2019 in Toulouse were packed with great talks1 and good food - hence the amuse-bouches word play.\nHere are some R code bits from the conference. Hopefully convincing enough to start using a new package or change a workflow. Not everything was brand-new, but it was helpful to have someone talking through their inspiration and examples.\nCheck out the speakers’ materials - soon there will be recordings too. Some of the examples are also copied straight from the speakers’ slide decks.\nTidy eval usethis pak Reshaping data vroom data.table rray 1. tidy eval Speaker: Lionel Henry (Slides)\nI never warmed up to the bang-bangs and enquo’s. Hence the new and more straight forward {{ }} (read: curly curly) for functional programming {{ arg }} in the tidyverse feel like a game-changer.\nFor those more familiar with the previous framework: {{ arg }} is a shortcut for !!enquo(arg).\ndplyr example Let’s say you have a dataset, here iris, and you want to compute the average Petal.Length for each Species:\nlibrary(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union iris %\u0026gt;% group_by(Species) %\u0026gt;% summarise(avg = mean(Petal.Length, na.rm = TRUE)) ## # A tibble: 3 x 2 ## Species avg ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55 You can use the “curly-curly” brackets if you want to turn this small bit of code into a function group_mean() with a data, by and var argument2 (and want to pass the variables on in an unquoted way):\ngroup_mean \u0026lt;- function(data, by, var) { data %\u0026gt;% group_by({{ by }}) %\u0026gt;% summarise(avg = mean({{ var }}, na.rm = TRUE)) } We can then apply group_mean() to any dataset that has a grouping and a continuous variable, for example, the mammals sleep dataset in ggplot2:\nlibrary(ggplot2) group_mean(data = msleep, by = vore, var = sleep_total) ggplot2 example Another common tidy eval application is ggplot2. In the example below, we want a customised plot: a scatterplot with a geom_smooth on top of it.\nlibrary(ggplot2) theme_set(theme_bw()) ggplot(data = iris, aes(x = Sepal.Length, y = Petal.Length, group = Species, color = Species) ) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) + ggtitle(\u0026quot;Pack this plot into a function.\u0026quot;) Again, we can wrap the “curly-curly” brackets around the arguments and apply them to a different dataset.\nplot_point_smooth \u0026lt;- function(data, x, y, gr = NULL, method = \u0026quot;lm\u0026quot;) { ggplot(data = data, aes({{ x }}, {{ y }}, group = {{ gr }}, color = {{ gr }}) ) + geom_point() + geom_smooth(method = method) } plot_point_smooth(msleep, x = sleep_total, y = sleep_rem, gr = NULL) + ggtitle(\u0026quot;Tidy eval with the msleep dataset\u0026quot;) 2. usethis Speaker: Jenny Bryan (Slides + Material + Demo)\n# install.packages(\u0026quot;usethis\u0026quot;) library(usethis) Once upon a time, there was the package devtools. Then devtools became too large, and now the usethis package is taking over some of the convenience functions for workflows.\nusethis is all about avoiding to copy+pasting. For example, there is a function to edit the .Rprofile called usethis::edit_r_profile(). Whenever there is a slightly complicated task ahead (say restarting R), the usethis package will talk you through the whole process.\nThere are lots of use_* to add or modify something to/in a project/package and three functions to create a package, a project or a github fork:\ncreate_package() create_project() create_from_github() Create a package If you want to create a package, do the following (see also screencast):\n## 1. create the package skeleton create_package(\u0026quot;~/tmp/mypackage\u0026quot;) ## 2. use git use_git() ## 3. add a license use_mit_license() ## 4. run check # install.packages(\u0026quot;devtools\u0026quot;) devtools::check() ## 5. commit all files with git ## 6. set up git + github use_github() ## will update the DESCRIPTION file ## 7. install the package devtools::install() ## 8. add a rmarkdown readme file use_readme_rmd() ## knit + commit + push ## 9. clean up if this was only a demo ## install.packages(\u0026quot;fs\u0026quot;) ## fs::dir_delete(\u0026quot;~/tmp/mypackage\u0026quot;) 3. pak Speaker: Gábor Csárdi (Slides)\n# install.packages(\u0026quot;pak\u0026quot;) ## or # devtools::install_github(\u0026quot;r-lib/pak\u0026quot;) It seems like pak will make package installation - conventional and for projects - more intuitive. Before installing anything, pak will give you a heads up on what will be installed or if there are any conflicts.\npak has two main functions: pak::pkg_* and pak:::proj_*\nConventional package installation Play around with usethis3 and see what happens:\npak::pkg_install(\u0026quot;usethis\u0026quot;) pak::pkg_remove(\u0026quot;usethis\u0026quot;) pak::pkg_install(\u0026quot;r-lib/usethis\u0026quot;) pak::pkg_status(\u0026quot;usethis\u0026quot;) Package installation for projects First, create a project with usethis, then install R packages directly into the project.\nusethis::create_project(\u0026quot;~/tmp/test\u0026quot;) ## check the directory dir() ## initialise a dedicated R packages folder pak:::proj_create() ## check the directory again dir() ## check the DESCRIPTION file readLines(\u0026quot;DESCRIPTION\u0026quot;) ## install usethis pak:::proj_install(\u0026quot;usethis\u0026quot;) ## this installs dependencies into a private project library readLines(\u0026quot;DESCRIPTION\u0026quot;) ## remove the project folder again fs::dir_delete(\u0026quot;~/tmp/test\u0026quot;) 4. Reshaping data Speaker: Hadley Wickham (Demo)\n# install.packages(\u0026quot;tidyverse/tidyr\u0026quot;) library(tidyr) What a history reshaping data in R already has! From reshape to melt + cast, over to gather + spread and now pivot_long + pivot_wide. Reshaping data stays a mind-bending task, but hopefully, these pivot_* functions will make life easier.\n# devtools::install_github(\u0026quot;chrk623/dataAnim\u0026quot;) # Master\u0026#39;s Thesis project by Charco Hui library(dataAnim) ## Our two toy datasets datoy_wide ## Name English Maths ## 1 Ben 19.0 58.5 ## 2 Sam 6.7 51.8 ## 3 Sarah 14.9 45.1 datoy_long ## Name Subject Score ## 1 Ben Maths 10.0 ## 2 Ben English 63.7 ## 3 Sam Maths 52.9 ## 4 Sam English 75.6 ## 5 Alex Maths 88.8 ## 6 Alex English 92.2 Let’s reshape the datasets4:\n## lets make it longer datoy_wide %\u0026gt;% pivot_longer(-Name, names_to = \u0026quot;Subject\u0026quot;, values_to = \u0026quot;Score\u0026quot;) ## lets make it wider datoy_long %\u0026gt;% dplyr::mutate(Time = 1:nrow(datoy_long)) %\u0026gt;% pivot_wider(names_from = \u0026quot;Subject\u0026quot;, values_from = c(\u0026quot;Score\u0026quot;, \u0026quot;Time\u0026quot;)) 5. vroom Speaker: Jim Hester (Slides + Screencast)\n## install.packages(\u0026quot;vroom\u0026quot;) library(vroom) Importing large datasets into R can be a painful task. Especially if you only need a subset of the columns. And apparently, our thoughts drift off after 10 sec5 staring at the screen where it is still loading the dataset.\ndata.table::fread() is always here to help. But now comes vroom!\nGet some large’ish data First, we need some large dataset. To not burden our laptops too much6, we will go for some exome based GWAS results.\n## Source: https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files path_to_file_1 \u0026lt;- \u0026quot;Height_AA_add_SV.txt.gz\u0026quot; path_to_file_2 \u0026lt;- \u0026quot;BMI_African_American.fmt.gzip\u0026quot; ## Height download.file( \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/8/80/Height_AA_add_SV.txt.gz\u0026quot;, path_to_file_1) ## BMI download.file( \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/3/33/BMI_African_American.fmt.gzip\u0026quot;, path_to_file_2) ## File size ## install.packages(\u0026quot;fs\u0026quot;) fs::file_size(path_to_file_1) ## 4.39M fs::file_size(path_to_file_2) ## 13.3M The two datasets have a mix of characters, numbers and decimals7.\nvroom vs DT Here is how vroom works and a basic comparison to data.table::fread (I let you do the proper benchmarking yourself).\nlibrary(dplyr) ## With vroom giant_vroom \u0026lt;- vroom::vroom(path_to_file_1) giant_vroom_subset \u0026lt;- giant_vroom %\u0026gt;% select(CHR, POS) %\u0026gt;% filter(CHR == 1) ## The equivalent with data.table giant_DT \u0026lt;- data.table::fread(path_to_file_1) giant_DT_subset \u0026lt;- giant_DT %\u0026gt;% select(CHR, POS) %\u0026gt;% filter(CHR == 1) col_select for the win ## Selecting columns giant_vroom_select \u0026lt;- vroom::vroom(path_to_file_1, col_select = list(SNPNAME, ends_with(\u0026quot;_MAF\u0026quot;))) head(giant_vroom_select) ## Preventing columns from being imported giant_vroom_remove \u0026lt;- vroom::vroom(path_to_file_1, col_select = -ExAC_AFR_MAF) head(giant_vroom_remove) ## Renaming on the fly giant_vroom_rename \u0026lt;- vroom::vroom(path_to_file_1, col_select = list(p = Pvalue, everything())) head(giant_vroom_rename) Combining multiple datasets data_combined \u0026lt;- vroom::vroom( c(path_to_file_1, path_to_file_2), id = \u0026quot;path\u0026quot;) table(data_combined$path) 6. data.table Speaker: Arun Srinivasan (Slides)\ndata.table has a pretty cool feature8:\n# install.packages(\u0026quot;data.table\u0026quot;) library(data.table) ## Warning: package \u0026#39;data.table\u0026#39; was built under R version 3.5.2 ## ## Attaching package: \u0026#39;data.table\u0026#39; ## The following object is masked from \u0026#39;package:dataAnim\u0026#39;: ## ## := ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## between, first, last ## Create a giant data.table p \u0026lt;- 2e6 dat \u0026lt;- data.table(x = sample(1e5, p, TRUE), y = runif(p)) ## Let\u0026#39;s select a few rows system.time( tmp \u0026lt;- dat[x %in% 2000:3000 ] ) ## do the same operation again system.time( tmp \u0026lt;- dat[x %in% 2000:3000 ] ) 7. rray Speaker: Davis Vaughan (Slides)\n# devtools::install_github(\u0026quot;r-lib/rray\u0026quot;) ## may take some time rray can do two things that are otherwise annoying/counter-intuitive in R:\nbroadcasting (recycling dimensions) subsetting (bag[,1, drop = FALSE]) Matrices with base-r Let’s look at an example of matrix operations in base-r9.\nFirst, we want to add two matrices with similar dimensions:\nmat_1 \u0026lt;- matrix(c(15, 10, 8, 6, 12, 9), byrow = FALSE, nrow = 2) mat_2 \u0026lt;- matrix(c(5, 2, 3), nrow = 1) ## broadcasting won\u0026#39;t work ❌ mat_1 + mat_2 ## Error in mat_1 + mat_2: non-conformable arrays Next, we want to select one matrix column:\ndim(mat_1[,2:3]) ## selecting two columns is fine ## [1] 2 2 ## subsetting won\u0026#39;t preserve the matrix class ❌ dim(mat_1[,1]) ## why not 2x1? ## NULL length(mat_1[,1]) ## ah, it turned into a vector! ## [1] 2 dim(mat_1[,1, drop = FALSE]) ## but with drop = FALSE we can keep it a matrix ## [1] 2 1 Matrices with rray Let’s do now the same task with rray.\nlibrary(rray) (mat_1_rray \u0026lt;- rray(c(15, 10, 8, 6, 12, 9), dim = c(2, 3))) (mat_2_rray \u0026lt;- rray(c(5, 2, 3), dim = c(1, 3))) ## Broadcasting works ✓ mat_1_rray + mat_2_rray ## Subsetting works ✓ dim(mat_1_rray[,2:3]) dim(mat_1_rray[,1]) ## smart functions mat_1_rray / rray_sum(mat_1_rray, axes = 1) rray_bind(mat_1_rray, mat_2_rray, .axis = 1) rray_bind(mat_1_rray, mat_2_rray, .axis = 2) More info Program + Slides: https://user2019.r-project.org/talk_schedule/ Collection of slides during the conference by Praer (Suthira Owlarn): https://github.com/sowla/useR2019-materials Recordings by the R Consortium on Youtube (keynotes available, rest soon to be published) Last but not least The rstatsmeme package is a little gem discovered thanks to Frie Preu:\n# devtools::install_github(\u0026quot;favstats/rstatsmemes\u0026quot;) library(rstatsmemes) show_me_an_R_meme() I cannot wait to see the recordings to catch up with the parallel sessions that I missed!↩\nSee Slide 49+.↩\nSee demo 1↩\nSee demo↩\nI can totally confirm that.↩\nIf you can, choose the UKBB + GIANT meta analysis results, which are pretty large.↩\nApparently, characters are the most challenging ones for speed.↩\nSee slides 26↩\nSee also slide 5.↩\n","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"781043ae7722e0a2f862ad1035f413d3","permalink":"https://sinarueeger.github.io/post/amuse-bouches-from-user-2019/","publishdate":"2019-07-18T00:00:00Z","relpermalink":"/post/amuse-bouches-from-user-2019/","section":"post","summary":"The 3+ days at useR!2019 in Toulouse were packed with great talks1 and good food - hence the amuse-bouches word play.\nHere are some R code bits from the conference. Hopefully convincing enough to start using a new package or change a workflow. Not everything was brand-new, but it was helpful to have someone talking through their inspiration and examples.\nCheck out the speakers’ materials - soon there will be recordings too.","tags":["conference","R"],"title":"Amuse-bouches from useR!2019","type":"post"},{"authors":null,"categories":null,"content":" How to make a good poster Recipe Content Design This is a tutorial on how to make a scientific poster from scratch.\nIt aims for first-time poster makers, but also for people like me that quickly lose themselves in choosing the right font for hours.\nIt includes a few self-declared best practices that helped me in the past. Those are at best ideas, and barely opinions. It is by no means a collection of strict rules and feedback is welcome!\nI got lots of tips from Flavia Hodel during stimulating discussions around poster presentation. Flavia also gave valueable feedback after reading through a draft.\nHow to make a good poster An excellent video by Mike Morrison recently made the rounds online. If you have not done so already, watch it before continuing.\nMike Morrison makes the point that most of the posters presented at conferences are not easy to grasp.\nThere are two principal reasons for this:\nThe content is not adapted for a poster presentation. For example reusing whole paragraphs from a manuscript. Poor design choices. For example small fonts, problematic colour choices, low-resolution figures. Separating content from design is important^[If you know Markdown or LaTex, you know what I mean.] because it will allow you to focus only on one or the other. And since \u0026ldquo;design\u0026rdquo; is a profession, a better description in what we are doing is \u0026ldquo;polishing\u0026rdquo; or \u0026ldquo;improving readability\u0026rdquo;.\nI will structure my tips into these two parts: content and design, along with a recipe that I like to follow.\nBut let\u0026rsquo;s talk first about what makes a poster presentation special.\nOral versus poster presentation If you get an oral presentation (congrats!), then you know that your voice will carry most of the talk. While your slides should be carefully designed, you can still rescue imperfect slides with a good talk.\nPoster presentations are somewhat special because they are at an intersection of a talk and an art exhibition. Meaning, you will once during the conference have the opportunity to guide other researchers through your poster. This is similar to a lightning talk, just more engaging.\nThe remainder of the conference, your poster will hang there, and maybe some people will still try to understand your topic. No matter how good you are at talking, now the poster has to be self explainable.\nIn fact, your poster should read like going through stand-alone slides (slides with more explanations to replace the speakers\u0026rsquo; voice).\nBack to the video While overall the video of Mike Morrison is excellent, I struggle with three things.\nFirst, it talks about emphasising this very clear conclusion. In real life, this is often not possible. For example, because a research project has just started. So instead of having this one research question, I\u0026rsquo;d go for the main message - what do you want that people remember? That can be the large sample size of your study or that you are developing a new method.\nSecond, the poster template still crams the details of the study into a small space, which is not ideal for readability.\nThird, it talks about a horizontal^[Update: As Peter Higgins pointed out, horizontal is common in the US, vertical is common in Europe.] poster, and this is often not possible at conferences, due to space constraints^[However, a straight forward solution is, to turn the design by 90 degrees.].\nRecipe Here is a recipe that I like to follow when I make a poster.\nGet ready Start at least 7 days before the poster needs to go into print^[Make sure you know how much time the printing service needs to print your poster.].\nThink of the main message: what is the one thing that you want the reader to remember? Write that message into a text file.\nTake a pen and sketch a draft of your poster on an A4 paper. What do you want to present and where does this content go on the poster? What kind of figures and tables to you want the reader to see? No need to write proper sentences, instead use keywords and skeletons of figures and tables. You can also do this step at a later stage, basically whatever suits you best for your creative process. For example, you might want to do first steps 4-7 before sketching a draft.\nGet your abstract and extract title, author and affiliation and save it into a text file too.\nCreate a folder img somewhere on your computer.\nCopy all text, tables and figures you possibly need into the img folder. Slides from previous talks can be a handy source.\nGet the logos of your university and affiliations and store them into the img folder too. If you have the choice, go for PNGs, as they have transparent backgrounds.\nChoose a tool that is easy to operate and does the things you want. My choice is keynote because I cannot bother with Adobe Illustrator. More details below.\nUpdate: Read the conference instructions, particularly on poster size and orientation (as recommended by Peter Higgins).\nResize document to the size recommended by the conference or A0 (841 mm x 1189 mm or 2384 pt x 3370 pt). This way, even though the poster might be proportional to A4, you can use the real size fonts.\nSplit the document into different boxes (or any compartmentation according to your sketch). For example four horizontal lines and one vertical lines will turn 10 boxes. Use rulers, guides or lines to help you with that. Each box will later answer a different question. Now you are ready to add content.\nAdd content Before adding content, choose a sans-serif font. Helvetica or Arial will do a good job. These fonts were carefully designed to be easy to read and look good on print. Later on, you can still decide to invest time into checkout google fonts, but remember - it needs to be sans-serif. The smallest text size should be 24 pt.\nFill the top rectangles.\nPlace the title on top. This is the first thing the reader should see.\nAdd authors and affiliation right below.\nAdd the logos next to the authors and affiliations.\nNext, add your main message below. Add contact and lab webpage at the very bottom of the poster. No need to adjust and fiddle around with boxes at this stage. Just fill the content.\nNow we are left with 7 boxes. How you distribute the content into these boxes depends on your topic. I work in a field where data and methodology is central, so I always dedicate one box to data and one box to methods. The first box will take care of the introduction and should answer the question Why is your research needed?\nThe second box should present the methods: What method did you use to answer your research question?\nThe third box takes care of data presentation: What kind of data did you use and how does it fit together with the method?\nThe fourth and fifth box are reserved for results: What are the results if you combine data and methods?\nThe six box is dedicated to a discussion or summary: What can you conclude from the results? What are the limitations? And where are you heading next?\nThe seventh box is for references and abbreviations.\nUpdate: Add a small portrait of yours to the upper corners or next to your contact email address. This way people can spot and talk to you outside the poster session (recommended by Federico Marini).\nUpdate: Add some empty post-it notes and a pencil next to your poster and ask readers for feedback (recommended by Federico Marini).\nPolishing \u0026amp; Design Now that you have all your content, you can start to improve the readability.\nIn order to quickly grasp a poster, it should have as less sentences as possible.\nConsider replacing some text with icons or figures.\nMake use of bullet points.\nAlign all text and shapes (again, rulers might help here).\nMake sure all section titles are equally sized.\nAre your figures (and the rest of your poster) colour-blind friendly?\nYou can start adding colours (if they serve a purpose).\nThink about whether your figures need captions or not.\nFinalising See how it looks like on a printed A4 version (update: or for less troublesome reading print it on A3, as mentioned by Michael MacAskill).\nAsk your colleagues for their opinion by showing them the printed version.\nYou might want to let it sit for a day or two. Then look at it with fresh and rested eyes.\nTrick-trick: Have a detailed and a lean version After a few days break you look at it again and you realise that it is still to busy with text.\nHere is the trick - keep two versions: the detailed one you already made (that can be accessed through a QR code), and the one that you will bring to the poster session with lot less details.\nMake a copy of your poster draft, name it poster_detailed.pdf. Put this pdf somewhere online (e.g. dropbox, google drive) and get a link to it.\nGenerate a QR-code of that link and download the png to your img/ folder.\nContinue working on your original file.\nCount your words. Your aim is now to cut the number of words in half - roughly. If you don\u0026rsquo;t know where to remove content, present your poster to a colleague from the same faculty (not the same lab). You will quickly realise over what details you jump. Remove those details.\nIf you can simplify the graphs - do that too.\nCount the words again - has it been halfed?\nAdd the QR-code to the bottom of the poster.\nPrinting Print it again in A4 for yourself and check: if all text and shapes are aligned and equally distributed, if the font is the same throughout the poster, if all authors or affiliations are present, for typos, if figures are clearly readable (and in high resolution), where the QR-code leads you, whether your contact email address is correct. That\u0026rsquo;s it - send it to the printing service! Content Before adding any content, think about your audience. These will be scientists, and they have probably heard about research you are working on, but don\u0026rsquo;t know the details and motivation behind your work. In fact, there will only be a handful of people that know exactly what your domain, topic and method does. All others are still interested though. So lets aim for these people as your audience.\nAfter looking at your poster, the reader should have an idea what the title means. Your poster title might be super clear to you and your lab colleagues, but anyone else will have a hard time understanding right away what you are doing.\nTherefore, one goal is, to clarify the words in your title.\nAfter separating your file with rulers as described above, you will have 8 boxes, of which you can use 6 for:\nIntroduction Method Data General results Specific results Discussion \u0026amp; Summary I recommend to give your boxes titles so the reader can jump from one to the other.\nWhat content should go in your boxes?\nI believe that a poster cannot carry a condensed version of a full manuscript. I think it is ok to focus on only one or two threads of your work. Your story telling needs to be coherent - that is all the reader wants.\nIntroduction I like to follow the paper writing guidelines by Jennifer Widom as a rough guide:\nWhat is the problem? Why is it interesting and important? Why is it hard? Why hasn\u0026rsquo;t it been solved before? What are the key components of my approach and results? Methods Unless your very focus is a new statistical methods, keep this section simple and clear. Make sure that it is clear why the method is able to answer your research question. You can use equations, but simplify them too, or annotate them with arrows and explain all fancy letters used. Data Mention the origin(s) of your data. What was the sample size? No need to describe all the variables you used. To save space, you can also group them into larger categories. Describe distinct features of your data that could influence the interpretation: e.g. if all your samples were male. Mention how you processed the data. Again, no need to be specific. If your area uses standard QC methods, mention that the data was processed according to standard QC techniques. Results Have two layers of detail. For example, go from general to specific; have a graph that summarises your results, then zoom into your results in a next graph. Use figures to illustrate your results and avoid tables. Simplify the figures and use the power of your poster making tool to annotate the figures. Make sure that the results are follow the notation of data and methodology. E.g. if you present results for a trend in age, make sure you have mentioned age before. Discussion This is the place to:\nsummarise your work conclude talk about limitations announce future plans Contact Add your email address. Add lab webpage. Add the QR code that leads to your (detailed) poster. References Add important references, for example software that you used. Make sure you cite original content. Design Or improving readability.\nTooling A good tool should satisfy the following criteria:\nDecent in zooming in and out. Variety of graphical options. Operable with ease. Something you can use over years. If you have practice with Adobe Illustrator or its\u0026rsquo; open-source counterpart Inkscape - go for it!\nOtherwise, google slides, Powerpoint, keynote - although limiting - do a the job too.\nFonts No matter what sophisticated typeface you had in mind, choose one of these sans-serif ones for a start: Helvetica or Arial. If you really have the time to play with other sans-serif fonts, check out google fonts. Making fonts is an actual job - remember to pay for special fonts. Most tools have a replace font option, so you can replace a font through out your poster. Font size Minimum 24 pt, but use it sparingly. Title should be largest with somewhere in between 100 and 140 pt. Icons Some icon services are for free, or at least partially: e.g. flat icon.\nFigures Principles for data visualisations could easily occupy another blog post. There is much to say, but let\u0026rsquo;s keep it short here.\nIf you use R and ggplot2 for making your figures, use one of the black and white themes, e.g. theme_set(theme_linedraw()).\nMake sure that the axis text size is also 24pt once it is on the poster.\nColours Colours are either used as decoration or to encode a variable. Do not use colour for your background^[Black or gray background and white text colour might work though.].\nAlways use colour-blind friendly colours.\nYou can test the colour-blind friendlyness of your poster and figures with https://colororacle.org, a colour-blindness simulator on your computer, or in a browser: https://www.color-blindness.com/coblis-color-blindness-simulator/.\nFor Figures:\nIf you use more colours than available in a discrete colour palette - drop that encoding. No one will be able to grasp. Use colour-blind friendly palettes from the colorblindr R-package. Resolution of figures Your principal figures should always be stored as PDF format. This way, you can easily turn them into a high-resolution PNG.\nTo turn a PDF into a high-resolution raster image, use convert from imagemagick.\nIf your file is called figure-1.pdf, then write:\nconvert -quality 100 -background white -alpha background -compress lzw -units pixelsperinch -resize \u0026#34;789x2625\u0026#34; -density 300 figure-1.pdf figure-1b.png If you have lots of pdf starting with figure-*, then write:\nfor file in figure-*.pdf; do \\ echo $file;\\ convert -quality 100 -background white -alpha background -compress lzw -flatten -units pixelsperinch -resize \u0026#34;789x2625\u0026#34; -density 300 $file `echo $file|cut -f1 -d\u0026#39;.\u0026#39;`.png;\\ done Links https://guides.library.ucla.edu/c.php?g=223540\u0026amp;p=1480858tag http://blogs.nature.com/naturejobs/2017/11/06/using-design-principles-to-inform-scientific-posters/ Update: Ten Simple Rules for a Good Poster Presentation (Plos CompBio) (mentioned by Federico Marini) Full disclosure Here are some of my posters from different stages of my career. Making posters is a process of improvement.\n2018 2017 ","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"b2ce2c070bea0f3df1866715d67e699a","permalink":"https://sinarueeger.github.io/post/scientific-poster/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/post/scientific-poster/","section":"post","summary":"How to make a good poster Recipe Content Design This is a tutorial on how to make a scientific poster from scratch.\nIt aims for first-time poster makers, but also for people like me that quickly lose themselves in choosing the right font for hours.\nIt includes a few self-declared best practices that helped me in the past. Those are at best ideas, and barely opinions. It is by no means a collection of strict rules and feedback is welcome!","tags":["science communication","conference"],"title":"A guide to poster making for scientific conferences","type":"post"},{"authors":null,"categories":null,"content":"I finished my PhD under the main supervision of Zoltán Kutalik in September 2018.\nYou can download my thesis or check out the slides from my public defense.\nAbstract Increasing our knowledge about biology in humans is essential for advances in medicine, such as early-stage diagnoses of diseases, drug development, public health strategies, and precision medicine. One approach to tackle this task is, to collect data on different components of a biological mechanism of interest, link these parts and try to construct an underlying model that helps us to explain the disease. To collect data, DNA is measured and the status of a disease is recorded for each individual in a dedicated group of people. In a first step, an analyst compares for each genetic variant across the whole genome the genetic mutations between people with the disease and healthy people; this is called a genome-wide association study (GWAS). Such first association screens rarely point right away to the true causal variants, but combined with additional biomedical (-omics) data and additional statistical methods it is possible to narrow down the true cause and gain insight into the biology of a disease. For example, by using GWAS results for two diseases (e.g. cardiovascular disease and obesity) and a statistical method called Mendelian randomisation, we are able to examine the causal effect of obesity on cardiovascular disease, or vice versa. These statistical follow-up investigations often require GWAS results for genetic variants than were unmeasured. During my PhD, I investigated a method called summary statistic imputation that precisely aims to solve the problem of inferring GWAS results for unmeasured genetic variants. Summary statistic imputation uses GWAS results and data from public sequencing data. My main findings were that imputation accuracy varies depending on certain characteristics of a genetic variant (e.g. low accuracy for rare mutations), as well as the size of publicly available sequencing data (low accuracy for small sized sequencing data). A further finding is, that summary statistic imputation can compete with imputation techniques that are based on individual-level data for certain subgroups of genetic variants (e.g. common variants).\nWith the help of summary statistic imputation researchers can facilitate follow-up investigations and thus gain more insight into the biology of diseases.\n","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"070d51a28892f81411bee8389f825d6d","permalink":"https://sinarueeger.github.io/project/phd-thesis/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/project/phd-thesis/","section":"project","summary":"PhD thesis material.","tags":["statistical genetics"],"title":"Integrative Statistical Analysis of -omics and GWAS data","type":"project"},{"authors":null,"categories":null,"content":"","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"0673e061ae94badb51ab44d83f8b8852","permalink":"https://sinarueeger.github.io/project/gggwas/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/project/gggwas/","section":"project","summary":"ggplot2 extension for visualising GWAS summary statistics.","tags":["statistical genetics","data visualisation","R"],"title":"R-package ggGWAS","type":"project"},{"authors":null,"categories":null,"content":"","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"1015e5348459ba232b9c99a584dbf21b","permalink":"https://sinarueeger.github.io/project/gwas.utils/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/project/gwas.utils/","section":"project","summary":"Helper functions when working with GWAS (summary) data.","tags":["statistical genetics","R"],"title":"R-package GWAS.utils","type":"project"},{"authors":null,"categories":null,"content":"","date":1559001600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559001600,"objectID":"2a64e5712fe7bfd61bd034d3051a241e","permalink":"https://sinarueeger.github.io/project/stat-genetics-ressources/","publishdate":"2019-05-28T00:00:00Z","relpermalink":"/project/stat-genetics-ressources/","section":"project","summary":"Collection of resources needed in statistical genetics: data, readings, software.","tags":["statistical genetics"],"title":"Statistical Genetics Resources","type":"project"},{"authors":null,"categories":null,"content":" Goal Two approaches Our toy data 1A. A solution that works: ldlink from NIH LDproxy LDmatrix 1B. A solution that almost works: ensembl.org What reference panels/population can we choose from? Access LD between a SNP and its region Access LD matrix Access LD between a SNP and many other SNPs Coloured locuszoom plot 2. Solutions that work half-through SNPsnap API provided by sph.umich 3. A solution that does not work rsnps::ld_search Conclusion Session Info R-packages used:\nlibrary(httr) library(jsonlite) library(xml2) library(ggplot2) theme_set(theme_bw()) library(tibble) library(tidyr) library(magrittr) library(glue) library(purrr) library(rsnps) library(data.table) library(janitor) library(stringr) library(rsnps) The squared correlation between genetic markers is one way to estimate linkage disequilibrium (LD). LD has to be computed all the time - either for as an input for statistical methods or to summarise results.\nHowever, accessing LD estimations quickly, for a specific population and in an automated way (e.g. with R) is suprisingly difficult.\nIn this blog post I am exploring how to do this efficiently.\nGoal At the end of this blog post, we want to know the genetic correlation between two or more markers in a specific human population, so that we can populate the locuszoom plot from the previous blog post with coloured dots.\nFor simplicity, I will use the terms correlation, squared correlation, r, r2 and LD interchangeably.\nTwo approaches In principle, there are two ways of doing accessing LD:\nDownload (or access) the genetic data from which you want to estimate your correlations + calculate the correlations using some efficient approach. Access precomputed LD estimations. Approach Advantages Downsides Useful when… (1) Local computation of LD LD matrix can be quickly updated to new reference panels Requires large computation and storage space (e.g. 1000 Genomes is \u0026gt;100 few GB large). i) LD for a large set of SNPs is needed ii) LD from non-standard reference panel is needed. (2) Access precomputed LD not need for large computation and storage space. limited to certain small sets of markers, limited to possibly outdated reference panels. LD for a small set of SNPs is needed For now, I will focus on approach (2), and then explore approach (1) in a future blog post.\nSpoiler: Using approach (2) does not get you far. It took me quite a while to gather all the solutions that are listed below, and yet there is not one perfect/ideal solution.\nOur toy data We will recycle the data from the previous blog post, where the focus was on extracting annotation using the package biomaRt. In this blog post, we will complete that locuszoom plot by adding the LD information.\n## Data Source URL url \u0026lt;- \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/2/21/BMI_All_ancestry.fmt.gzip\u0026quot; ## Import BMI summary statistics dat.bmi \u0026lt;- read_tsv(file = url) ## ## taking too long, let\u0026#39;s use fread instead. dat.bmi \u0026lt;- data.table::fread(url, verbose = FALSE) ## Rename some columns dat.bmi \u0026lt;- dat.bmi %\u0026gt;% rename(SNP = SNPNAME, P = Pvalue) ## Extract region dat.bmi.sel \u0026lt;- dat.bmi %\u0026gt;% slice(which.min(P)) dat.bmi.sel ## range region range \u0026lt;- 5e+05 sel.chr \u0026lt;- dat.bmi.sel$CHR sel.pos \u0026lt;- dat.bmi.sel$POS data \u0026lt;- dat.bmi %\u0026gt;% filter(CHR == sel.chr, between(POS, sel.pos - range, sel.pos + range)) head(data) (snp \u0026lt;- dat.bmi.sel$SNP) What we are interested in is the LD between our top SNP rs1421085 and all other 82 SNPs nearby.\nThis dataset has positions on build GRCh37, while most databases are on build GRCh38 by now.\nsm \u0026lt;- rsnps::ncbi_snp_summary(snp) %\u0026gt;% separate(chrpos, c(\u0026quot;chr\u0026quot;, \u0026quot;pos\u0026quot;)) sel.pos == as.numeric(sm$pos) ## [1] FALSE Let’s quickly repeat what our primary goal is:\nExtract the correlation between SNPs\nwithout downloading any data, fairly quick and in R. 1A. A solution that works: ldlink from NIH ldlink is a website provided by NIH to easily (and programmatically) request LD estimates in population groups.\nLD is estimated from Phase 3 of the 1000 Genomes Project and super- and subpopulations can be selected.\nThere are different ways to access the LD estimations (e.g. LDpair, LDmatrix, LDproxy) and the same modules are also available through the API.\nTo access the API, you need to register for a token (takes a few seconds).\nMYTOKEN \u0026lt;- \u0026quot;a_mix_of_numbers_and_characters\u0026quot; Let’s look at two modules:\nLDproxy: access LD between a SNP and its region LDmatrix: access LD matrix LDproxy To get the LD between a SNP and its region.\nFirst, access the API:\nLDproxy_raw \u0026lt;- system( glue::glue(\u0026quot;curl -k -X GET \u0026#39;https://ldlink.nci.nih.gov/LDlinkRest/ldproxy?var={snp}\u0026amp;pop=EUR\u0026amp;r2_d=r2\u0026amp;token={MYTOKEN}\u0026#39;\u0026quot;), intern = TRUE ) Then, do a bit of data wrangling to get a tidy data frame:\nLDproxy \u0026lt;- LDproxy_raw %\u0026gt;% purrr::map(., function(x) stringr::str_split(x, \u0026quot;\\t\u0026quot;) %\u0026gt;% unlist()) %\u0026gt;% ## remove all the tabs do.call(rbind, .) %\u0026gt;% ## turn into a matrix data.frame() %\u0026gt;% ## turn into a data frame janitor::row_to_names(1) %\u0026gt;% ## make the first row the column names rename(SNP = RS_Number) %\u0026gt;% ## rename RS_Number as SNP mutate_at(vars(MAF:R2), function(x) as.numeric(as.character(x))) %\u0026gt;% ## turn MAF:R2 columns numeric mutate(SNP = as.character(SNP)) ## turn SNP from a factor into a character head(LDproxy) ## SNP Coord Alleles MAF Distance Dprime R2 ## 1 rs1421085 chr16:53800954 (T/C) 0.4324 0 1.0000 1.0000 ## 2 rs11642015 chr16:53802494 (C/T) 0.4324 1540 1.0000 1.0000 ## 3 rs62048402 chr16:53803223 (G/A) 0.4324 2269 1.0000 1.0000 ## 4 rs1558902 chr16:53803574 (T/A) 0.4324 2620 1.0000 1.0000 ## 5 rs55872725 chr16:53809123 (C/T) 0.4324 8169 1.0000 1.0000 ## 6 rs56094641 chr16:53806453 (A/G) 0.4344 5499 0.9959 0.9839 ## Correlated_Alleles RegulomeDB Function ## 1 T=T,C=C 5 NA ## 2 T=C,C=T 4 NA ## 3 T=G,C=A 5 NA ## 4 T=T,C=A 7 NA ## 5 T=C,C=T 4 NA ## 6 T=A,C=G 6 NA Next, join the original summary stats data with the LDproxy data frame.\ndata_ldproxy \u0026lt;- data %\u0026gt;% right_join(LDproxy, by = c(\u0026quot;SNP\u0026quot; = \u0026quot;SNP\u0026quot;)) Lastly, plot the summary statistics with the point colour indicating the R2.\nggplot(data = data_ldproxy) + geom_point(aes(POS, -log10(P), color = R2), shape = 16) + labs( title = \u0026quot;Locuszoom plot for BMI GWAS\u0026quot;, subtitle = paste( \u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot; ), caption = paste(\u0026quot;Data source:\u0026quot;, url) ) + geom_point( data = data_ldproxy %\u0026gt;% filter(SNP == snp), aes(POS, -log10(P)), color = \u0026quot;black\u0026quot;, shape = 16 ) + scale_color_distiller(\u0026quot;R2 (LDproxy)\u0026quot;, type = \u0026quot;div\u0026quot;, palette = \u0026quot;Spectral\u0026quot;, limits = c(0, 1) ) LDmatrix LDmatrix module accesses the pairwise LD between a set of SNPs.\nAgain, first access the API:\nsnplist \u0026lt;- data %\u0026gt;% filter(str_detect(SNP, \u0026quot;rs\u0026quot;)) %\u0026gt;% pull(SNP) %\u0026gt;% paste(collapse = \u0026quot;%0A\u0026quot;) LDmatrix_raw \u0026lt;- system( glue::glue(\u0026quot;curl -k -X GET \u0026#39;https://ldlink.nci.nih.gov/LDlinkRest/ldmatrix?snps={snplist}\u0026amp;pop=CEU%2BTSI%2BFIN%2BGBR%2BIBS\u0026amp;r2_d=r2\u0026amp;token={MYTOKEN}\u0026#39;\u0026quot;), intern = TRUE ) If you want to access the dprime (d’) values, write r2_d=d. If you want to access certain sub populations, let’s say CEU, TSI and FIN, concatenate them with %B in between: CEU%2BTSI%2BFIN. Then, do a little data tidying:\nLDmatrix \u0026lt;- LDmatrix_raw %\u0026gt;% purrr::map(., function(x) stringr::str_split(x, \u0026quot;\\t\u0026quot;) %\u0026gt;% unlist()) %\u0026gt;% do.call(rbind, .) %\u0026gt;% ## turn into a matrix data.frame() %\u0026gt;% ## turn into a data.frame janitor::row_to_names(1) ## make the first line the column names LDmatrix_long \u0026lt;- LDmatrix %\u0026gt;% gather(\u0026quot;SNP2\u0026quot;, \u0026quot;R2\u0026quot;, -RS_number) %\u0026gt;% ## from wide to long rename(SNP = RS_number) %\u0026gt;% ## rename RS_number mutate(R2 = as.numeric(R2)) %\u0026gt;% ## make R2 numeric mutate_if(is.factor, as.character) ## make all factor columns characters ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: NAs introduced by coercion head(LDmatrix_long) ## SNP SNP2 R2 ## 1 rs61754093 rs61754093 1.000 ## 2 rs181111349 rs61754093 NA ## 3 rs199662749 rs61754093 NA ## 4 rs189080082 rs61754093 0.000 ## 5 rs6499548 rs61754093 0.023 ## 6 rs139704369 rs61754093 NA Next, join the original summary stats data with the LDmatrix_long data frame.\ndata_ldmatrix \u0026lt;- data %\u0026gt;% right_join(LDmatrix_long, by = c(\u0026quot;SNP\u0026quot; = \u0026quot;SNP\u0026quot;)) Lastly, plot the summary statistics with the point colour indicating the R2.\nggplot(data = data_ldmatrix %\u0026gt;% filter(SNP2 == snp)) + geom_point(aes(POS, -log10(P), color = R2)) + labs( title = \u0026quot;Locuszoom plot for BMI GWAS\u0026quot;, subtitle = paste( \u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot; ), caption = paste(\u0026quot;Data source:\u0026quot;, url) ) + geom_point( data = data_ldmatrix %\u0026gt;% filter(SNP == snp \u0026amp; SNP2 == snp), aes(POS, -log10(P)), color = \u0026quot;black\u0026quot;, shape = 16 ) + scale_color_distiller(\u0026quot;R2 (LDmatrix)\u0026quot;, type = \u0026quot;div\u0026quot;, palette = \u0026quot;Spectral\u0026quot;, limits = c(0, 1) ) 1B. A solution that almost works: ensembl.org The REST API of Ensembl can do a lot (see options here). For example access precomputed LD. The webpage even provides R code to do so, which is from where I copied some snippets below.\nTo access the rest API at ensembl, we need the following three packages loaded.\nlibrary(httr) library(jsonlite) library(xml2) What reference panels/population can we choose from? Currently, the largest and hence most popular public reference panel is 1000 Genomes reference panel (1KG). The 26 populations of roughly 100 individuals each can be grouped into five super populations: African (AFR), American (AMR), European (EUR), South Asian (SAS), East Asian (EAS).\nWe can ask the ENSEMBL API from what populations reference panels are available. This will return us a data frame.\nserver \u0026lt;- \u0026quot;https://rest.ensembl.org\u0026quot; ext \u0026lt;- \u0026quot;/info/variation/populations/homo_sapiens?filter=LD\u0026quot; r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) head(fromJSON(toJSON(content(r)))) ## description size ## 1 African Caribbean in Barbados 96 ## 2 African Ancestry in Southwest US 61 ## 3 Bengali in Bangladesh 86 ## 4 Chinese Dai in Xishuangbanna, China 93 ## 5 Utah residents with Northern and Western European ancestry 99 ## 6 Han Chinese in Bejing, China 103 ## name ## 1 1000GENOMES:phase_3:ACB ## 2 1000GENOMES:phase_3:ASW ## 3 1000GENOMES:phase_3:BEB ## 4 1000GENOMES:phase_3:CDX ## 5 1000GENOMES:phase_3:CEU ## 6 1000GENOMES:phase_3:CHB name stands for the population identifier. size refers to the number of individuals in the reference panel. Note that these are all populations with around 100 individuals (the correlation estimation will have an error that scales with the sample size). There are also the five super population available (although not listed here), simply replace the last three characters in name by EUR, AFR, AMR, EAS, SAS.\n(From this blog post.)\nWe want the LD information, so that we can add this info to the locuszoom plot. But how do we know which population to pick? One way is to read up what kind of individuals were present. In our case - mostly Europeans (EUR). But we could also build some pooled LD matrix of different populations.\nNow that we know which reference panel we want to use, we can use the different rest APIs.\nAccess LD between a SNP and its region Access LD matrix Access LD between a SNP and many other SNPs Access LD between a SNP and its region This API is described here.\nThe default window size is 500 kb. There are also thresholds for r2 (e.g. if you want to filter all SNPs with an r2 \u0026gt; 0.8).\nThe only input required is the SNP rsid, marked with {snp}.\nsnp ## [1] \u0026quot;rs1421085\u0026quot; server \u0026lt;- \u0026quot;https://rest.ensembl.org\u0026quot; ext \u0026lt;- glue::glue(\u0026quot;/ld/human/{snp}/1000GENOMES:phase_3:EUR?\u0026quot;) ## Window size in kb. The maximum allowed value for the window size is 500 kb. LD is computed for the given variant and all variants that are located within the specified window. r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) LD.SNP.region \u0026lt;- as_tibble(fromJSON(toJSON(content(r)))) %\u0026gt;% unnest() %\u0026gt;% mutate(r2 = as.numeric(r2)) ## Warning: `cols` is now required. ## Please use `cols = c(variation1, d_prime, r2, population_name, variation2)` head(LD.SNP.region) ## # A tibble: 6 x 5 ## variation1 d_prime r2 population_name variation2 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 rs1421085 0.327013 0.0834 1000GENOMES:phase_3:EUR rs8043738 ## 2 rs1421085 0.992084 0.409 1000GENOMES:phase_3:EUR rs2042031 ## 3 rs1421085 0.846280 0.647 1000GENOMES:phase_3:EUR rs11642841 ## 4 rs1421085 1.000000 0.957 1000GENOMES:phase_3:EUR rs9940128 ## 5 rs1421085 0.999948 0.0552 1000GENOMES:phase_3:EUR rs73612011 ## 6 rs1421085 0.907868 0.648 1000GENOMES:phase_3:EUR rs8057044 As a result, LD.snp.region contains the r2 of our top SNP with all SNPs that were +/- 500 kb away.\nWhat if we want the correlation between all SNPs?\nAccess LD matrix For this, we need the rest API here.\nWe can calculate the LD matrix of a full region, max 1 Mb wide. For fast computation, we limit it to +/- 50 kb.\n## Query region. A maximum of 1Mb is allowed. ext \u0026lt;- glue::glue(\u0026quot;/ld/human/region/{sel.chr}:{sel.pos - range/20}..{sel.pos + range/20}/1000GENOMES:phase_3:EUR?\u0026quot;) r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) LD.matrix.region \u0026lt;- as_tibble(fromJSON(toJSON(content(r)))) %\u0026gt;% unnest() %\u0026gt;% mutate(r2 = as.numeric(r2)) ## Warning: `cols` is now required. ## Please use `cols = c(population_name, r2, variation2, variation1, d_prime)` head(LD.matrix.region) ## # A tibble: 6 x 5 ## population_name r2 variation2 variation1 d_prime ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1000GENOMES:phase_3:EUR 0.761 rs9933509 rs8057044 0.999998 ## 2 1000GENOMES:phase_3:EUR 1 rs150763868 rs72803680 1.000000 ## 3 1000GENOMES:phase_3:EUR 0.106 rs7206122 rs62033401 0.999940 ## 4 1000GENOMES:phase_3:EUR 0.731 rs9935401 rs8057044 0.999997 ## 5 1000GENOMES:phase_3:EUR 0.106 rs141816793 rs62033399 0.999943 ## 6 1000GENOMES:phase_3:EUR 0.996 rs9941349 rs9931900 1.000000 Access LD between a SNP and many other SNPs The third and last option is to pass on a set of SNP rs ids, and access the LD among these. Implemented in the ENSEMBL API is only the LD between two SNPs, so we will have to extend this to many SNPs.\nextract_ld \u0026lt;- function(SNP.id2 = NULL, SNP.id1 = NULL, POP = NULL) { ext \u0026lt;- glue::glue(\u0026quot;/ld/human/pairwise/{SNP.id1}/{SNP.id2}/\u0026quot;) ## filter POP further down server \u0026lt;- \u0026quot;https://rest.ensembl.org\u0026quot; r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) out \u0026lt;- as_tibble(fromJSON(toJSON(content(r)))) %\u0026gt;% unnest() %\u0026gt;% filter(stringr::str_detect(population_name, POP)) return(out) } ## see futher down why intersect here other.snps \u0026lt;- intersect(LD.SNP.region$variation2, data$SNP) ## cacluate LD for all other.snps SNPs LD.matrix.snps \u0026lt;- purrr::map_df(other.snps, extract_ld, snp, \u0026quot;EUR\u0026quot;) %\u0026gt;% mutate(r2 = as.numeric(r2)) %\u0026gt;% bind_rows() %\u0026gt;% unnest() ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(d_prime, variation1, variation2, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, variation1, d_prime, population_name, r2)` ## Warning: `cols` is now required. ## Please use `cols = c(variation1, d_prime, variation2, population_name, r2)` ## Warning: `cols` is now required. ## Please use `cols = c(r2, population_name, d_prime, variation1, variation2)` ## Warning: `cols` is now required. ## Please use `cols = c(population_name, r2, variation1, d_prime, variation2)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(d_prime, variation1, variation2, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c()` LD.matrix.snps ## # A tibble: 0 x 5 ## # … with 5 variables: variation2 \u0026lt;chr\u0026gt;, d_prime \u0026lt;chr\u0026gt;, variation1 \u0026lt;chr\u0026gt;, ## # r2 \u0026lt;dbl\u0026gt;, population_name \u0026lt;chr\u0026gt; Calculate the LD matrix (LD.matrix.region) or the LD between SNP pairs (LD.matrix.snps) takes a lot of time!\nColoured locuszoom plot For the locuszoom plot we need only the correlation between the top SNP and all other SNPs. So we join the object LD.SNP.region to data.\ndata_ensembl \u0026lt;- data %\u0026gt;% full_join(LD.SNP.region, by = c(\u0026quot;SNP\u0026quot; = \u0026quot;variation2\u0026quot;)) ggplot(data = data_ensembl) + geom_point(aes(POS, -log10(P), color = r2), shape = 16) + labs( title = \u0026quot;Locuszoom plot for BMI GWAS\u0026quot;, subtitle = paste(\u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot;), caption = paste(\u0026quot;Data source:\u0026quot;, url) ) + geom_point( data = data_ensembl %\u0026gt;% filter(SNP == \u0026quot;rs1421085\u0026quot;), aes(POS, -log10(P)), color = \u0026quot;black\u0026quot;, shape = 16 ) + scale_color_distiller(\u0026quot;R2 (ensembl)\u0026quot;, type = \u0026quot;div\u0026quot;, palette = \u0026quot;Spectral\u0026quot;, limits = c(0, 1)) ## Warning: Removed 205 rows containing missing values (geom_point). 2. Solutions that work half-through SNPsnap SNPsnap: https://data.broadinstitute.org/mpg/snpsnap/database_download.html uses a limited set of 1KG populations (EUR, EAS, WAFR). API provided by sph.umich API uses limited set of 1KG populations (ALL, EUR) see github issue 3. A solution that does not work rsnps::ld_search A perfect solution would have been the function ld_search from R package rsnps. It has arguments to choose the reference panel, the population, the distance from the focal SNP.\nThe problem is that it only uses old reference panels (HapMap and 1KG-phase1). Meaning, many newer reference panel populations are left out.\nBut the main problem is, that the broad institute has taken down the snap server that ld_search used to access (see github issue); hence ld_search is defunct.\nConclusion The ldlink API with the LDproxy module seems the most perfect solution for now.\nThis will probably change with changing technology and larger reference panels.\nSession Info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.14.5 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] janitor_1.2.0 data.table_1.12.2 rsnps_0.3.2.9121 ## [4] glue_1.3.1.9000 magrittr_1.5 xml2_1.2.0 ## [7] jsonlite_1.6 httr_1.4.0 forcats_0.4.0.9000 ## [10] stringr_1.4.0 dplyr_0.8.3.9000 purrr_0.3.2 ## [13] readr_1.3.1 tidyr_0.8.3.9000 tibble_2.1.3 ## [16] ggplot2_3.2.0.9000 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.8 haven_2.1.1 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.2.0.9000 generics_0.0.2 htmltools_0.3.6 ## [9] yaml_2.2.0 utf8_1.1.4 XML_3.98-1.20 rlang_0.4.0 ## [13] pillar_1.4.2 httpcode_0.2.0 withr_2.1.2 modelr_0.1.4 ## [17] readxl_1.3.1 plyr_1.8.4 munsell_0.5.0 blogdown_0.13 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.4 evaluate_0.14 ## [25] knitr_1.23 curl_3.3 fansi_0.4.0 triebeard_0.3.0 ## [29] urltools_1.7.3 broom_0.5.2 Rcpp_1.0.1 scales_1.0.0 ## [33] backports_1.1.4 hms_0.4.2 digest_0.6.20 stringi_1.4.3 ## [37] bookdown_0.11 grid_3.5.1 cli_1.1.0 tools_3.5.1 ## [41] lazyeval_0.2.2 crul_0.8.0 crayon_1.3.4 pkgconfig_2.0.2 ## [45] zeallot_0.1.0 ellipsis_0.2.0.1 lubridate_1.7.4 assertthat_0.2.1 ## [49] rmarkdown_1.13 rstudioapi_0.10 R6_2.4.0 icon_0.1.0 ## [53] nlme_3.1-140 compiler_3.5.1 ","date":1558396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558396800,"objectID":"eac79e0c4c3c5307b9eaee6ecedb3ae9","permalink":"https://sinarueeger.github.io/post/get-ld-remotely/","publishdate":"2019-05-21T00:00:00Z","relpermalink":"/post/get-ld-remotely/","section":"post","summary":"Goal Two approaches Our toy data 1A. A solution that works: ldlink from NIH LDproxy LDmatrix 1B. A solution that almost works: ensembl.org What reference panels/population can we choose from? Access LD between a SNP and its region Access LD matrix Access LD between a SNP and many other SNPs Coloured locuszoom plot 2. Solutions that work half-through SNPsnap API provided by sph.umich 3. A solution that does not work rsnps::ld_search Conclusion Session Info R-packages used:","tags":["statistical genetics","R","data visualisation"],"title":"LD Part 1","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides Academic\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nA fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://sinarueeger.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":null,"content":" ","date":1544468400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544468400,"objectID":"d124c794115c91ba7343c2dc6d794b27","permalink":"https://sinarueeger.github.io/talk/r-ladies/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/r-ladies/","section":"talk","summary":" ","tags":["workflow","R","R-Ladies"],"title":"Introduction to Drake","type":"talk"},{"authors":null,"categories":null,"content":" Goal Get started Data Add geographical coordinates Create leaflet Save the map Reason for deviation from the original This post provides the R-Code to map the 26 populations of the 1000 Genomes project.\nGoal Create a map similar to the one1 on the front page of http://www.internationalgenome.org/ in a reproducible manner.\nVersion on internationalgenome.org\nGet started Packages needed:\n## accessed via :: # library(mapview) # library(readxl) # library(readr) # library(purrr) # library(tidyr) # library(forcats) library(leaflet) library(dplyr) library(ggmap) ## for geocode, devtools::install_github(\u0026quot;dkahle/ggmap\u0026quot;) ggmap requires a google map api key2:\nget one here: https://developers.google.com/maps/documentation/geocoding/get-api-key then run register_google(key = \"my_api_key\") Data The population counts and labels are from ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130606_sample_info/ (download xlsx file). The super population labels are from here (pasted into a csv, then the location was inferred). Download the population counts and labels first:\nurl \u0026lt;- \u0026quot;ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130606_sample_info/20130606_sample_info.xlsx\u0026quot; url.bitly \u0026lt;- \u0026quot;http://bit.ly/2MQTr02\u0026quot; download.file(url, \u0026quot;20130606_sample_info.xlsx\u0026quot;, mode = \u0026quot;wb\u0026quot;) Import file into R:\ndf \u0026lt;- readxl::read_excel(\u0026quot;20130606_sample_info.xlsx\u0026quot;, sheet = \u0026quot;Sample Info\u0026quot;) # \u0026gt;\u0026gt; Sample Info Some data wrangling:\n## count number of individuals by population ## rename population \u0026gt; POP n.pop \u0026lt;- df %\u0026gt;% group_by(Population) %\u0026gt;% summarise(n = n()) %\u0026gt;% rename(POP = Population) ## import super population names and details to the location of populations ## copied from here: url.spop \u0026lt;- \u0026quot;http://www.internationalgenome.org/faq/which-populations-are-part-your-study/\u0026quot; ## added location manually (!) - found this the only option to prevent overlapping locations. ## Also, description involves a mix of location and origin. ## rename superpopulation \u0026gt; SPOP n.spop \u0026lt;- readr::read_csv(\u0026quot;../../static/post/2018-12-05-1000genomes-map/sample_info_superpop.csv\u0026quot;) %\u0026gt;% rename(POP = `Population Code`, SPOP = `Super Population Code`) ## join the two information n.1kg \u0026lt;- left_join(n.pop, n.spop, by = c(\u0026quot;POP\u0026quot; = \u0026quot;POP\u0026quot;)) Add geographical coordinates Parts of the code below is from a map created by Daniela Vazquez for R-Ladies: https://github.com/rladies/Map-RLadies-Growing.\nThis is the part where we annotate the dataframe n.1kg with where the individuals live (not their ancestry). Repeat this until there are no warnings() about QUERY LIMITS (the while loop takes care of this).\nWe will use the ggmap package, which accesses the google maps api.\nA workaround is to set source = \"dsk\" (works for a limited number of queries)3.\nn.1kg \u0026lt;- n.1kg %\u0026gt;% mutate(purrr::map(.$location, geocode, source = \u0026quot;dsk\u0026quot;)) %\u0026gt;% tidyr::unnest() ## running into the inevitable QUERY LIMITS problems, lets use the approach from https://github.com/rladies/Map-RLadies-Growing n.1kg.withloc \u0026lt;- n.1kg %\u0026gt;% filter(!is.na(lon)) while(nrow(n.1kg.withloc) != nrow(n.1kg)) { # repeat this until there are no warnings() about QUERY LIMITS temp \u0026lt;- n.1kg %\u0026gt;% select(-lon, -lat) %\u0026gt;% anti_join(n.1kg.withloc %\u0026gt;% select(-lon, -lat)) %\u0026gt;% mutate(longlat = purrr::map(.$location, geocode, source = \u0026quot;dsk\u0026quot;)) %\u0026gt;% tidyr::unnest() %\u0026gt;% filter(!is.na(lon)) n.1kg.withloc \u0026lt;- n.1kg.withloc %\u0026gt;% bind_rows(temp) %\u0026gt;% distinct() } n.1kg \u0026lt;- n.1kg.withloc ## glue POP and `Population Description` together n.1kg \u0026lt;- n.1kg %\u0026gt;% mutate(pop.desc = paste0(POP, \u0026quot; : \u0026quot;, `Population Description`, \u0026quot; (\u0026quot;, SPOP, \u0026quot;)\u0026quot;)) ## given that only a number of geolocation are possible with the google API, this ## should probably stored out ## readr::write_csv(n.1kg, path = \u0026quot;1kg_sample_info_location.csv\u0026quot;) Create leaflet Map locations a world map with leaflet\n## if you have stroed the data in the previous chunk: ## readr::read_csv(\u0026quot;1kg_sample_info_location.csv\u0026quot;) Define shiny icons:\nicons \u0026lt;- awesomeIcons( icon = \u0026#39;user\u0026#39;, #people\u0026#39;, iconColor = \u0026#39;black\u0026#39;, library = \u0026#39;fa\u0026#39;, #ion markerColor = as.character(forcats::fct_recode(as.factor(n.1kg$SPOP), red = \u0026quot;EUR\u0026quot;, blue = \u0026quot;AFR\u0026quot;, green = \u0026quot;AMR\u0026quot;, gray = \u0026quot;EAS\u0026quot;, orange = \u0026quot;SAS\u0026quot;)) ## ok, thats not too pretty, but turns out, hex colors won\u0026#39;t work ) ## we need to create a vector that maps cols to SPOP from the markerColor argument above cols \u0026lt;- c(\u0026quot;#E50102\u0026quot;, \u0026quot;#00A9DD\u0026quot;, \u0026quot;#57BA1F\u0026quot;, \u0026quot;#575757\u0026quot;, \u0026quot;#FD8E00\u0026quot;) SPOP \u0026lt;- c(\u0026quot;EUR\u0026quot;, \u0026quot;AFR\u0026quot;, \u0026quot;AMR\u0026quot;, \u0026quot;EAS\u0026quot;, \u0026quot;SAS\u0026quot;) ## separate icon that will display the information ## ------------------------------------------------ icon.info \u0026lt;- awesomeIcons( icon = \u0026#39;info\u0026#39;, #people\u0026#39;, iconColor = \u0026#39;white\u0026#39;, library = \u0026#39;fa\u0026#39;, #ion markerColor = \u0026quot;white\u0026quot; ) Create map:\nm \u0026lt;- leaflet(data = n.1kg) %\u0026gt;% addTiles() %\u0026gt;% # Add default OpenStreetMap map tiles addAwesomeMarkers(lat=~lat, lng=~lon, label = ~htmltools::htmlEscape(pop.desc), icon = icons) %\u0026gt;% addAwesomeMarkers(lat=-45, lng=-107, popup = glue::glue(\u0026quot;Source: https://github.com/sinarueeger/map-1000genomes\u0026quot;), icon = icon.info) %\u0026gt;% ## this bit has potential to be displayed as a href. #glue::glue(\u0026quot;Source: {url.bitly} + {url.spop} (manual tidying)\u0026quot;), icon = icon.info) %\u0026gt;% addLegend(\u0026quot;bottomright\u0026quot;, colors =cols, labels= SPOP, opacity = 1) m # Print the map Save the map ## save to png ## ------------ mapview::mapshot(m, file = \u0026quot;map-1000genomes-populations.png\u0026quot;) ## save to hmtl ## ------------- htmlwidgets::saveWidget(m, file=\u0026quot;map-1000genomes-populations.html\u0026quot;) Reason for deviation from the original I mapped the populations according to the current location but coloured them according to ancestry.\nThis is a png and cannot be altered.↩\nSeen here: https://stackoverflow.com/questions/36175529/getting-over-query-limit-after-one-request-with-geocode.↩\nSee https://stackoverflow.com/questions/36175529/getting-over-query-limit-after-one-request-with-geocode.↩\n","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"5a079f2f8096ce6048f7aa7477164910","permalink":"https://sinarueeger.github.io/post/1kgmap/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/post/1kgmap/","section":"post","summary":"Goal Get started Data Add geographical coordinates Create leaflet Save the map Reason for deviation from the original This post provides the R-Code to map the 26 populations of the 1000 Genomes project.\nGoal Create a map similar to the one1 on the front page of http://www.internationalgenome.org/ in a reproducible manner.\nVersion on internationalgenome.org\nGet started Packages needed:\n## accessed via :: # library(mapview) # library(readxl) # library(readr) # library(purrr) # library(tidyr) # library(forcats) library(leaflet) library(dplyr) library(ggmap) ## for geocode, devtools::install_github(\u0026quot;dkahle/ggmap\u0026quot;) ggmap requires a google map api key2:","tags":["data visualisation","R","maps"],"title":"Create a map of the 1000 Genomes project reference populations","type":"post"},{"authors":[],"categories":null,"content":" ","date":1543924800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543924800,"objectID":"e2e2dbd61149f7b5ffc2983cea602799","permalink":"https://sinarueeger.github.io/talk/r-lunchs/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/r-lunchs/","section":"talk","summary":" ","tags":["workflow","best practices","R"],"title":"Workflow \u0026 best practices for projects","type":"talk"},{"authors":null,"categories":null,"content":" What is a “project folder”? Why now? Why we tidy up: authority and incentive Challenges What I want The options Drake Getting started More Examples Resources But wait: drake does not care about messy folders What is next? When is the right time to tidy Is it worth it? Recently, I started to seriously1 think about the tidiness of data analysis project folders and the implications of tidying up.\nI was lucky enough to talk about what I have figured out so far at the Genève R User Group. While I am not done yet with reflecting on this2, I wanted to write down my thoughts that lead to my presentation3. So what follows is just “thinking out loud”.\nUpdate: In February 2019, Amanda Dobbyn gave a talk at R-Ladies NYC about drake. All material here.\nTrailer. Your browser does not support the video tag. Presentation trailer made with Maëlle Salmon’s instructions. What is a “project folder”? To me, a project folder is anything that contains the (R-)scripts necessary to run a data analysis and create the corresponding report. It is like a framed piece of work that you can take and place somewhere else. And probably it will take the form of the Figure from the R4DS book below:\nAdapted from Figure in R4DS book. Ideally, you should be able to take that folder as it is, run it on another computer and get the same results. Unfortunately, this is not always the case - at least with my project folders.\nI think that the tidiness of a project folder, how it is structured and how it tells the user to execute what and when, correlate strongly with the whole repeatability, replicability and reproducibility aspect.\nWhy now? The reason I started to dig deeper into workflow management possibilities in R, is, that I was changing jobs, and I had to clean up my old project folders from almost five years of analysing genetic data 😱. And so I faced this gigantic mess a bit of a mess, spread over several servers and computers, some version controlled, others not, with implemented “best practices” from different waves of trying to improve. I tried to clean up as good as I could, but I told myself that this would not happen again. At my new job, I would use version control for everything, and I would use something make-like (e.g. remake) to indicate the “recipe” of a project and be in control of what is recomputed and what is not4.\nWhy we tidy up: authority and incentive I have a long-time interest in tidiness in general, and from studying my behaviour I came up with the theory that tidiness is only present when a) somebody tells you to do it, or b) you are rewarded for it.\nHere are some examples:\nIf you want to compile an R-package you have little to no freedom in how to name folders. You must have a given folder and file structure. Otherwise, it won’t compile. This dictated and unified folder structure makes it easy for R users to understand what is where in an R-package. No matter who coded it. R package structure. Figure from http://r-pkgs.had.co.nz/package.html. If you work on several different projects at the same time, it is beneficial to have structure, so that you can quickly dive back into a project.\nFollowing good practices also leaves you more time to do the fun stuff, like modelling and creating data visualisation.\nChallenges I started by wondering why maintaining a tidy and coherent folder structure was so difficult for me to maintain. So I came up with a list (which is undoubtedly going to change over time):\nHaving different places for computation (Laptop, Server1, Server2, …). Not using git consistently. Unclear separation of the folders data (raw input data), processed-data and output-data (results). Data deliveries: data hardly ever arrives in one tidy folder, but instead comes at different time points and so poses other challenges. Having many different best practices implemented: so each project would have its own set of folder names and file naming convention, leading to little overview of the analysis and its iteration steps → cleaning, modelling, visualisation, reports. Using similar code in many different R scripts → redundant code. Having no punishment for not cleaning up (and also not seeing the benefit). What I want Then I asked myself what I want to achieve with implementing (and sticking to) something new.\nMaking it easy for colleagues at work to rerun (and understand) the project → “repeatability” Making it easy for others to rerun and to understand the project → “reproducibility”5 Making it easy for others to rerun the code with different data → “replicability” Next, I looked for solutions. First, I would need to use coherent folder names. Second, I would need to have a file that indicates the recipe of an analysis. Third, I would implement most free floating and redundant code into functions. Fourth, I would minimise unnecessary computation by caching results. Fifth, I would start using unit tests6.\nThe options There are many different ready-to-use software packages out there. I was thinking of going back to using make, that I used years ago. Then I came across {remake}, which seemed just what I needed. A colleague at work was using stu and was recommending it. But then the Swiss Institute of Bioinformatics offered a course on Make-like declarative workflows with R taught by Kirill Müller, which I could not attend. Luckily, thanks to the excellent online course material, I could learn it by myself.\nDrake The presentation Make-like declarative workflows with R presented the R-package {drake} (drake = Data Frames in R for Make7).\n{Drake} was created by Will Landau and reviewed by rOpenSci. On the github page it says that {drake} is a “general-purpose workflow manager for data-driven tasks”. Sounds perfect!\nThe way I understand it is, that it is based on make (and overlaps with the R-package {remake}). Therefore when making a change to an analysis and re-running it, it only re-compute the dependent parts. But compared to make, {drake} is much more convenient to use. Plus it is scalable to parallel computing. And it is intuitive to use, meaning, colleagues can learn it quickly.\nGetting started Best is, to run the mini example provided in the package, and then go from there. Drake has many other examples provided; you can check them by running drake::drake_examples().\ninstall.packages(\"drake\") Run drake::drake_example(\"main\") → this will download a folder called main. Go to the terminal. You can look at all the files contained in main by writing tree main (this works on MacOS) main/ ├── COPYRIGHT.md ├── LICENSE.md ├── README.md ├── clean.R ├── make.R ├── raw_data.xlsx └── report.Rmd Next, open make.R. The key functions are drake_plan() and make(). Add the following bit before and after make(plan). config \u0026lt;- drake_config(plan) vis_drake_graph(config) Run all code for a first time. Change something (e.g. the plot function). Rerun and watch the colours change in vis_drake_graph(config). Use functions readd() and loadd() to work with the produced output. checkout .drake/ folder. This is where all the cached work is stored. By running this example, you will see that drake_plan() is used to create a recipe of the analysis and make() is used to execute that recipe. make() will create objects, such as fit and hist in the example and store them in the folder .drake/.\nreadd() is used to return an object from cache. This is handy when we only want to display an object. loadd() on the other hand is used to load an object into our session (similarly to load).\nMore To further checkout options I recommend - The slides from Christine Stawitz (presented at R-Ladies Seattle in June 2018). - The material by Amanda Dobbyn (presented at R-Ladies NYC in February 2019). (Update)\nBoth presentations provide a good overview of the options {drake} provides.\nExamples I also created some tiny examples that use genetic data. It has four folders:\nwild-west: this is how I was structuring folders till now (this example was used to introduce the analysis during the presentation). wild-west-pro: same as 1. but with an README.md. drake: implementing 1. into drake. drake-adv: implementing 1. into a more realistic, hierarchical folder structure. The examples use genetic data that was originally used in the crowdAI openSNP height prediction challenge. The full openSNP data set was prepared by my colleague Olivier Naret and can be downloaded here. The examples use a small subset of the entire dataset that can be downloaded here.\nResources Here are a bunch of resources that helped me understand {drake}:\nGithub Repo This tutorial and cheatsheet by Kirill Müller. Overview of options: Make-like declarative workflows with R by Christine Stawitz. Best practices for drake projects. Lots of tutorials and examples. But wait: drake does not care about messy folders True! I can have a make.R file anywhere and it will still work. But I believe that the shift in logic that you have to get used to with {drake} makes you care more about folder structure.\nWhat is next? I am currently reading the PlosCompBio paper Good enough practices in scientific computing - a great read, giving me lots of ideas!\nI want to use {drake} in a more complex setting. There are also other R-packages that help with project workflows. And I should invest some time to come up with a test suite for data analysis projects.\nWhen is the right time to tidy At the Genève RUG meetup, we were also discussing when we think is the right time to tidy up.\nProject folders evolve. Especially at the beginning of a project, we are busy figuring things out, wrangling data, fitting models, making plots and telling people what we found out. This can take some time. But at one point we are ready to write a report.\nIt is probably at that stage (when we write a report) that we can “frame” that project into something that is “stable” and “portable”.\nAlthough - I am not sure we have to wait that long. I think the benefits of {drake} (e.g. caching) already help us at an earlier stage.\nIs it worth it? I think there is a trade-off between dedicating days to tidying up and not caring about structure at all. Same with tooling. For example, if we use a tool, say make, but no one else but us knows how to use it, it is going to be hard for colleagues to understand and use project folders that use make. We have to keep that balance in mind.\nSeriously, meaning, different from previous, half-hearted attempts.↩\nJust started reading Good enough practices in scientific computing - great paper!↩\nThanks to Maëlle for pointing out that this is a good thing to do!↩\nAnd while at it, I would totally decrease my coffee consumption too and never procrastinate again 😉.↩\nThe terminology is really confusing at times. I rely on this definition. ↩\nThanks to my colleague for the idea!↩\nI am still wondering how “Data Frames in R for Make” adds up to “drake” 🤔.↩\n","date":1539043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539043200,"objectID":"6afbc2ff36dc45b7457de9036b143829","permalink":"https://sinarueeger.github.io/post/drake/","publishdate":"2018-10-09T00:00:00Z","relpermalink":"/post/drake/","section":"post","summary":"What is a “project folder”? Why now? Why we tidy up: authority and incentive Challenges What I want The options Drake Getting started More Examples Resources But wait: drake does not care about messy folders What is next? When is the right time to tidy Is it worth it? Recently, I started to seriously1 think about the tidiness of data analysis project folders and the implications of tidying up.","tags":["drake","R","projects"],"title":"Tidying workflows in R","type":"post"},{"authors":[],"categories":null,"content":" ","date":1538679600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538679600,"objectID":"7c868d5e4adaad351525337d14504726","permalink":"https://sinarueeger.github.io/talk/geneve-rug/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/geneve-rug/","section":"talk","summary":" ","tags":["workflow","R"],"title":"Tidying workflows \u0026 R community","type":"talk"},{"authors":[],"categories":null,"content":" ","date":1536777000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536777000,"objectID":"fd0549ad004f2a2079c743f9f3a0a32d","permalink":"https://sinarueeger.github.io/talk/geek-girls-carrots/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/geek-girls-carrots/","section":"talk","summary":" ","tags":["R","introduction"],"title":"An introduction to (problem solving with) R","type":"talk"},{"authors":["**R\u0026uuml;eger, S**","McDaid, A","Kutalik, Z"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"b4edcfb7d66f21452231eaaa94f14806","permalink":"https://sinarueeger.github.io/publication/plosgen-2018-ssimp-application/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/plosgen-2018-ssimp-application/","section":"publication","summary":"","tags":[""],"title":"Evaluation and application of summary statistic imputation to discover new height-associated loci","type":"publication"},{"authors":["**R\u0026uuml;eger, S**","McDaid, A","Kutalik, Z"],"categories":null,"content":"","date":1535760000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535760000,"objectID":"e364d4763049b6bb668db3250aa8d3c5","permalink":"https://sinarueeger.github.io/publication/biorxiv-2018-ssimp-method/","publishdate":"2018-09-01T00:00:00Z","relpermalink":"/publication/biorxiv-2018-ssimp-method/","section":"publication","summary":"","tags":[""],"title":"Improved imputation of summary statistics for admixed populations","type":"publication"},{"authors":null,"categories":null,"content":" Goal Getting it done 1. Get summary statistics Visualising associations Identify genomic region with lowest P-value 2. Extracting annotation biomaRt Learning resources Using biomaRt Quick d-tour: assembly GRCh37 or GRCh38? Extracting gene name for one SNP Extracting gene names for genomic region 3. Combining summary statistics and annotation Wouldn’t it be nice… Source In the world of genome-wide association studies (GWAS), we often get a list of genetic markers (SNPs) that seem for some reason relevant for a particular outcome. At the same time, we have little knowledge about these genetic variants that come in cryptic combinations of characters and numbers.\nFor example:\nWe might get asked how frequent the SNP rs1421085 is in a range of populations. We need to extract all known SNPs that are within 1 Mb of rs1421085. A genomic region turns out to be highly relevant for some disease, and we want to know all genes contained in that genomic region. Unless you sit on lots of genetic data, the list of SNPs come from a summarised form. Minimally, these summary statistic datasets contain the SNP identifier (SNPNAME), the effect size (beta) and the standard error (se). Sometimes the position (POS) and chromosome (CHR) is provided instead of the SNP identifier, and sometimes both are available. Then there is usually other information coming from the study data; for example, the allele frequency in the study this is shown on the LHS in the table below. However, hardly ever do the datasets contain annotation, such as the gene where the SNP resides, the phenotypes it is associated with or the minor allele frequency (MAF) in a specific population this is illustrated in the green coloured part in the table below.\nGWAS summary statistics dataset Annotation CHR POS SNPNAME REF ALT beta se Gene Linked phenotype Global MAF 7 75163169 rs1167827 A G 0.028 0.0032 HIP1 BMI 0.4720450 12 122781897 rs11057405 G A -0.021 0.0037 CLIP1 BMI 0.0327476 10 114758349 rs7903146 C T -0.031 0.0023 TCF7L2 BMI 0.2278350 1 49589847 rs657452 A G -0.015 0.0025 AGBL4 BMI 0.4532750 1 49589847 rs657452 A G -0.015 0.0025 AC099788.1 BMI 0.4532750 This makes sense. Genetic data from cohorts will be used for years and won’t change, but the annotation will change over the years. Take the chromosomal position of SNPs that changes with every new human genome reference assembly.\nTo get our hands on annotation, we need to consult external databases that are - lucky us! - public. But first, let us define what we want to see at the end of the blog post.\nGoal For this blog post, we keep it simple and only focus on the genes contained in a genomic region, but this can be easily extended to any other annotation available in databases.\nIn the illustration below1, we want to know the gene starting and end positions around one particular gene region to create an informative locuszoom plot.\nIn this case, we want to visualise the GWAS P-value (-log10(P-value)) of a genomic region (with each point representing a SNP), with the corresponding genes at the bottom. This is similar to a plot done with LocusZoom, a tool that takes summary statistics as input and outputs a pretty graph for a desired genomic region, including gene annotation, LD information and more. Getting it done There are currently more than 10 Mio SNPs known, and knowing their functions and genes by heart would equal to some superpower. Which is why we 2 public databases, such as dbSNP and ensembl3.\nWhat we like even more, is, to make our analyses reproducible and automate annotation and lookups.\nIn this blog post, I will show how to zoom into GWAS results and annotate the plot based on the information about that genomic region using R. Here is the plan:\nGet summary statistics Extract annotation Combine summary statistics and annotation Before tackling the first item, we want to have all R-packages installed \u0026amp; ready to use:\n## Packages needed to run code ## ---------------------------- # install.packages(\u0026quot;dplyr\u0026quot;) ## data manipulation # install.packages(\u0026quot;data.table\u0026quot;) ## read data, fast! # install.packages(\u0026quot;forcats\u0026quot;) ## dealing with factors # install.packages(\u0026quot;ggplot2\u0026quot;) ## dataviz # install.packages(\u0026quot;magrittr\u0026quot;) ## piping # install.packages(\u0026quot;metafolio\u0026quot;) ## colorpalette # install.packages(\u0026quot;skimr\u0026quot;) ## summarising data # install.packages(\u0026quot;qqman\u0026quot;) ## Manhattan plot # install.packages(\u0026quot;patchwork\u0026quot;) ## assembling plots # source(\u0026quot;https://bioconductor.org/biocLite.R\u0026quot;) # biocLite(\u0026quot;biomaRt\u0026quot;) ## annotation ## Optional packages for Rmd ## -------------------------- # install.packages(\u0026quot;kableExtra\u0026quot;) ## making pretty tables # install.packages(\u0026quot;devtools\u0026quot;) # devtools::install_github(\u0026quot;hadley/emo\u0026quot;) ## emojis # devtools::install_github(\u0026quot;ropenscilabs/icon\u0026quot;) ## icons 1. Get summary statistics First, we need some GWAS summary statistics.\nThere are lots of resources for publicly available GWAS summary statistics.\nWe will look at BMI, because it can be accessed easily4 and because it is relatively small for a genomic dataset. The data is from the Genetic Investigation of ANthropometric Traits (GIANT) consortium. You can download the dataset5 here or load it directly into R.\n## Data Source URL url \u0026lt;- \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/2/21/BMI_All_ancestry.fmt.gzip\u0026quot; #url \u0026lt;- \u0026quot;jenger.riken.jp/1analysisresult_qtl_download/All_2017_BMI_BBJ_autosome.txt.gz\u0026quot; ## Import BMI summary statistics dat.bmi \u0026lt;- read_tsv(file = url) ## ## taking too long, let\u0026#39;s use fread instead. dat.bmi \u0026lt;- data.table::fread(url, verbose = FALSE) I added verbose = FALSE because it will complain that there is an unexpected character in column 1, which appears to be numerical. This is because chromosome X will only appear towards the end of the dataset.\nNext, we rename some columns to something more conventional.\n## Rename some columns dat.bmi \u0026lt;- dat.bmi %\u0026gt;% rename(SNP = SNPNAME, P = Pvalue) Now, let’s look at the data with the skimr package.\nskimr::skim(dat.bmi) ## Skim summary statistics ## n obs: 246328 ## n variables: 10 ## ## ── Variable type:character ────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## ALT 0 246328 246328 1 1 0 4 ## CHR 0 246328 246328 1 2 0 23 ## ExAC_MAF 0 246328 246328 1 325 0 51423 ## GMAF 0 246328 246328 1 17 0 9964 ## REF 0 246328 246328 1 1 0 4 ## SNP 0 246328 246328 1 11 0 243206 ## ## ── Variable type:integer ──────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 p50 ## POS 0 246328 246328 7.6e+07 5.7e+07 11885 3.2e+07 6.1e+07 ## p75 p100 hist ## 1.1e+08 2.5e+08 ▇▇▅▅▃▂▁▁ ## ## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 ## beta 977 245351 246328 0.00051 0.13 -3.2 -0.032 ## P 977 245351 246328 0.48 0.3 8.6e-269 0.21 ## se 0 246328 246328 Inf NaN 0.002 0.023 ## p50 p75 p100 hist ## -0.00017 0.031 3.2 ▁▁▁▇▇▁▁▁ ## 0.47 0.73 1 ▇▇▆▆▆▆▆▆ ## 0.059 0.11 Inf ▇▁▁▁▁▁▁▁ The reference allele (REF), alternative allele (ALT), SNP identifier (SNP) and chromosome (CHR) are characters. There are four unique values for ALT and REF: A, C, G, T, and 23 unique values for CHR - seems about right. The two columns with the minor allele frequencies measured in GIANT and ExAC datasets (GMAF, ExAC_MAF) are characters too because the allele is concatenated. The chromosomal position (POS) is an integer. Then there is the actual association of each SNP with BMI (beta, se, P).\nThis dataset has 246328 rows and 10 columns. What if we want to visualise this all at once? In particular, we are interested if there are ANY associations between the genetic markers and BMI. The mini P distribution in the skimr output does not reveal much.\nVisualising associations Visualising all associations at once can be done with a Manhattan plot, where the x-axis represents chromosomeCHR and the chromosomal position POS, and the y-axis the -log10(P)-value. Let’s use the R-package qqman6 for that.\nqqman::manhattan(dat.bmi %\u0026gt;% mutate(CHR = as.numeric(as.character(fct_recode(CHR, \u0026quot;23\u0026quot; = \u0026quot;X\u0026quot;)))) %\u0026gt;% filter(-log10(P)\u0026gt;1), chr=\u0026quot;CHR\u0026quot;, bp=\u0026quot;POS\u0026quot;, snp=\u0026quot;SNP\u0026quot;, p=\u0026quot;P\u0026quot;, suggestiveline =FALSE, genomewideline = FALSE, chrlabs = c(1:22, \u0026quot;X\u0026quot;), cex = 0.4) We can spot immediately, that there are loads of SNPs with P-values smaller than \\(10^{-100}\\) (sample size was around 700K).\nOf course, there are many other solutions to spot real associations, but this is not the point of this blog post ;-).\nIdentify genomic region with lowest P-value Now that we know that there are lots of genetic markers associated with BMI, we want to look at a specific genomic region and figure out what genes it contains. For illustrative purposes, we pick the genomic region with the lowest P-value.\ndat.bmi.sel \u0026lt;- dat.bmi %\u0026gt;% slice(which.min(P)) dat.bmi.sel ## CHR POS REF ALT SNP GMAF ExAC_MAF beta se P ## 1 16 53800954 T C rs1421085 C:0.2286 - 0.078 0.0022 8.6e-269 SNP identifier rs1421085 that has the lowest P-value (\\(P = 8.6\\times 10^{-269}\\)).\nNow we can visualise the summary statistics of that genomic region (\\(\\pm 500 \\cdot 10^{3}\\)).\nrange \u0026lt;- 5e+05 sel.chr \u0026lt;- dat.bmi.sel$CHR sel.pos \u0026lt;- dat.bmi.sel$POS dat.bmi.sel.region \u0026lt;- dat.bmi %\u0026gt;% filter(CHR == sel.chr, between(POS, sel.pos - range, sel.pos + range)) p1 \u0026lt;- ggplot(data = dat.bmi.sel.region) + geom_point(aes(POS, -log10(P)), shape = 1) + labs(title = \u0026quot;Locuszoomplot for BMI GWAS\u0026quot;, subtitle = paste(\u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot;), caption = paste(\u0026quot;Data source:\u0026quot;, url)) print(p1) Next, we want to know if rs1421085 is part of a gene, and if yes, which one.\n2. Extracting annotation biomaRt Thankfully, there is an R-package called biomaRt that can do this for us.\nI had only come across this package a few weeks ago, so I apologise in advance that I won’t make full use of all the features that the package offers.\nbiomaRt should not be confused with biomartr. biomartr is an rOpenSci package by Hajk-Georg Drost. I needed this tweet to realise that these were two separate yet related R-packages.\nDid you ever want to reproducibly retrieve thousands of genomes across the tree of life using only one R command? Then have a look at the new version of biomartr which is on its way to CRAN! https://t.co/kWF5XCoGhj #bioinformatics #rstats #Genomics pic.twitter.com/EZHJP0n1f9 — Hajk-Georg Drost (@HajkDrost) June 28, 2018 So far, I did not fully grasp the benefits of biomartr compared to biomaRt for annotation of human data. But I will look more into it.\nI also got a tip from Marianna Foos to check out rsnps, an R-package dealing with SNP annotation.\nLearning resources I used mainly two sources to learn what I wanted to do:\nVignette of biomaRt on Bioconductor7. This question on StackOverflow. Using biomaRt First, we need to load the biomaRt package from Bioconductor.\nlibrary(biomaRt) Next, we specify which database to use. You can use the functions listMart(), listEnsembl() and listDatasets() to select from the right biomart and dataset. We will need to extract SNPs, hence biomart = \"snp\".\nsnp.ensembl \u0026lt;- useEnsembl(biomart = \u0026quot;snp\u0026quot;, dataset = \u0026quot;hsapiens_snp\u0026quot;) class(snp.ensembl) ## [1] \u0026quot;Mart\u0026quot; ## attr(,\u0026quot;package\u0026quot;) ## [1] \u0026quot;biomaRt\u0026quot; # other ways of selecting the mart snp.mart \u0026lt;- useMart(biomart = # \u0026#39;ENSEMBL_MART_SNP\u0026#39;, dataset=\u0026#39;hsapiens_snp\u0026#39;) # gene.mart \u0026lt;- useMart(\u0026#39;ensembl\u0026#39;, dataset=\u0026#39;hsapiens_gene_ensembl\u0026#39;) Last, we extract the gene id to which our SNP belongs using the function getBM(). Along with that, we also extract other information, like the minor allele frequency minor_allele_freq. To check which attributes and filters are available, run listAttributes(snp.ensembl) and listFilters(snp.ensembl).\nout.bm \u0026lt;- getBM( attributes = c(\u0026quot;ensembl_gene_stable_id\u0026quot;, \u0026quot;refsnp_id\u0026quot;, \u0026quot;chr_name\u0026quot;, \u0026quot;chrom_start\u0026quot;, \u0026quot;chrom_end\u0026quot;, \u0026quot;minor_allele\u0026quot;, \u0026quot;minor_allele_freq\u0026quot;), # \u0026quot;ensembl_transcript_stable_id\u0026quot;, # \u0026quot;consequence_type_tv\u0026quot;), filters = \u0026quot;snp_filter\u0026quot;, values = \u0026quot;rs1421085\u0026quot;,#dat.bmi.sel$SNP, mart = snp.ensembl) This gives us - as chosen in attributes - the gene identifier, the SNP identifier, the chromosome name, chromosomal position (start + end), minor allele and minor allele frequency. The output in out.bm corresponds to this webpage entry on the ensembl webpage.\nout.bm ## ensembl_gene_stable_id refsnp_id chr_name chrom_start chrom_end ## 1 ENSG00000140718 rs1421085 16 53767042 53767042 ## minor_allele minor_allele_freq ## 1 C 0.228634 Quick d-tour: assembly GRCh37 or GRCh38? Before getting the gene names, we want to check if the positions in the dataset are from assembly GRCh37 or GRCh38. This is a handy thing because often only SNP identifiers are reported. Or SNP identifiers are reported, but with positions on a different assembly.\nifelse(sel.pos == out.bm$chrom_start, \u0026quot;\\u2713: same assembley (GRCh38)\u0026quot;, \u0026quot;\\u2717: not the same assembley\u0026quot;) ## [1] \u0026quot;✗: not the same assembley\u0026quot; The position is not matching, because the databases that we are looking at is based on the most recent human assembly GRCh38, but the BMI summary statistics dataset is based on human assembly GRCh37. The command listEnsemblArchives() will list you the URLs needed to get access to an archived assembly. So let’s pull out the archived GRCh37 version with the argument host = 'http://grch37.ensembl.org'.\nsnp.ensembl.grch37 \u0026lt;- useMart(host=\u0026#39;http://grch37.ensembl.org\u0026#39;, biomart=\u0026#39;ENSEMBL_MART_SNP\u0026#39;, dataset=\u0026#39;hsapiens_snp\u0026#39;) out.bm.grch37 \u0026lt;- getBM( attributes = c(\u0026#39;ensembl_gene_stable_id\u0026#39;, \u0026#39;refsnp_id\u0026#39;, \u0026#39;chr_name\u0026#39;, \u0026#39;chrom_start\u0026#39;, \u0026#39;chrom_end\u0026#39;, \u0026#39;minor_allele\u0026#39;, \u0026#39;minor_allele_freq\u0026#39;), filters = \u0026#39;snp_filter\u0026#39;, values = dat.bmi.sel$SNP, mart = snp.ensembl.grch37 ) out.bm.grch37 ## ensembl_gene_stable_id refsnp_id chr_name chrom_start chrom_end ## 1 ENSG00000140718 rs1421085 16 53800954 53800954 ## minor_allele minor_allele_freq ## 1 C 0.228634 Let’s check again.\nifelse(sel.pos == out.bm.grch37$chrom_start, \u0026quot;\\u2713: same assembley (grch37)\u0026quot;, \u0026quot;\\u2717: not the same assembley\u0026quot;) ## [1] \u0026quot;✓: same assembley (grch37)\u0026quot; Flavia Hodel pointed out that adding the argument GRCh = 37 works too! That’s a handy argument (but limited to GRCh versions 37 and 38).\nsnp.ensembl.grch37.alt \u0026lt;- useEnsembl(biomart = \u0026quot;snp\u0026quot;, dataset = \u0026quot;hsapiens_snp\u0026quot;, GRCh = 37) out.bm.grch37 \u0026lt;- getBM( attributes = c(\u0026#39;ensembl_gene_stable_id\u0026#39;, \u0026#39;refsnp_id\u0026#39;, \u0026#39;chr_name\u0026#39;, \u0026#39;chrom_start\u0026#39;, \u0026#39;chrom_end\u0026#39;, \u0026#39;minor_allele\u0026#39;, \u0026#39;minor_allele_freq\u0026#39;), filters = \u0026#39;snp_filter\u0026#39;, values = dat.bmi.sel$SNP, mart = snp.ensembl.grch37.alt ) out.bm.grch37 ## ensembl_gene_stable_id refsnp_id chr_name chrom_start chrom_end ## 1 ENSG00000140718 rs1421085 16 53800954 53800954 ## minor_allele minor_allele_freq ## 1 C 0.228634 ifelse(sel.pos == out.bm.grch37$chrom_start, \u0026quot;\\u2713: same assembley (grch37)\u0026quot;, \u0026quot;\\u2717: not the same assembley\u0026quot;) ## [1] \u0026quot;✓: same assembley (grch37)\u0026quot; Extracting gene name for one SNP Next, we want to get the gene name where rs1421085 falls into.\nLet’s check which attributes contain the string gene.\nlistAttributes(snp.ensembl) %\u0026gt;% slice(str_which(name, \u0026quot;gene\u0026quot;)) ## name description page ## 1 associated_gene Associated gene with phenotype snp ## 2 ensembl_gene_stable_id Gene stable ID snp Now use ensembl_gene_stable_id and associated_gene as additional attributes.\n## extract gene ## ---------- out.bm.snp2gene \u0026lt;- getBM( attributes = c(\u0026#39;refsnp_id\u0026#39;, \u0026#39;allele\u0026#39;, \u0026#39;chrom_start\u0026#39;, \u0026#39;chr_name\u0026#39;, \u0026#39;ensembl_gene_stable_id\u0026#39;), filters = c(\u0026#39;snp_filter\u0026#39;), values = dat.bmi.sel$SNP, mart = snp.ensembl) out.bm.snp2gene ## refsnp_id allele chrom_start chr_name ensembl_gene_stable_id ## 1 rs1421085 T/C 53767042 16 ENSG00000140718 ## Attribute `associated_gene` is `Associated gene with phenotype`. ## Extract string ## ---------- gene.ensembl \u0026lt;- useEnsembl(biomart = \u0026quot;ensembl\u0026quot;, dataset = \u0026quot;hsapiens_gene_ensembl\u0026quot;, GRCh = 37) # we will need an additional mart for genes ## because we are using positions from GRCh = 37 in a next query, we need to pass that information on. out.bm.gene \u0026lt;- getBM(attributes = c(\u0026#39;external_gene_name\u0026#39;), filters = c(\u0026#39;ensembl_gene_id\u0026#39;), values = unique(out.bm.snp2gene$ensembl_gene_stable_id), mart = gene.ensembl) out.bm.gene ## external_gene_name ## 1 FTO The gene that contains SNP rs1421085 is called FTO.\nExtracting gene names for genomic region So now we know that rs1421085 is part of FTO. But where does FTO start and end? And what are the genes nearby? For this purpose, we want to visualise the summary statistics of the full genomic region (\\(\\pm 250 \\cdot 10^3\\) Mb). We just recycle the previous code, but instead of providing a SNP-id, we provide the chromosome, the start and the end position.\nout.bm.genes.region \u0026lt;- getBM( attributes = c(\u0026#39;start_position\u0026#39;,\u0026#39;end_position\u0026#39;,\u0026#39;ensembl_gene_id\u0026#39;,\u0026#39;external_gene_name\u0026#39;, \u0026#39;gene_biotype\u0026#39;), filters = c(\u0026#39;chromosome_name\u0026#39;,\u0026#39;start\u0026#39;,\u0026#39;end\u0026#39;), values = list(sel.chr, sel.pos - range, sel.pos + range), mart = gene.ensembl) head(out.bm.genes.region) ## start_position end_position ensembl_gene_id external_gene_name ## 1 53088945 53363062 ENSG00000177200 CHD9 ## 2 53332136 53333704 ENSG00000261056 RP11-454F8.2 ## 3 53368474 53368576 ENSG00000238645 snoU13 ## 4 53371365 53371483 ENSG00000202193 RNA5SP427 ## 5 53395931 53397590 ENSG00000259962 RP11-44F14.4 ## 6 53398894 53406995 ENSG00000260078 RP11-44F14.1 ## gene_biotype ## 1 protein_coding ## 2 pseudogene ## 3 snoRNA ## 4 rRNA ## 5 pseudogene ## 6 pseudogene We can plot out.bm.genes.region with a line range plot, where each horizontal line represents one gene.\n## rank gene names according to start position out.bm.genes.region \u0026lt;- out.bm.genes.region %\u0026gt;% mutate(external_gene_name = fct_reorder(external_gene_name, start_position, .desc = TRUE)) ## plot ggplot(data = out.bm.genes.region) + geom_linerange(aes(x = external_gene_name, ymin = start_position, ymax = end_position)) + coord_flip() + ylab(\u0026quot;\u0026quot;) Let’s try to make that pretty. We can group the genes by gene_biotype and colour them accordingly. And we move the protein-coding genes to the top row and colour it black.\n## define plot range for x-axis plot.range \u0026lt;- c(min(sel.pos - range, out.bm.genes.region$start_position), max(sel.pos + range, out.bm.genes.region$end_position)) ## rank gene_biotype label out.bm.genes.region \u0026lt;- out.bm.genes.region %\u0026gt;% mutate(gene_biotype_fac = fct_relevel(as.factor(gene_biotype), \u0026quot;protein_coding\u0026quot;), external_gene_name = fct_reorder2(external_gene_name, start_position, gene_biotype_fac, .desc = TRUE)) ## plot p2 \u0026lt;- ggplot(data = out.bm.genes.region) + geom_linerange(aes(x = external_gene_name, ymin = start_position, ymax = end_position, colour = gene_biotype_fac, group = gene_biotype_fac)) + coord_flip() + ylab(\u0026quot;\u0026quot;) + ylim(plot.range) + geom_text(aes(x = external_gene_name, y = start_position, label = external_gene_name, colour = gene_biotype_fac), fontface = 2, alpha = I(0.7), hjust = \u0026quot;right\u0026quot;, size= 2.5) + labs(title = \u0026quot;\u0026quot;, subtitle = paste0(\u0026quot;Genes\u0026quot;), caption = paste0(\u0026quot;Data source: \u0026quot;, gene.ensembl@host, \u0026quot; + Data set: \u0026quot;, gene.ensembl@dataset), color = \u0026quot;Gene Biotype\u0026quot;) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), strip.text.y = element_text(angle = 0), legend.position=\u0026quot;bottom\u0026quot;, panel.grid.major.y = element_blank()) + expand_limits(y=c(-1, 1)) + scale_color_manual(values = c(\u0026quot;black\u0026quot;, metafolio::gg_color_hue(nlevels(out.bm.genes.region$gene_biotype_fac)-1))) ## hack to have 11 colors, but probably merging some gene biotype groups could make sense too. ## consider colorblindr::palette_OkabeIto_black print(p2) Some short genes are starting with AC and LINCO. We can check what they are and/or consult a biologist.\n3. Combining summary statistics and annotation Now we are ready to combine plot p1 and p2.\nlibrary(patchwork) p1b \u0026lt;- p1 + xlab(\u0026quot;\u0026quot;) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) + xlim(plot.range) p1b + p2 + plot_layout(ncol = 1, heights = c(6, 6)) Having both plots combined, we can spot that there are six protein-coding genes in the region. But the P-value peak is located in gene FTO.\nIf this were work done for a real project, this would now be the time to get back to domain experts, discuss the results and figure out what other annotation they need8.\nWouldn’t it be nice… … to polish that plot even more? For starters, the colour code is not ideal… … to add information about the correlation between SNPs, like in LocusZoom plots? … or have an interactive plot9: hovering over the SNP points would light up the corresponding genes or give some other information, like the allele frequency. Totally 😉 Seems like great material for future blog posts!\nSource The R Markdown file is here. Some more biomaRt code snippets are in a gist. If you know another R-package to solve a similar problem or have feedback, you can comment below 👇.\nThanks to Maëlle Salmon you get a real plot here instead of a blurry hand drawing 😉 She gave lots of feedback on my first version - much appreciated!↩\nAwesome icons in R? checkout these instructions by Mitchell O’Hara-Wild on the rOpenSci webpage.↩\nThanks to my colleagues with biology background for explaining me the differences between these databases and what a biomart is!↩\nInitially, I wanted to use data from two recent studies by the Psychiatric Genomics Consortium (PGC) on schizophrenia (SCZ) and bipoloar disorder (BD), as well as Major depressive disorder (MDD). Like most consortia, PGC provides summary statistics that can be downloaded. However, before downloading anything, the user needs to acknowledge and agree to a list of conditions - which I think is an excellent approach! - therefore we cannot directly load it into R.↩\nData source: Yengo et al. (2018).↩\nA guide to crafting Manhattan plots by Yan Holtz for the R graph gallery↩\nI recently listened to a podcast by Saskia Freytag and NJ Tierney where they talk about the differences between CRAN and Bioconductor (there are many!). The podcast is called Credibly Curious.↩\n+1 for reproducibility!↩\nLiza Darrous pointed out the interactive Manhattan R function manhanttanly.↩\n","date":1532908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532908800,"objectID":"878cf620f59faea12ff979b4a1095038","permalink":"https://sinarueeger.github.io/post/locuszoomplot/","publishdate":"2018-07-30T00:00:00Z","relpermalink":"/post/locuszoomplot/","section":"post","summary":"Goal Getting it done 1. Get summary statistics Visualising associations Identify genomic region with lowest P-value 2. Extracting annotation biomaRt Learning resources Using biomaRt Quick d-tour: assembly GRCh37 or GRCh38? Extracting gene name for one SNP Extracting gene names for genomic region 3. Combining summary statistics and annotation Wouldn’t it be nice… Source In the world of genome-wide association studies (GWAS), we often get a list of genetic markers (SNPs) that seem for some reason relevant for a particular outcome.","tags":["statistical genetics","R","data visualisation"],"title":"Locuszoom plot of GWAS summary statistics","type":"post"},{"authors":null,"categories":null,"content":"This is a brief write-up of my satRdays Cardiff experience.\nFirst - what is a satRday?\nIt is an awesome concept: attending an R conference organised by a local RUG on a Saturday.\nThe programme in Cardiff had parallel sessions - tough decision-making to pick between promising talks!\ndplyr workshop Kathrine Tansey kept us busy with a workshop on dplyr in the morning.\n💻 rstudio-cloud project\nPackaging workshop Heather Turner upgraded us on packaging.\n💡 useful tips and workflow! Eager to apply what I learned.\n💻 rstudio-cloud project\n➡️ Slides\nIntroduction to tidytext Textmining with Nujcharee (เป็ด), applied to #metoo twitter data:\nPackage reviews with rOpenSci Maëlle Salmon presented the review process of rOpenSci and then reviewed the review process using the force of textmining.\n💡 Rigorous, but friendly reviewing process at rOpenSci.\nMore info on submitting a package for review here.\n➡️ Slides\nAirtable \u0026amp; R Amy McDougall presented airtable (💡!) and the R interface airtabler.\nAmy wrote a blogpost on this topic too (which I found through Locke Data\u0026rsquo;s write up here).\n➡️ Slides\nLightening talks Counting and weighing Penguins Philipp Boersch-Supan telling us about the challenges of counting penguins (and apparently Rcpp helps).\nIntegrating command-line tools with R Erle Holgersen: embrace the system() function for command-line snippets in R.\nFor example:\nls.directory \u0026lt;- system(\u0026#34;ls -all\u0026#34;, intern = TRUE) 💡 intern = TRUE is my new friend!\nOther tips:\nuse the command line functionality in data.table::fread(\u0026quot;\u0026quot;) make use of tempfile() git in five Steph Locke gave git in a nutshell.\n💡 Recommendation: use GitKraken as a GUI client.\ntidy eval Nic Crane explaining the basics of tidy eval and when it is needed (💡 functions!).\nA simple Bayesian workflow Paul Robinson presented his Bioconductor package dealing with proteins.\n➡️ Slides\nR-Forwards and R-Ladies Remote Heather Turner presented two #rstats related initiatives.\nR Forwards is an initiative to widen the participation of under-represented groups ➡️ looking for volunteers that take on tasks.\nR-Ladies remote chapter: has monthly coffee breaks on slack!\nFinal remarks 👏 Kudos to the organisers! They paid lots of attention to details and did an awesome jexcellentaking everyone feel welcome!\n✍️ Mental note to self: I should probably learn how to take pics at the right time + live tweet 😉\nFurther information 📁 Slides repo.\n📘 Check out Maëlle\u0026rsquo;s blogpost on Sto rrrify #satRdayCDF 2018.\n📘 Loads of info from the Locke Data team members.\n#satRdayCDF on Twitter.\n⏭️ Next satRday is on September 1 2018 in Amsterdam.\n📅 Check all upcoming events.\nAlso 👇\n","date":1530662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530662400,"objectID":"7af74600ae0107467cf199df6c223a76","permalink":"https://sinarueeger.github.io/post/satrdaycardiff/","publishdate":"2018-07-04T00:00:00Z","relpermalink":"/post/satrdaycardiff/","section":"post","summary":"This is a brief write-up of my satRdays Cardiff experience.\nFirst - what is a satRday?\nIt is an awesome concept: attending an R conference organised by a local RUG on a Saturday.\nThe programme in Cardiff had parallel sessions - tough decision-making to pick between promising talks!\ndplyr workshop Kathrine Tansey kept us busy with a workshop on dplyr in the morning.\n💻 rstudio-cloud project\nPackaging workshop Heather Turner upgraded us on packaging.","tags":["conference","R","satRday","rOpenSci","package","dplyr"],"title":"satRday Cardiff 2018","type":"post"},{"authors":null,"categories":null,"content":"I work (broadly speaking) in epidemiology. Within collaborations we often have to share sensitive data across institutions and are therefore likely to not share IT facilities. But the most often used options - bare e-mail or file hosting - are not secure, as they both work via a server that could potentially be exposed.\nThere are a handful of secure options around (ProtonMail or keybase.io, both working via encryption), but for those that trust open source projects the most and are familiar with a terminal I have written some basic instructions for asymmetric GPG encryption.\nI should say that I am not an encryption expert at all. But, although there is a lot of talk about data protection, I never came across a compact, easy-to-follow instruction for the sender of the document and the recipient, valid for all three operating systems (Linux, Mac, Windows). Therefore, I tried to write one, mainly for myself and collaborators. Feedback is appreciated!\n","date":1523644859,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523644859,"objectID":"dbca7dfcf69c05fd04ce13b346a6aeb2","permalink":"https://sinarueeger.github.io/post/2018-04-04-encryption-of-files/","publishdate":"2018-04-13T20:40:59+02:00","relpermalink":"/post/2018-04-04-encryption-of-files/","section":"post","summary":"I work (broadly speaking) in epidemiology. Within collaborations we often have to share sensitive data across institutions and are therefore likely to not share IT facilities. But the most often used options - bare e-mail or file hosting - are not secure, as they both work via a server that could potentially be exposed.\nThere are a handful of secure options around (ProtonMail or keybase.io, both working via encryption), but for those that trust open source projects the most and are familiar with a terminal I have written some basic instructions for asymmetric GPG encryption.","tags":["encryption","best practices","workflow"],"title":"Getting started with encryption of documents","type":"post"},{"authors":["**R\u0026uuml;eger, S**","Bochud, P-Y","Dufour, J-F","M\u0026uuml;llhaupt, B","Semela, D","Heim, M H","Moradpour, D","Cerny, A","Malinverni, R","Booth, D R","Suppiah, V","George, J","Argiro, L","Halfon, P","Bourli\u0026egrave;re, M","Talal, A H","Jacobson, I M","Patin, E","Nalpas, B","Poynard, T","Pol, S","Abel, L","Kutalik, Z","Negro, F"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"8089d020d4b95ead47a18ea77aaa9b9a","permalink":"https://sinarueeger.github.io/publication/gut-2015-fibrosis/","publishdate":"2015-09-01T00:00:00Z","relpermalink":"/publication/gut-2015-fibrosis/","section":"publication","summary":"","tags":[],"title":"Impact of common risk factors of fibrosis progression in chronic hepatitis C","type":"publication"}]