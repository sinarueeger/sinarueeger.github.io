[{"authors":["admin"],"categories":null,"content":"I am part of the FinnGen analysis team, where I analyse large-scale genome and phenotype data. My interests include statistical modelling, statistical genetics, programming with R, reproducible analysis workflows, and topics intersecting health, medicine and technology.\nI received my PhD in Life Sciences in 2018 from the University of Lausanne, where I worked on the imputation of GWAS summary statistics supervised by Zoltán Kutalik. What I enjoy the most about applied statistics is the interdisciplinary aspect of collaborations that allows me to glimpse into different domains.\nIn my spare time, I enjoy exploring nature on a bike and contributing to open-source projects.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"https://sinarueeger.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I am part of the FinnGen analysis team, where I analyse large-scale genome and phenotype data. My interests include statistical modelling, statistical genetics, programming with R, reproducible analysis workflows, and topics intersecting health, medicine and technology.\nI received my PhD in Life Sciences in 2018 from the University of Lausanne, where I worked on the imputation of GWAS summary statistics supervised by Zoltán Kutalik. What I enjoy the most about applied statistics is the interdisciplinary aspect of collaborations that allows me to glimpse into different domains.","tags":null,"title":"Sina Rüeger","type":"author"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   ","date":1567681200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567681200,"objectID":"9a7dc40750fb0851cdfa1304bab0c88d","permalink":"https://sinarueeger.github.io/talk/robust-data-analysis/","publishdate":"2017-01-01T00:00:00+02:00","relpermalink":"/talk/robust-data-analysis/","section":"talk","summary":" Click on the Slides button above to view the built-in slides feature.   ","tags":["best practices","R"],"title":"Robust data analysis - an introduction to R","type":"talk"},{"authors":null,"categories":null,"content":"This is the course material to the workshop Robust data analysis: an introduction to R held at the Open Science in Practice Summer School 2019 at EPFL, Switzerland, and taught with the help of Allie Burns.\nHere are the slides and the workshop handbook.\nI drew inspiration for the workshop from the following material:\n Let them have cake first by Mine CetinkayaRundel. Ten quick tips for teaching programming by Neil Brown and Greg Wilson. The beautiful illustrations by Allison Horst. Keynote at #useR2019 + mindset by Julia Stewart Lowndes.  ","date":1567544400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567544400,"objectID":"ef1a3aedaf0659e9447e76e403cdcecf","permalink":"https://sinarueeger.github.io/project/robust-data-analysis/","publishdate":"2019-09-04T00:00:00+03:00","relpermalink":"/project/robust-data-analysis/","section":"project","summary":"Course material for R introduction workshop held at [Open science in practice](http://osip2019.epfl.ch/) summer school.","tags":["R"],"title":"Robust data analysis - an introduction to R","type":"project"},{"authors":null,"categories":null,"content":" The 3+ days at useR!2019 in Toulouse were packed with great talks1 and good food - hence the amuse-bouches word play.\nHere are some R code bits from the conference. Hopefully convincing enough to start using a new package or change a workflow. Not everything was brand-new, but it was helpful to have someone talking through their inspiration and examples.\nCheck out the speakers’ materials - soon there will be recordings too. Some of the examples are also copied straight from the speakers’ slide decks.\nTidy eval usethis pak Reshaping data vroom data.table rray  1. tidy eval Speaker: Lionel Henry (Slides)\nI never warmed up to the bang-bangs and enquo’s. Hence the new and more straight forward {{ }} (read: curly curly) for functional programming {{ arg }} in the tidyverse feel like a game-changer.\nFor those more familiar with the previous framework: {{ arg }} is a shortcut for !!enquo(arg).\ndplyr example Let’s say you have a dataset, here iris, and you want to compute the average Petal.Length for each Species:\nlibrary(dplyr) ## ## Attaching package: \u0026#39;dplyr\u0026#39; ## The following objects are masked from \u0026#39;package:stats\u0026#39;: ## ## filter, lag ## The following objects are masked from \u0026#39;package:base\u0026#39;: ## ## intersect, setdiff, setequal, union iris %\u0026gt;% group_by(Species) %\u0026gt;% summarise(avg = mean(Petal.Length, na.rm = TRUE)) ## # A tibble: 3 x 2 ## Species avg ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; ## 1 setosa 1.46 ## 2 versicolor 4.26 ## 3 virginica 5.55 You can use the “curly-curly” brackets if you want to turn this small bit of code into a function group_mean() with a data, by and var argument2 (and want to pass the variables on in an unquoted way):\ngroup_mean \u0026lt;- function(data, by, var) { data %\u0026gt;% group_by({{ by }}) %\u0026gt;% summarise(avg = mean({{ var }}, na.rm = TRUE)) } We can then apply group_mean() to any dataset that has a grouping and a continuous variable, for example, the mammals sleep dataset in ggplot2:\nlibrary(ggplot2) group_mean(data = msleep, by = vore, var = sleep_total)  ggplot2 example Another common tidy eval application is ggplot2. In the example below, we want a customised plot: a scatterplot with a geom_smooth on top of it.\nlibrary(ggplot2) theme_set(theme_bw()) ggplot(data = iris, aes(x = Sepal.Length, y = Petal.Length, group = Species, color = Species) ) + geom_point() + geom_smooth(method = \u0026quot;lm\u0026quot;) + ggtitle(\u0026quot;Pack this plot into a function.\u0026quot;) Again, we can wrap the “curly-curly” brackets around the arguments and apply them to a different dataset.\nplot_point_smooth \u0026lt;- function(data, x, y, gr = NULL, method = \u0026quot;lm\u0026quot;) { ggplot(data = data, aes({{ x }}, {{ y }}, group = {{ gr }}, color = {{ gr }}) ) + geom_point() + geom_smooth(method = method) } plot_point_smooth(msleep, x = sleep_total, y = sleep_rem, gr = NULL) + ggtitle(\u0026quot;Tidy eval with the msleep dataset\u0026quot;)   2. usethis Speaker: Jenny Bryan (Slides + Material + Demo)\n# install.packages(\u0026quot;usethis\u0026quot;) library(usethis) Once upon a time, there was the package devtools. Then devtools became too large, and now the usethis package is taking over some of the convenience functions for workflows.\nusethis is all about avoiding to copy+pasting. For example, there is a function to edit the .Rprofile called usethis::edit_r_profile(). Whenever there is a slightly complicated task ahead (say restarting R), the usethis package will talk you through the whole process.\nThere are lots of use_* to add or modify something to/in a project/package and three functions to create a package, a project or a github fork:\ncreate_package() create_project() create_from_github()  Create a package If you want to create a package, do the following (see also screencast):\n## 1. create the package skeleton create_package(\u0026quot;~/tmp/mypackage\u0026quot;) ## 2. use git use_git() ## 3. add a license use_mit_license() ## 4. run check # install.packages(\u0026quot;devtools\u0026quot;) devtools::check() ## 5. commit all files with git ## 6. set up git + github use_github() ## will update the DESCRIPTION file ## 7. install the package devtools::install() ## 8. add a rmarkdown readme file use_readme_rmd() ## knit + commit + push ## 9. clean up if this was only a demo ## install.packages(\u0026quot;fs\u0026quot;) ## fs::dir_delete(\u0026quot;~/tmp/mypackage\u0026quot;)   3. pak Speaker: Gábor Csárdi (Slides)\n# install.packages(\u0026quot;pak\u0026quot;) ## or # devtools::install_github(\u0026quot;r-lib/pak\u0026quot;) It seems like pak will make package installation - conventional and for projects - more intuitive. Before installing anything, pak will give you a heads up on what will be installed or if there are any conflicts.\npak has two main functions: pak::pkg_* and pak:::proj_*\nConventional package installation Play around with usethis3 and see what happens:\npak::pkg_install(\u0026quot;usethis\u0026quot;) pak::pkg_remove(\u0026quot;usethis\u0026quot;) pak::pkg_install(\u0026quot;r-lib/usethis\u0026quot;) pak::pkg_status(\u0026quot;usethis\u0026quot;)  Package installation for projects First, create a project with usethis, then install R packages directly into the project.\nusethis::create_project(\u0026quot;~/tmp/test\u0026quot;) ## check the directory dir() ## initialise a dedicated R packages folder pak:::proj_create() ## check the directory again dir() ## check the DESCRIPTION file readLines(\u0026quot;DESCRIPTION\u0026quot;) ## install usethis pak:::proj_install(\u0026quot;usethis\u0026quot;) ## this installs dependencies into a private project library readLines(\u0026quot;DESCRIPTION\u0026quot;) ## remove the project folder again fs::dir_delete(\u0026quot;~/tmp/test\u0026quot;)   4. Reshaping data Speaker: Hadley Wickham (Demo)\n# install.packages(\u0026quot;tidyverse/tidyr\u0026quot;) library(tidyr) What a history reshaping data in R already has! From reshape to melt + cast, over to gather + spread and now pivot_long + pivot_wide. Reshaping data stays a mind-bending task, but hopefully, these pivot_* functions will make life easier.\n# devtools::install_github(\u0026quot;chrk623/dataAnim\u0026quot;) # Master\u0026#39;s Thesis project by Charco Hui library(dataAnim) ## Our two toy datasets datoy_wide ## Name English Maths ## 1 Ben 19.0 58.5 ## 2 Sam 6.7 51.8 ## 3 Sarah 14.9 45.1 datoy_long ## Name Subject Score ## 1 Ben Maths 10.0 ## 2 Ben English 63.7 ## 3 Sam Maths 52.9 ## 4 Sam English 75.6 ## 5 Alex Maths 88.8 ## 6 Alex English 92.2 Let’s reshape the datasets4:\n## lets make it longer datoy_wide %\u0026gt;% pivot_longer(-Name, names_to = \u0026quot;Subject\u0026quot;, values_to = \u0026quot;Score\u0026quot;) ## lets make it wider datoy_long %\u0026gt;% dplyr::mutate(Time = 1:nrow(datoy_long)) %\u0026gt;% pivot_wider(names_from = \u0026quot;Subject\u0026quot;, values_from = c(\u0026quot;Score\u0026quot;, \u0026quot;Time\u0026quot;))  5. vroom Speaker: Jim Hester (Slides + Screencast)\n## install.packages(\u0026quot;vroom\u0026quot;) library(vroom) Importing large datasets into R can be a painful task. Especially if you only need a subset of the columns. And apparently, our thoughts drift off after 10 sec5 staring at the screen where it is still loading the dataset.\ndata.table::fread() is always here to help. But now comes vroom!\nGet some large’ish data First, we need some large dataset. To not burden our laptops too much6, we will go for some exome based GWAS results.\n## Source: https://portals.broadinstitute.org/collaboration/giant/index.php/GIANT_consortium_data_files path_to_file_1 \u0026lt;- \u0026quot;Height_AA_add_SV.txt.gz\u0026quot; path_to_file_2 \u0026lt;- \u0026quot;BMI_African_American.fmt.gzip\u0026quot; ## Height download.file( \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/8/80/Height_AA_add_SV.txt.gz\u0026quot;, path_to_file_1) ## BMI download.file( \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/3/33/BMI_African_American.fmt.gzip\u0026quot;, path_to_file_2) ## File size ## install.packages(\u0026quot;fs\u0026quot;) fs::file_size(path_to_file_1) ## 4.39M fs::file_size(path_to_file_2) ## 13.3M The two datasets have a mix of characters, numbers and decimals7.\n vroom vs DT Here is how vroom works and a basic comparison to data.table::fread (I let you do the proper benchmarking yourself).\nlibrary(dplyr) ## With vroom giant_vroom \u0026lt;- vroom::vroom(path_to_file_1) giant_vroom_subset \u0026lt;- giant_vroom %\u0026gt;% select(CHR, POS) %\u0026gt;% filter(CHR == 1) ## The equivalent with data.table giant_DT \u0026lt;- data.table::fread(path_to_file_1) giant_DT_subset \u0026lt;- giant_DT %\u0026gt;% select(CHR, POS) %\u0026gt;% filter(CHR == 1)  col_select for the win ## Selecting columns giant_vroom_select \u0026lt;- vroom::vroom(path_to_file_1, col_select = list(SNPNAME, ends_with(\u0026quot;_MAF\u0026quot;))) head(giant_vroom_select) ## Preventing columns from being imported giant_vroom_remove \u0026lt;- vroom::vroom(path_to_file_1, col_select = -ExAC_AFR_MAF) head(giant_vroom_remove) ## Renaming on the fly giant_vroom_rename \u0026lt;- vroom::vroom(path_to_file_1, col_select = list(p = Pvalue, everything())) head(giant_vroom_rename)  Combining multiple datasets data_combined \u0026lt;- vroom::vroom( c(path_to_file_1, path_to_file_2), id = \u0026quot;path\u0026quot;) table(data_combined$path)   6. data.table Speaker: Arun Srinivasan (Slides)\ndata.table has a pretty cool feature8:\n# install.packages(\u0026quot;data.table\u0026quot;) library(data.table) ## Warning: package \u0026#39;data.table\u0026#39; was built under R version 3.5.2 ## ## Attaching package: \u0026#39;data.table\u0026#39; ## The following object is masked from \u0026#39;package:dataAnim\u0026#39;: ## ## := ## The following objects are masked from \u0026#39;package:dplyr\u0026#39;: ## ## between, first, last ## Create a giant data.table p \u0026lt;- 2e6 dat \u0026lt;- data.table(x = sample(1e5, p, TRUE), y = runif(p)) ## Let\u0026#39;s select a few rows system.time( tmp \u0026lt;- dat[x %in% 2000:3000 ] ) ## do the same operation again system.time( tmp \u0026lt;- dat[x %in% 2000:3000 ] )  7. rray Speaker: Davis Vaughan (Slides)\n# devtools::install_github(\u0026quot;r-lib/rray\u0026quot;) ## may take some time rray can do two things that are otherwise annoying/counter-intuitive in R:\n broadcasting (recycling dimensions) subsetting (bag[,1, drop = FALSE])  Matrices with base-r Let’s look at an example of matrix operations in base-r9.\nFirst, we want to add two matrices with similar dimensions:\nmat_1 \u0026lt;- matrix(c(15, 10, 8, 6, 12, 9), byrow = FALSE, nrow = 2) mat_2 \u0026lt;- matrix(c(5, 2, 3), nrow = 1) ## broadcasting won\u0026#39;t work ❌ mat_1 + mat_2 ## Error in mat_1 + mat_2: non-conformable arrays Next, we want to select one matrix column:\ndim(mat_1[,2:3]) ## selecting two columns is fine ## [1] 2 2 ## subsetting won\u0026#39;t preserve the matrix class ❌ dim(mat_1[,1]) ## why not 2x1? ## NULL length(mat_1[,1]) ## ah, it turned into a vector! ## [1] 2 dim(mat_1[,1, drop = FALSE]) ## but with drop = FALSE we can keep it a matrix ## [1] 2 1  Matrices with rray Let’s do now the same task with rray.\nlibrary(rray) (mat_1_rray \u0026lt;- rray(c(15, 10, 8, 6, 12, 9), dim = c(2, 3))) (mat_2_rray \u0026lt;- rray(c(5, 2, 3), dim = c(1, 3))) ## Broadcasting works ✓ mat_1_rray + mat_2_rray ## Subsetting works ✓ dim(mat_1_rray[,2:3]) dim(mat_1_rray[,1]) ## smart functions mat_1_rray / rray_sum(mat_1_rray, axes = 1) rray_bind(mat_1_rray, mat_2_rray, .axis = 1) rray_bind(mat_1_rray, mat_2_rray, .axis = 2)   More info  Program + Slides: https://user2019.r-project.org/talk_schedule/ Collection of slides during the conference by Praer (Suthira Owlarn): https://github.com/sowla/useR2019-materials Recordings by the R Consortium on Youtube (keynotes available, rest soon to be published)   Last but not least The rstatsmeme package is a little gem discovered thanks to Frie Preu:\n# devtools::install_github(\u0026quot;favstats/rstatsmemes\u0026quot;) library(rstatsmemes) show_me_an_R_meme()   I cannot wait to see the recordings to catch up with the parallel sessions that I missed!↩\n See Slide 49+.↩\n See demo 1↩\n See demo↩\n I can totally confirm that.↩\n If you can, choose the UKBB + GIANT meta analysis results, which are pretty large.↩\n Apparently, characters are the most challenging ones for speed.↩\n See slides 26↩\n See also slide 5.↩\n   ","date":1563408000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563408000,"objectID":"781043ae7722e0a2f862ad1035f413d3","permalink":"https://sinarueeger.github.io/post/amuse-bouches-from-user-2019/","publishdate":"2019-07-18T00:00:00Z","relpermalink":"/post/amuse-bouches-from-user-2019/","section":"post","summary":"The 3+ days at useR!2019 in Toulouse were packed with great talks1 and good food - hence the amuse-bouches word play.\nHere are some R code bits from the conference. Hopefully convincing enough to start using a new package or change a workflow. Not everything was brand-new, but it was helpful to have someone talking through their inspiration and examples.\nCheck out the speakers’ materials - soon there will be recordings too.","tags":["conference","R"],"title":"Amuse-bouches from useR!2019","type":"post"},{"authors":null,"categories":null,"content":"  How to make a good poster Recipe Content Design  This is a tutorial on how to make a scientific poster from scratch.\nIt aims for first-time poster makers, but also for people like me that quickly lose themselves in choosing the right font for hours.\nIt includes a few self-declared best practices that helped me in the past. Those are at best ideas, and barely opinions. It is by no means a collection of strict rules and feedback is welcome!\nI got lots of tips from Flavia Hodel during stimulating discussions around poster presentation. Flavia also gave valueable feedback after reading through a draft.\nHow to make a good poster An excellent video by Mike Morrison recently made the rounds online. If you have not done so already, watch it before continuing.\nMike Morrison makes the point that most of the posters presented at conferences are not easy to grasp.\nThere are two principal reasons for this:\n The content is not adapted for a poster presentation. For example reusing whole paragraphs from a manuscript. Poor design choices. For example small fonts, problematic colour choices, low-resolution figures.  Separating content from design is important1 because it will allow you to focus only on one or the other. And since \u0026ldquo;design\u0026rdquo; is a profession, a better description in what we are doing is \u0026ldquo;polishing\u0026rdquo; or \u0026ldquo;improving readability\u0026rdquo;.\nI will structure my tips into these two parts: content and design, along with a recipe that I like to follow.\nBut let\u0026rsquo;s talk first about what makes a poster presentation special.\nOral versus poster presentation If you get an oral presentation (congrats!), then you know that your voice will carry most of the talk. While your slides should be carefully designed, you can still rescue imperfect slides with a good talk.\nPoster presentations are somewhat special because they are at an intersection of a talk and an art exhibition. Meaning, you will once during the conference have the opportunity to guide other researchers through your poster. This is similar to a lightning talk, just more engaging.\nThe remainder of the conference, your poster will hang there, and maybe some people will still try to understand your topic. No matter how good you are at talking, now the poster has to be self explainable.\nIn fact, your poster should read like going through stand-alone slides (slides with more explanations to replace the speakers\u0026rsquo; voice).\nBack to the video While overall the video of Mike Morrison is excellent, I struggle with three things.\nFirst, it talks about emphasising this very clear conclusion. In real life, this is often not possible. For example, because a research project has just started. So instead of having this one research question, I\u0026rsquo;d go for the main message - what do you want that people remember? That can be the large sample size of your study or that you are developing a new method.\nSecond, the poster template still crams the details of the study into a small space, which is not ideal for readability.\nThird, it talks about a horizontal2 poster, and this is often not possible at conferences, due to space constraints3.\nRecipe Here is a recipe that I like to follow when I make a poster.\nGet ready  Start at least 7 days before the poster needs to go into print4.\n Think of the main message: what is the one thing that you want the reader to remember? Write that message into a text file.\n Take a pen and sketch a draft of your poster on an A4 paper. What do you want to present and where does this content go on the poster? What kind of figures and tables to you want the reader to see? No need to write proper sentences, instead use keywords and skeletons of figures and tables. You can also do this step at a later stage, basically whatever suits you best for your creative process. For example, you might want to do first steps 4-7 before sketching a draft. \n Get your abstract and extract title, author and affiliation and save it into a text file too.\n Create a folder img somewhere on your computer.\n Copy all text, tables and figures you possibly need into the img folder. Slides from previous talks can be a handy source.\n Get the logos of your university and affiliations and store them into the img folder too. If you have the choice, go for PNGs, as they have transparent backgrounds.\n Choose a tool that is easy to operate and does the things you want. My choice is keynote because I cannot bother with Adobe Illustrator. More details below.\n Update: Read the conference instructions, particularly on poster size and orientation (as recommended by Peter Higgins).\n Resize document to the size recommended by the conference or A0 (841 mm x 1189 mm or 2384 pt x 3370 pt). This way, even though the poster might be proportional to A4, you can use the real size fonts.\n Split the document into different boxes (or any compartmentation according to your sketch). For example four horizontal lines and one vertical lines will turn 10 boxes. Use rulers, guides or lines to help you with that. Each box will later answer a different question.   Now you are ready to add content.\nAdd content  Before adding content, choose a sans-serif font. Helvetica or Arial will do a good job. These fonts were carefully designed to be easy to read and look good on print. Later on, you can still decide to invest time into checkout google fonts, but remember - it needs to be sans-serif. The smallest text size should be 24 pt.\n Fill the top rectangles.\n Place the title on top. This is the first thing the reader should see.\n Add authors and affiliation right below.\n Add the logos next to the authors and affiliations.\n Next, add your main message below.  Add contact and lab webpage at the very bottom of the poster.  No need to adjust and fiddle around with boxes at this stage. Just fill the content.\n Now we are left with 7 boxes. How you distribute the content into these boxes depends on your topic. I work in a field where data and methodology is central, so I always dedicate one box to data and one box to methods.  The first box will take care of the introduction and should answer the question Why is your research needed?\n The second box should present the methods: What method did you use to answer your research question?\n The third box takes care of data presentation: What kind of data did you use and how does it fit together with the method?\n The fourth and fifth box are reserved for results: What are the results if you combine data and methods?\n The six box is dedicated to a discussion or summary: What can you conclude from the results? What are the limitations? And where are you heading next?\n The seventh box is for references and abbreviations.\n Update: Add a small portrait of yours to the upper corners or next to your contact email address. This way people can spot and talk to you outside the poster session (recommended by Federico Marini).\n Update: Add some empty post-it notes and a pencil next to your poster and ask readers for feedback (recommended by Federico Marini).\n  Polishing \u0026amp; Design Now that you have all your content, you can start to improve the readability.\nIn order to quickly grasp a poster, it should have as less sentences as possible.\n Consider replacing some text with icons or figures.\n Make use of bullet points.\n Align all text and shapes (again, rulers might help here).\n Make sure all section titles are equally sized.\n Are your figures (and the rest of your poster) colour-blind friendly?\n You can start adding colours (if they serve a purpose).\n Think about whether your figures need captions or not.\n  Finalising  See how it looks like on a printed A4 version (update: or for less troublesome reading print it on A3, as mentioned by Michael MacAskill).\n Ask your colleagues for their opinion by showing them the printed version.\n You might want to let it sit for a day or two. Then look at it with fresh and rested eyes.\n  Trick-trick: Have a detailed and a lean version After a few days break you look at it again and you realise that it is still to busy with text.\nHere is the trick - keep two versions: the detailed one you already made (that can be accessed through a QR code), and the one that you will bring to the poster session with lot less details.\n Make a copy of your poster draft, name it poster_detailed.pdf. Put this pdf somewhere online (e.g. dropbox, google drive) and get a link to it.\n Generate a QR-code of that link and download the png to your img/ folder.\n Continue working on your original file.\n Count your words. Your aim is now to cut the number of words in half - roughly. If you don\u0026rsquo;t know where to remove content, present your poster to a colleague from the same faculty (not the same lab). You will quickly realise over what details you jump. Remove those details.\n If you can simplify the graphs - do that too.\n Count the words again - has it been halfed?\n Add the QR-code to the bottom of the poster.\n  Printing  Print it again in A4 for yourself and check:\n if all text and shapes are aligned and equally distributed, if the font is the same throughout the poster, if all authors or affiliations are present, for typos, if figures are clearly readable (and in high resolution), where the QR-code leads you, whether your contact email address is correct.   That\u0026rsquo;s it - send it to the printing service!\n  Content Before adding any content, think about your audience. These will be scientists, and they have probably heard about research you are working on, but don\u0026rsquo;t know the details and motivation behind your work. In fact, there will only be a handful of people that know exactly what your domain, topic and method does. All others are still interested though. So lets aim for these people as your audience.\nAfter looking at your poster, the reader should have an idea what the title means. Your poster title might be super clear to you and your lab colleagues, but anyone else will have a hard time understanding right away what you are doing.\nTherefore, one goal is, to clarify the words in your title.\nAfter separating your file with rulers as described above, you will have 8 boxes, of which you can use 6 for:\n Introduction Method Data General results Specific results Discussion \u0026amp; Summary  I recommend to give your boxes titles so the reader can jump from one to the other.\nWhat content should go in your boxes?\nI believe that a poster cannot carry a condensed version of a full manuscript. I think it is ok to focus on only one or two threads of your work. Your story telling needs to be coherent - that is all the reader wants.\nIntroduction I like to follow the paper writing guidelines by Jennifer Widom as a rough guide:\n What is the problem? Why is it interesting and important? Why is it hard? Why hasn\u0026rsquo;t it been solved before? What are the key components of my approach and results?  Methods  Unless your very focus is a new statistical methods, keep this section simple and clear.\n Make sure that it is clear why the method is able to answer your research question. You can use equations, but simplify them too, or annotate them with arrows and explain all fancy letters used.  Data  Mention the origin(s) of your data. What was the sample size? No need to describe all the variables you used. To save space, you can also group them into larger categories. Describe distinct features of your data that could influence the interpretation: e.g. if all your samples were male. Mention how you processed the data. Again, no need to be specific. If your area uses standard QC methods, mention that the data was processed according to standard QC techniques.  Results  Have two layers of detail. For example, go from general to specific; have a graph that summarises your results, then zoom into your results in a next graph. Use figures to illustrate your results and avoid tables. Simplify the figures and use the power of your poster making tool to annotate the figures. Make sure that the results are follow the notation of data and methodology. E.g. if you present results for a trend in age, make sure you have mentioned age before.  Discussion This is the place to:\n summarise your work conclude talk about limitations announce future plans  Contact  Add your email address. Add lab webpage. Add the QR code that leads to your (detailed) poster.  References  Add important references, for example software that you used. Make sure you cite original content.  Design Or improving readability.\nTooling A good tool should satisfy the following criteria:\n Decent in zooming in and out. Variety of graphical options. Operable with ease. Something you can use over years.  If you have practice with Adobe Illustrator or its\u0026rsquo; open-source counterpart Inkscape - go for it!\nOtherwise, google slides, Powerpoint, keynote - although limiting - do a the job too.\nFonts  No matter what sophisticated typeface you had in mind, choose one of these sans-serif ones for a start: Helvetica or Arial. If you really have the time to play with other sans-serif fonts, check out google fonts. Making fonts is an actual job - remember to pay for special fonts. Most tools have a replace font option, so you can replace a font through out your poster.  Font size  Minimum 24 pt, but use it sparingly. Title should be largest with somewhere in between 100 and 140 pt.  Icons Some icon services are for free, or at least partially: e.g. flat icon.\nFigures Principles for data visualisations could easily occupy another blog post. There is much to say, but let\u0026rsquo;s keep it short here.\nIf you use R and ggplot2 for making your figures, use one of the black and white themes, e.g. theme_set(theme_linedraw()).\nMake sure that the axis text size is also 24pt once it is on the poster.\nColours Colours are either used as decoration or to encode a variable. Do not use colour for your background5.\nAlways use colour-blind friendly colours.\nYou can test the colour-blind friendlyness of your poster and figures with https://colororacle.org, a colour-blindness simulator on your computer, or in a browser: https://www.color-blindness.com/coblis-color-blindness-simulator/.\nFor Figures:\n If you use more colours than available in a discrete colour palette - drop that encoding. No one will be able to grasp. Use colour-blind friendly palettes from the colorblindr R-package.  % mutate(swatch = cell_spec( swatch, color = \"white\", bold = T, background = hex )) %% kable(escape = F, align = \"c\") %% kable_styling(c(\"striped\", \"condensed\"), full_width = F) ``` ---- Resolution of figures Your principal figures should always be stored as PDF format. This way, you can easily turn them into a high-resolution PNG.\nTo turn a PDF into a high-resolution raster image, use convert from imagemagick.\nIf your file is called figure-1.pdf, then write:\nconvert -quality 100 -background white -alpha background -compress lzw -units pixelsperinch -resize \u0026quot;789x2625\u0026quot; -density 300 figure-1.pdf figure-1b.png  If you have lots of pdf starting with figure-*, then write:\nfor file in figure-*.pdf; do \\ echo $file;\\ convert -quality 100 -background white -alpha background -compress lzw -flatten -units pixelsperinch -resize \u0026quot;789x2625\u0026quot; -density 300 $file `echo $file|cut -f1 -d'.'`.png;\\ done  Links  https://guides.library.ucla.edu/c.php?g=223540\u0026amp;p=1480858tag http://blogs.nature.com/naturejobs/2017/11/06/using-design-principles-to-inform-scientific-posters/ Update: Ten Simple Rules for a Good Poster Presentation (Plos CompBio) (mentioned by Federico Marini)  Full disclosure Here are some of my posters from different stages of my career. Making posters is a process of improvement.\n2018   2017    If you know Markdown or LaTex, you know what I mean. ^ Update: As Peter Higgins pointed out, horizontal is common in the US, vertical is common in Europe. ^ However, a straight forward solution is, to turn the design by 90 degrees. ^ Make sure you know how much time the printing service needs to print your poster. ^ Black or gray background and white text colour might work though. ^   ","date":1560124800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1560124800,"objectID":"b2ce2c070bea0f3df1866715d67e699a","permalink":"https://sinarueeger.github.io/post/scientific-poster/","publishdate":"2019-06-10T00:00:00Z","relpermalink":"/post/scientific-poster/","section":"post","summary":"How to make a good poster Recipe Content Design  This is a tutorial on how to make a scientific poster from scratch.\nIt aims for first-time poster makers, but also for people like me that quickly lose themselves in choosing the right font for hours.\nIt includes a few self-declared best practices that helped me in the past. Those are at best ideas, and barely opinions. It is by no means a collection of strict rules and feedback is welcome!","tags":["science communication","conference"],"title":"A guide to poster making for scientific conferences","type":"post"},{"authors":null,"categories":null,"content":" I finished my PhD under the main supervision of Zolt\u0026#225;n Kutalik in September 2018.\nYou can download my thesis or check out the slides from my public defense.\nAbstract Increasing our knowledge about biology in humans is essential for advances in medicine, such as early-stage diagnoses of diseases, drug development, public health strategies, and precision medicine. One approach to tackle this task is, to collect data on different components of a biological mechanism of interest, link these parts and try to construct an underlying model that helps us to explain the disease. To collect data, DNA is measured and the status of a disease is recorded for each individual in a dedicated group of people. In a first step, an analyst compares for each genetic variant across the whole genome the genetic mutations between people with the disease and healthy people; this is called a genome-wide association study (GWAS). Such first association screens rarely point right away to the true causal variants, but combined with additional biomedical (-omics) data and additional statistical methods it is possible to narrow down the true cause and gain insight into the biology of a disease. For example, by using GWAS results for two diseases (e.g. cardiovascular disease and obesity) and a statistical method called Mendelian randomisation, we are able to examine the causal effect of obesity on cardiovascular disease, or vice versa. These statistical follow-up investigations often require GWAS results for genetic variants than were unmeasured. During my PhD, I investigated a method called summary statistic imputation that precisely aims to solve the problem of inferring GWAS results for unmeasured genetic variants. Summary statistic imputation uses GWAS results and data from public sequencing data. My main findings were that imputation accuracy varies depending on certain characteristics of a genetic variant (e.g. low accuracy for rare mutations), as well as the size of publicly available sequencing data (low accuracy for small sized sequencing data). A further finding is, that summary statistic imputation can compete with imputation techniques that are based on individual-level data for certain subgroups of genetic variants (e.g. common variants).\nWith the help of summary statistic imputation researchers can facilitate follow-up investigations and thus gain more insight into the biology of diseases.\n","date":1558990800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558990800,"objectID":"070d51a28892f81411bee8389f825d6d","permalink":"https://sinarueeger.github.io/project/phd-thesis/","publishdate":"2019-05-28T00:00:00+03:00","relpermalink":"/project/phd-thesis/","section":"project","summary":"PhD thesis material.","tags":["statistical genetics"],"title":"Integrative Statistical Analysis of -omics and GWAS data","type":"project"},{"authors":null,"categories":null,"content":"","date":1558990800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558990800,"objectID":"1015e5348459ba232b9c99a584dbf21b","permalink":"https://sinarueeger.github.io/project/gwas.utils/","publishdate":"2019-05-28T00:00:00+03:00","relpermalink":"/project/gwas.utils/","section":"project","summary":"Helper functions when working with GWAS (summary) data.","tags":["statistical genetics","R"],"title":"R-package GWAS.utils","type":"project"},{"authors":null,"categories":null,"content":"","date":1558990800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558990800,"objectID":"0673e061ae94badb51ab44d83f8b8852","permalink":"https://sinarueeger.github.io/project/gggwas/","publishdate":"2019-05-28T00:00:00+03:00","relpermalink":"/project/gggwas/","section":"project","summary":"ggplot2 extension for visualising GWAS summary statistics.","tags":["statistical genetics","data visualisation","R"],"title":"R-package ggGWAS","type":"project"},{"authors":null,"categories":null,"content":"","date":1558990800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558990800,"objectID":"2a64e5712fe7bfd61bd034d3051a241e","permalink":"https://sinarueeger.github.io/project/stat-genetics-ressources/","publishdate":"2019-05-28T00:00:00+03:00","relpermalink":"/project/stat-genetics-ressources/","section":"project","summary":"Collection of resources needed in statistical genetics: data, readings, software.","tags":["statistical genetics"],"title":"Statistical Genetics Resources","type":"project"},{"authors":null,"categories":null,"content":"  Goal Two approaches Our toy data 1A. A solution that works: ldlink from NIH LDproxy LDmatrix  1B. A solution that almost works: ensembl.org What reference panels/population can we choose from? Access LD between a SNP and its region Access LD matrix Access LD between a SNP and many other SNPs Coloured locuszoom plot  2. Solutions that work half-through SNPsnap API provided by sph.umich  3. A solution that does not work rsnps::ld_search  Conclusion Session Info   R-packages used:\nlibrary(httr) library(jsonlite) library(xml2) library(ggplot2) theme_set(theme_bw()) library(tibble) library(tidyr) library(magrittr) library(glue) library(purrr) library(rsnps) library(data.table) library(janitor) library(stringr) library(rsnps) The squared correlation between genetic markers is one way to estimate linkage disequilibrium (LD). LD has to be computed all the time - either for as an input for statistical methods or to summarise results.\nHowever, accessing LD estimations quickly, for a specific population and in an automated way (e.g. with R) is suprisingly difficult.\nIn this blog post I am exploring how to do this efficiently.\nGoal At the end of this blog post, we want to know the genetic correlation between two or more markers in a specific human population, so that we can populate the locuszoom plot from the previous blog post with coloured dots.\nFor simplicity, I will use the terms correlation, squared correlation, r, r2 and LD interchangeably.\n Two approaches In principle, there are two ways of doing accessing LD:\nDownload (or access) the genetic data from which you want to estimate your correlations + calculate the correlations using some efficient approach. Access precomputed LD estimations.      Approach  Advantages  Downsides  Useful when…    (1) Local computation of LD LD matrix can be quickly updated to new reference panels Requires large computation and storage space (e.g. 1000 Genomes is \u0026gt;100 few GB large). i) LD for a large set of SNPs is needed ii) LD from non-standard reference panel is needed.  (2) Access precomputed LD not need for large computation and storage space. limited to certain small sets of markers, limited to possibly outdated reference panels. LD for a small set of SNPs is needed    For now, I will focus on approach (2), and then explore approach (1) in a future blog post.\nSpoiler: Using approach (2) does not get you far. It took me quite a while to gather all the solutions that are listed below, and yet there is not one perfect/ideal solution.\n Our toy data We will recycle the data from the previous blog post, where the focus was on extracting annotation using the package biomaRt. In this blog post, we will complete that locuszoom plot by adding the LD information.\n## Data Source URL url \u0026lt;- \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/2/21/BMI_All_ancestry.fmt.gzip\u0026quot; ## Import BMI summary statistics dat.bmi \u0026lt;- read_tsv(file = url) ## ## taking too long, let\u0026#39;s use fread instead. dat.bmi \u0026lt;- data.table::fread(url, verbose = FALSE) ## Rename some columns dat.bmi \u0026lt;- dat.bmi %\u0026gt;% rename(SNP = SNPNAME, P = Pvalue) ## Extract region dat.bmi.sel \u0026lt;- dat.bmi %\u0026gt;% slice(which.min(P)) dat.bmi.sel ## range region range \u0026lt;- 5e+05 sel.chr \u0026lt;- dat.bmi.sel$CHR sel.pos \u0026lt;- dat.bmi.sel$POS data \u0026lt;- dat.bmi %\u0026gt;% filter(CHR == sel.chr, between(POS, sel.pos - range, sel.pos + range)) head(data) (snp \u0026lt;- dat.bmi.sel$SNP) What we are interested in is the LD between our top SNP rs1421085 and all other 82 SNPs nearby.\nThis dataset has positions on build GRCh37, while most databases are on build GRCh38 by now.\nsm \u0026lt;- rsnps::ncbi_snp_summary(snp) %\u0026gt;% separate(chrpos, c(\u0026quot;chr\u0026quot;, \u0026quot;pos\u0026quot;)) sel.pos == as.numeric(sm$pos) ## [1] FALSE Let’s quickly repeat what our primary goal is:\nExtract the correlation between SNPs\n without downloading any data, fairly quick and in R.   1A. A solution that works: ldlink from NIH ldlink is a website provided by NIH to easily (and programmatically) request LD estimates in population groups.\nLD is estimated from Phase 3 of the 1000 Genomes Project and super- and subpopulations can be selected.\nThere are different ways to access the LD estimations (e.g. LDpair, LDmatrix, LDproxy) and the same modules are also available through the API.\nTo access the API, you need to register for a token (takes a few seconds).\nMYTOKEN \u0026lt;- \u0026quot;a_mix_of_numbers_and_characters\u0026quot; Let’s look at two modules:\n LDproxy: access LD between a SNP and its region LDmatrix: access LD matrix  LDproxy To get the LD between a SNP and its region.\nFirst, access the API:\nLDproxy_raw \u0026lt;- system( glue::glue(\u0026quot;curl -k -X GET \u0026#39;https://ldlink.nci.nih.gov/LDlinkRest/ldproxy?var={snp}\u0026amp;pop=EUR\u0026amp;r2_d=r2\u0026amp;token={MYTOKEN}\u0026#39;\u0026quot;), intern = TRUE ) Then, do a bit of data wrangling to get a tidy data frame:\nLDproxy \u0026lt;- LDproxy_raw %\u0026gt;% purrr::map(., function(x) stringr::str_split(x, \u0026quot;\\t\u0026quot;) %\u0026gt;% unlist()) %\u0026gt;% ## remove all the tabs do.call(rbind, .) %\u0026gt;% ## turn into a matrix data.frame() %\u0026gt;% ## turn into a data frame janitor::row_to_names(1) %\u0026gt;% ## make the first row the column names rename(SNP = RS_Number) %\u0026gt;% ## rename RS_Number as SNP mutate_at(vars(MAF:R2), function(x) as.numeric(as.character(x))) %\u0026gt;% ## turn MAF:R2 columns numeric mutate(SNP = as.character(SNP)) ## turn SNP from a factor into a character head(LDproxy) ## SNP Coord Alleles MAF Distance Dprime R2 ## 1 rs1421085 chr16:53800954 (T/C) 0.4324 0 1.0000 1.0000 ## 2 rs11642015 chr16:53802494 (C/T) 0.4324 1540 1.0000 1.0000 ## 3 rs62048402 chr16:53803223 (G/A) 0.4324 2269 1.0000 1.0000 ## 4 rs1558902 chr16:53803574 (T/A) 0.4324 2620 1.0000 1.0000 ## 5 rs55872725 chr16:53809123 (C/T) 0.4324 8169 1.0000 1.0000 ## 6 rs56094641 chr16:53806453 (A/G) 0.4344 5499 0.9959 0.9839 ## Correlated_Alleles RegulomeDB Function ## 1 T=T,C=C 5 NA ## 2 T=C,C=T 4 NA ## 3 T=G,C=A 5 NA ## 4 T=T,C=A 7 NA ## 5 T=C,C=T 4 NA ## 6 T=A,C=G 6 NA Next, join the original summary stats data with the LDproxy data frame.\ndata_ldproxy \u0026lt;- data %\u0026gt;% right_join(LDproxy, by = c(\u0026quot;SNP\u0026quot; = \u0026quot;SNP\u0026quot;)) Lastly, plot the summary statistics with the point colour indicating the R2.\nggplot(data = data_ldproxy) + geom_point(aes(POS, -log10(P), color = R2), shape = 16) + labs( title = \u0026quot;Locuszoom plot for BMI GWAS\u0026quot;, subtitle = paste( \u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot; ), caption = paste(\u0026quot;Data source:\u0026quot;, url) ) + geom_point( data = data_ldproxy %\u0026gt;% filter(SNP == snp), aes(POS, -log10(P)), color = \u0026quot;black\u0026quot;, shape = 16 ) + scale_color_distiller(\u0026quot;R2 (LDproxy)\u0026quot;, type = \u0026quot;div\u0026quot;, palette = \u0026quot;Spectral\u0026quot;, limits = c(0, 1) )  LDmatrix LDmatrix module accesses the pairwise LD between a set of SNPs.\nAgain, first access the API:\nsnplist \u0026lt;- data %\u0026gt;% filter(str_detect(SNP, \u0026quot;rs\u0026quot;)) %\u0026gt;% pull(SNP) %\u0026gt;% paste(collapse = \u0026quot;%0A\u0026quot;) LDmatrix_raw \u0026lt;- system( glue::glue(\u0026quot;curl -k -X GET \u0026#39;https://ldlink.nci.nih.gov/LDlinkRest/ldmatrix?snps={snplist}\u0026amp;pop=CEU%2BTSI%2BFIN%2BGBR%2BIBS\u0026amp;r2_d=r2\u0026amp;token={MYTOKEN}\u0026#39;\u0026quot;), intern = TRUE )  If you want to access the dprime (d’) values, write r2_d=d. If you want to access certain sub populations, let’s say CEU, TSI and FIN, concatenate them with %B in between: CEU%2BTSI%2BFIN.  Then, do a little data tidying:\nLDmatrix \u0026lt;- LDmatrix_raw %\u0026gt;% purrr::map(., function(x) stringr::str_split(x, \u0026quot;\\t\u0026quot;) %\u0026gt;% unlist()) %\u0026gt;% do.call(rbind, .) %\u0026gt;% ## turn into a matrix data.frame() %\u0026gt;% ## turn into a data.frame janitor::row_to_names(1) ## make the first line the column names LDmatrix_long \u0026lt;- LDmatrix %\u0026gt;% gather(\u0026quot;SNP2\u0026quot;, \u0026quot;R2\u0026quot;, -RS_number) %\u0026gt;% ## from wide to long rename(SNP = RS_number) %\u0026gt;% ## rename RS_number mutate(R2 = as.numeric(R2)) %\u0026gt;% ## make R2 numeric mutate_if(is.factor, as.character) ## make all factor columns characters ## Warning: attributes are not identical across measure variables; ## they will be dropped ## Warning: NAs introduced by coercion head(LDmatrix_long) ## SNP SNP2 R2 ## 1 rs61754093 rs61754093 1.000 ## 2 rs181111349 rs61754093 NA ## 3 rs199662749 rs61754093 NA ## 4 rs189080082 rs61754093 0.000 ## 5 rs6499548 rs61754093 0.023 ## 6 rs139704369 rs61754093 NA Next, join the original summary stats data with the LDmatrix_long data frame.\ndata_ldmatrix \u0026lt;- data %\u0026gt;% right_join(LDmatrix_long, by = c(\u0026quot;SNP\u0026quot; = \u0026quot;SNP\u0026quot;)) Lastly, plot the summary statistics with the point colour indicating the R2.\nggplot(data = data_ldmatrix %\u0026gt;% filter(SNP2 == snp)) + geom_point(aes(POS, -log10(P), color = R2)) + labs( title = \u0026quot;Locuszoom plot for BMI GWAS\u0026quot;, subtitle = paste( \u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot; ), caption = paste(\u0026quot;Data source:\u0026quot;, url) ) + geom_point( data = data_ldmatrix %\u0026gt;% filter(SNP == snp \u0026amp; SNP2 == snp), aes(POS, -log10(P)), color = \u0026quot;black\u0026quot;, shape = 16 ) + scale_color_distiller(\u0026quot;R2 (LDmatrix)\u0026quot;, type = \u0026quot;div\u0026quot;, palette = \u0026quot;Spectral\u0026quot;, limits = c(0, 1) )   1B. A solution that almost works: ensembl.org The REST API of Ensembl can do a lot (see options here). For example access precomputed LD. The webpage even provides R code to do so, which is from where I copied some snippets below.\nTo access the rest API at ensembl, we need the following three packages loaded.\nlibrary(httr) library(jsonlite) library(xml2) What reference panels/population can we choose from?  Currently, the largest and hence most popular public reference panel is 1000 Genomes reference panel (1KG). The 26 populations of roughly 100 individuals each can be grouped into five super populations: African (AFR), American (AMR), European (EUR), South Asian (SAS), East Asian (EAS).\nWe can ask the ENSEMBL API from what populations reference panels are available. This will return us a data frame.\nserver \u0026lt;- \u0026quot;https://rest.ensembl.org\u0026quot; ext \u0026lt;- \u0026quot;/info/variation/populations/homo_sapiens?filter=LD\u0026quot; r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) head(fromJSON(toJSON(content(r)))) ## description size ## 1 African Caribbean in Barbados 96 ## 2 African Ancestry in Southwest US 61 ## 3 Bengali in Bangladesh 86 ## 4 Chinese Dai in Xishuangbanna, China 93 ## 5 Utah residents with Northern and Western European ancestry 99 ## 6 Han Chinese in Bejing, China 103 ## name ## 1 1000GENOMES:phase_3:ACB ## 2 1000GENOMES:phase_3:ASW ## 3 1000GENOMES:phase_3:BEB ## 4 1000GENOMES:phase_3:CDX ## 5 1000GENOMES:phase_3:CEU ## 6 1000GENOMES:phase_3:CHB name stands for the population identifier. size refers to the number of individuals in the reference panel. Note that these are all populations with around 100 individuals (the correlation estimation will have an error that scales with the sample size). There are also the five super population available (although not listed here), simply replace the last three characters in name by EUR, AFR, AMR, EAS, SAS.\n (From this blog post.)\nWe want the LD information, so that we can add this info to the locuszoom plot. But how do we know which population to pick? One way is to read up what kind of individuals were present. In our case - mostly Europeans (EUR). But we could also build some pooled LD matrix of different populations.\nNow that we know which reference panel we want to use, we can use the different rest APIs.\n Access LD between a SNP and its region Access LD matrix Access LD between a SNP and many other SNPs   Access LD between a SNP and its region  This API is described here.\nThe default window size is 500 kb. There are also thresholds for r2 (e.g. if you want to filter all SNPs with an r2 \u0026gt; 0.8).\nThe only input required is the SNP rsid, marked with {snp}.\nsnp ## [1] \u0026quot;rs1421085\u0026quot; server \u0026lt;- \u0026quot;https://rest.ensembl.org\u0026quot; ext \u0026lt;- glue::glue(\u0026quot;/ld/human/{snp}/1000GENOMES:phase_3:EUR?\u0026quot;) ## Window size in kb. The maximum allowed value for the window size is 500 kb. LD is computed for the given variant and all variants that are located within the specified window. r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) LD.SNP.region \u0026lt;- as_tibble(fromJSON(toJSON(content(r)))) %\u0026gt;% unnest() %\u0026gt;% mutate(r2 = as.numeric(r2)) ## Warning: `cols` is now required. ## Please use `cols = c(variation1, d_prime, r2, population_name, variation2)` head(LD.SNP.region) ## # A tibble: 6 x 5 ## variation1 d_prime r2 population_name variation2 ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 rs1421085 0.327013 0.0834 1000GENOMES:phase_3:EUR rs8043738 ## 2 rs1421085 0.992084 0.409 1000GENOMES:phase_3:EUR rs2042031 ## 3 rs1421085 0.846280 0.647 1000GENOMES:phase_3:EUR rs11642841 ## 4 rs1421085 1.000000 0.957 1000GENOMES:phase_3:EUR rs9940128 ## 5 rs1421085 0.999948 0.0552 1000GENOMES:phase_3:EUR rs73612011 ## 6 rs1421085 0.907868 0.648 1000GENOMES:phase_3:EUR rs8057044 As a result, LD.snp.region contains the r2 of our top SNP with all SNPs that were +/- 500 kb away.\nWhat if we want the correlation between all SNPs?\n Access LD matrix  For this, we need the rest API here.\nWe can calculate the LD matrix of a full region, max 1 Mb wide. For fast computation, we limit it to +/- 50 kb.\n## Query region. A maximum of 1Mb is allowed. ext \u0026lt;- glue::glue(\u0026quot;/ld/human/region/{sel.chr}:{sel.pos - range/20}..{sel.pos + range/20}/1000GENOMES:phase_3:EUR?\u0026quot;) r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) LD.matrix.region \u0026lt;- as_tibble(fromJSON(toJSON(content(r)))) %\u0026gt;% unnest() %\u0026gt;% mutate(r2 = as.numeric(r2)) ## Warning: `cols` is now required. ## Please use `cols = c(population_name, r2, variation2, variation1, d_prime)` head(LD.matrix.region) ## # A tibble: 6 x 5 ## population_name r2 variation2 variation1 d_prime ## \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; ## 1 1000GENOMES:phase_3:EUR 0.761 rs9933509 rs8057044 0.999998 ## 2 1000GENOMES:phase_3:EUR 1 rs150763868 rs72803680 1.000000 ## 3 1000GENOMES:phase_3:EUR 0.106 rs7206122 rs62033401 0.999940 ## 4 1000GENOMES:phase_3:EUR 0.731 rs9935401 rs8057044 0.999997 ## 5 1000GENOMES:phase_3:EUR 0.106 rs141816793 rs62033399 0.999943 ## 6 1000GENOMES:phase_3:EUR 0.996 rs9941349 rs9931900 1.000000  Access LD between a SNP and many other SNPs  The third and last option is to pass on a set of SNP rs ids, and access the LD among these. Implemented in the ENSEMBL API is only the LD between two SNPs, so we will have to extend this to many SNPs.\nextract_ld \u0026lt;- function(SNP.id2 = NULL, SNP.id1 = NULL, POP = NULL) { ext \u0026lt;- glue::glue(\u0026quot;/ld/human/pairwise/{SNP.id1}/{SNP.id2}/\u0026quot;) ## filter POP further down server \u0026lt;- \u0026quot;https://rest.ensembl.org\u0026quot; r \u0026lt;- GET(paste(server, ext, sep = \u0026quot;\u0026quot;), content_type(\u0026quot;application/json\u0026quot;)) stop_for_status(r) out \u0026lt;- as_tibble(fromJSON(toJSON(content(r)))) %\u0026gt;% unnest() %\u0026gt;% filter(stringr::str_detect(population_name, POP)) return(out) } ## see futher down why intersect here other.snps \u0026lt;- intersect(LD.SNP.region$variation2, data$SNP) ## cacluate LD for all other.snps SNPs LD.matrix.snps \u0026lt;- purrr::map_df(other.snps, extract_ld, snp, \u0026quot;EUR\u0026quot;) %\u0026gt;% mutate(r2 = as.numeric(r2)) %\u0026gt;% bind_rows() %\u0026gt;% unnest() ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(d_prime, variation1, variation2, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, variation1, d_prime, population_name, r2)` ## Warning: `cols` is now required. ## Please use `cols = c(variation1, d_prime, variation2, population_name, r2)` ## Warning: `cols` is now required. ## Please use `cols = c(r2, population_name, d_prime, variation1, variation2)` ## Warning: `cols` is now required. ## Please use `cols = c(population_name, r2, variation1, d_prime, variation2)` ## Warning: `cols` is now required. ## Please use `cols = c(variation2, d_prime, variation1, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c(d_prime, variation1, variation2, r2, population_name)` ## Warning: `cols` is now required. ## Please use `cols = c()` LD.matrix.snps ## # A tibble: 0 x 5 ## # … with 5 variables: variation2 \u0026lt;chr\u0026gt;, d_prime \u0026lt;chr\u0026gt;, variation1 \u0026lt;chr\u0026gt;, ## # r2 \u0026lt;dbl\u0026gt;, population_name \u0026lt;chr\u0026gt; Calculate the LD matrix (LD.matrix.region) or the LD between SNP pairs (LD.matrix.snps) takes a lot of time!\n Coloured locuszoom plot For the locuszoom plot we need only the correlation between the top SNP and all other SNPs. So we join the object LD.SNP.region to data.\ndata_ensembl \u0026lt;- data %\u0026gt;% full_join(LD.SNP.region, by = c(\u0026quot;SNP\u0026quot; = \u0026quot;variation2\u0026quot;)) ggplot(data = data_ensembl) + geom_point(aes(POS, -log10(P), color = r2), shape = 16) + labs( title = \u0026quot;Locuszoom plot for BMI GWAS\u0026quot;, subtitle = paste(\u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot;), caption = paste(\u0026quot;Data source:\u0026quot;, url) ) + geom_point( data = data_ensembl %\u0026gt;% filter(SNP == \u0026quot;rs1421085\u0026quot;), aes(POS, -log10(P)), color = \u0026quot;black\u0026quot;, shape = 16 ) + scale_color_distiller(\u0026quot;R2 (ensembl)\u0026quot;, type = \u0026quot;div\u0026quot;, palette = \u0026quot;Spectral\u0026quot;, limits = c(0, 1)) ## Warning: Removed 205 rows containing missing values (geom_point).   2. Solutions that work half-through SNPsnap   SNPsnap: https://data.broadinstitute.org/mpg/snpsnap/database_download.html uses a limited set of 1KG populations (EUR, EAS, WAFR).   API provided by sph.umich   API uses limited set of 1KG populations (ALL, EUR) see github issue     3. A solution that does not work rsnps::ld_search  A perfect solution would have been the function ld_search from R package rsnps. It has arguments to choose the reference panel, the population, the distance from the focal SNP.\nThe problem is that it only uses old reference panels (HapMap and 1KG-phase1). Meaning, many newer reference panel populations are left out.\nBut the main problem is, that the broad institute has taken down the snap server that ld_search used to access (see github issue); hence ld_search is defunct.\n  Conclusion The ldlink API with the LDproxy module seems the most perfect solution for now.\nThis will probably change with changing technology and larger reference panels.\n Session Info sessionInfo() ## R version 3.5.1 (2018-07-02) ## Platform: x86_64-apple-darwin15.6.0 (64-bit) ## Running under: macOS 10.14.5 ## ## Matrix products: default ## BLAS: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRblas.0.dylib ## LAPACK: /Library/Frameworks/R.framework/Versions/3.5/Resources/lib/libRlapack.dylib ## ## locale: ## [1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] janitor_1.2.0 data.table_1.12.2 rsnps_0.3.2.9121 ## [4] glue_1.3.1.9000 magrittr_1.5 xml2_1.2.0 ## [7] jsonlite_1.6 httr_1.4.0 forcats_0.4.0.9000 ## [10] stringr_1.4.0 dplyr_0.8.3.9000 purrr_0.3.2 ## [13] readr_1.3.1 tidyr_0.8.3.9000 tibble_2.1.3 ## [16] ggplot2_3.2.0.9000 tidyverse_1.2.1 ## ## loaded via a namespace (and not attached): ## [1] tidyselect_0.2.5 xfun_0.8 haven_2.1.1 lattice_0.20-38 ## [5] colorspace_1.4-1 vctrs_0.2.0.9000 generics_0.0.2 htmltools_0.3.6 ## [9] yaml_2.2.0 utf8_1.1.4 XML_3.98-1.20 rlang_0.4.0 ## [13] pillar_1.4.2 httpcode_0.2.0 withr_2.1.2 modelr_0.1.4 ## [17] readxl_1.3.1 plyr_1.8.4 munsell_0.5.0 blogdown_0.13 ## [21] gtable_0.3.0 cellranger_1.1.0 rvest_0.3.4 evaluate_0.14 ## [25] knitr_1.23 curl_3.3 fansi_0.4.0 triebeard_0.3.0 ## [29] urltools_1.7.3 broom_0.5.2 Rcpp_1.0.1 scales_1.0.0 ## [33] backports_1.1.4 hms_0.4.2 digest_0.6.20 stringi_1.4.3 ## [37] bookdown_0.11 grid_3.5.1 cli_1.1.0 tools_3.5.1 ## [41] lazyeval_0.2.2 crul_0.8.0 crayon_1.3.4 pkgconfig_2.0.2 ## [45] zeallot_0.1.0 ellipsis_0.2.0.1 lubridate_1.7.4 assertthat_0.2.1 ## [49] rmarkdown_1.13 rstudioapi_0.10 R6_2.4.0 icon_0.1.0 ## [53] nlme_3.1-140 compiler_3.5.1  ","date":1558396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558396800,"objectID":"eac79e0c4c3c5307b9eaee6ecedb3ae9","permalink":"https://sinarueeger.github.io/post/get-ld-remotely/","publishdate":"2019-05-21T00:00:00Z","relpermalink":"/post/get-ld-remotely/","section":"post","summary":"Goal Two approaches Our toy data 1A. A solution that works: ldlink from NIH LDproxy LDmatrix  1B. A solution that almost works: ensembl.org What reference panels/population can we choose from? Access LD between a SNP and its region Access LD matrix Access LD between a SNP and many other SNPs Coloured locuszoom plot  2. Solutions that work half-through SNPsnap API provided by sph.umich  3. A solution that does not work rsnps::ld_search  Conclusion Session Info   R-packages used:","tags":["statistical genetics","R","data visualisation"],"title":"LD Part 1","type":"post"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://sinarueeger.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   ","date":1544461200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544461200,"objectID":"d124c794115c91ba7343c2dc6d794b27","permalink":"https://sinarueeger.github.io/talk/r-ladies/","publishdate":"2017-01-01T00:00:00+02:00","relpermalink":"/talk/r-ladies/","section":"talk","summary":" Click on the Slides button above to view the built-in slides feature.   ","tags":["workflow","R","R-Ladies"],"title":"Introduction to Drake","type":"talk"},{"authors":null,"categories":null,"content":"         Goal Get started Data Add geographical coordinates Create leaflet Save the map Reason for deviation from the original   This post provides the R-Code to map the 26 populations of the 1000 Genomes project.\nGoal Create a map similar to the one1 on the front page of http://www.internationalgenome.org/ in a reproducible manner.\nVersion on internationalgenome.org\n  Get started Packages needed:\n## accessed via :: # library(mapview) # library(readxl) # library(readr) # library(purrr) # library(tidyr) # library(forcats) library(leaflet) library(dplyr) library(ggmap) ## for geocode, devtools::install_github(\u0026quot;dkahle/ggmap\u0026quot;) ggmap requires a google map api key2:\nget one here: https://developers.google.com/maps/documentation/geocoding/get-api-key then run register_google(key = \"my_api_key\")   Data  The population counts and labels are from ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130606_sample_info/ (download xlsx file). The super population labels are from here (pasted into a csv, then the location was inferred).  Download the population counts and labels first:\nurl \u0026lt;- \u0026quot;ftp.1000genomes.ebi.ac.uk/vol1/ftp/technical/working/20130606_sample_info/20130606_sample_info.xlsx\u0026quot; url.bitly \u0026lt;- \u0026quot;http://bit.ly/2MQTr02\u0026quot; download.file(url, \u0026quot;20130606_sample_info.xlsx\u0026quot;, mode = \u0026quot;wb\u0026quot;) Import file into R:\ndf \u0026lt;- readxl::read_excel(\u0026quot;20130606_sample_info.xlsx\u0026quot;, sheet = \u0026quot;Sample Info\u0026quot;) # \u0026gt;\u0026gt; Sample Info Some data wrangling:\n## count number of individuals by population ## rename population \u0026gt; POP n.pop \u0026lt;- df %\u0026gt;% group_by(Population) %\u0026gt;% summarise(n = n()) %\u0026gt;% rename(POP = Population) ## import super population names and details to the location of populations ## copied from here: url.spop \u0026lt;- \u0026quot;http://www.internationalgenome.org/faq/which-populations-are-part-your-study/\u0026quot; ## added location manually (!) - found this the only option to prevent overlapping locations. ## Also, description involves a mix of location and origin. ## rename superpopulation \u0026gt; SPOP n.spop \u0026lt;- readr::read_csv(\u0026quot;../../static/post/2018-12-05-1000genomes-map/sample_info_superpop.csv\u0026quot;) %\u0026gt;% rename(POP = `Population Code`, SPOP = `Super Population Code`) ## join the two information n.1kg \u0026lt;- left_join(n.pop, n.spop, by = c(\u0026quot;POP\u0026quot; = \u0026quot;POP\u0026quot;))  Add geographical coordinates Parts of the code below is from a map created by Daniela Vazquez for R-Ladies: https://github.com/rladies/Map-RLadies-Growing.\nThis is the part where we annotate the dataframe n.1kg with where the individuals live (not their ancestry). Repeat this until there are no warnings() about QUERY LIMITS (the while loop takes care of this).\nWe will use the ggmap package, which accesses the google maps api.\nA workaround is to set source = \"dsk\" (works for a limited number of queries)3.\nn.1kg \u0026lt;- n.1kg %\u0026gt;% mutate(purrr::map(.$location, geocode, source = \u0026quot;dsk\u0026quot;)) %\u0026gt;% tidyr::unnest() ## running into the inevitable QUERY LIMITS problems, lets use the approach from https://github.com/rladies/Map-RLadies-Growing n.1kg.withloc \u0026lt;- n.1kg %\u0026gt;% filter(!is.na(lon)) while(nrow(n.1kg.withloc) != nrow(n.1kg)) { # repeat this until there are no warnings() about QUERY LIMITS temp \u0026lt;- n.1kg %\u0026gt;% select(-lon, -lat) %\u0026gt;% anti_join(n.1kg.withloc %\u0026gt;% select(-lon, -lat)) %\u0026gt;% mutate(longlat = purrr::map(.$location, geocode, source = \u0026quot;dsk\u0026quot;)) %\u0026gt;% tidyr::unnest() %\u0026gt;% filter(!is.na(lon)) n.1kg.withloc \u0026lt;- n.1kg.withloc %\u0026gt;% bind_rows(temp) %\u0026gt;% distinct() } n.1kg \u0026lt;- n.1kg.withloc ## glue POP and `Population Description` together n.1kg \u0026lt;- n.1kg %\u0026gt;% mutate(pop.desc = paste0(POP, \u0026quot; : \u0026quot;, `Population Description`, \u0026quot; (\u0026quot;, SPOP, \u0026quot;)\u0026quot;)) ## given that only a number of geolocation are possible with the google API, this ## should probably stored out ## readr::write_csv(n.1kg, path = \u0026quot;1kg_sample_info_location.csv\u0026quot;)  Create leaflet Map locations a world map with leaflet\n## if you have stroed the data in the previous chunk: ## readr::read_csv(\u0026quot;1kg_sample_info_location.csv\u0026quot;) Define shiny icons:\nicons \u0026lt;- awesomeIcons( icon = \u0026#39;user\u0026#39;, #people\u0026#39;, iconColor = \u0026#39;black\u0026#39;, library = \u0026#39;fa\u0026#39;, #ion markerColor = as.character(forcats::fct_recode(as.factor(n.1kg$SPOP), red = \u0026quot;EUR\u0026quot;, blue = \u0026quot;AFR\u0026quot;, green = \u0026quot;AMR\u0026quot;, gray = \u0026quot;EAS\u0026quot;, orange = \u0026quot;SAS\u0026quot;)) ## ok, thats not too pretty, but turns out, hex colors won\u0026#39;t work ) ## we need to create a vector that maps cols to SPOP from the markerColor argument above cols \u0026lt;- c(\u0026quot;#E50102\u0026quot;, \u0026quot;#00A9DD\u0026quot;, \u0026quot;#57BA1F\u0026quot;, \u0026quot;#575757\u0026quot;, \u0026quot;#FD8E00\u0026quot;) SPOP \u0026lt;- c(\u0026quot;EUR\u0026quot;, \u0026quot;AFR\u0026quot;, \u0026quot;AMR\u0026quot;, \u0026quot;EAS\u0026quot;, \u0026quot;SAS\u0026quot;) ## separate icon that will display the information ## ------------------------------------------------ icon.info \u0026lt;- awesomeIcons( icon = \u0026#39;info\u0026#39;, #people\u0026#39;, iconColor = \u0026#39;white\u0026#39;, library = \u0026#39;fa\u0026#39;, #ion markerColor = \u0026quot;white\u0026quot; ) Create map:\nm \u0026lt;- leaflet(data = n.1kg) %\u0026gt;% addTiles() %\u0026gt;% # Add default OpenStreetMap map tiles addAwesomeMarkers(lat=~lat, lng=~lon, label = ~htmltools::htmlEscape(pop.desc), icon = icons) %\u0026gt;% addAwesomeMarkers(lat=-45, lng=-107, popup = glue::glue(\u0026quot;Source: https://github.com/sinarueeger/map-1000genomes\u0026quot;), icon = icon.info) %\u0026gt;% ## this bit has potential to be displayed as a href. #glue::glue(\u0026quot;Source: {url.bitly} + {url.spop} (manual tidying)\u0026quot;), icon = icon.info) %\u0026gt;% addLegend(\u0026quot;bottomright\u0026quot;, colors =cols, labels= SPOP, opacity = 1) m # Print the map  {\"x\":{\"options\":{\"crs\":{\"crsClass\":\"L.CRS.EPSG3857\",\"code\":null,\"proj4def\":null,\"projectedBounds\":null,\"options\":{}}},\"calls\":[{\"method\":\"addTiles\",\"args\":[\"//{s}.tile.openstreetmap.org/{z}/{x}/{y}.png\",null,null,{\"minZoom\":0,\"maxZoom\":18,\"tileSize\":256,\"subdomains\":\"abc\",\"errorTileUrl\":\"\",\"tms\":false,\"noWrap\":false,\"zoomOffset\":0,\"zoomReverse\":false,\"opacity\":1,\"zIndex\":1,\"detectRetina\":false,\"attribution\":\"\u0026copy; OpenStreetMap contributors, CC-BY-SA\"}]},{\"method\":\"addAwesomeMarkers\",\"args\":[[13.16667,34.5003,24,22.08829,40.11995,39.9075,35,4,10,60.16952,50.416667,29.813142,13.5,40,54.75844,35.6895,10.75,0.60751,7.13833,34.05223,-10,31.54972,18.24829,55.9483399,43.41667,7.38778],[-59.53333,-111.50098,90,101.0248,-111.67031,116.39723,105,-72,8,24.93545,-4.75,-95.309789,-15.5,-4,-2.69531,139.69171,106.66667,34.76966,-11.67056,-118.24368,-76,74.34361,-66.49989,-3.1932723,11,3.89639],{\"icon\":\"user\",\"markerColor\":[\"blue\",\"blue\",\"orange\",\"gray\",\"red\",\"gray\",\"gray\",\"green\",\"blue\",\"red\",\"red\",\"orange\",\"blue\",\"red\",\"orange\",\"gray\",\"gray\",\"blue\",\"blue\",\"green\",\"green\",\"orange\",\"green\",\"orange\",\"red\",\"blue\"],\"iconColor\":\"black\",\"spin\":false,\"squareMarker\":false,\"iconRotate\":0,\"font\":\"monospace\",\"prefix\":\"fa\"},null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},null,null,null,null,[\"ACB : African Caribbeans in Barbados (AFR)\",\"ASW : Americans of African Ancestry in SW USA (AFR)\",\"BEB : Bengali from Bangladesh (SAS)\",\"CDX : Chinese Dai in Xishuangbanna, China (EAS)\",\"CEU : Utah Residents (CEPH) with Northern and Western European Ancestry (EUR)\",\"CHB : Han Chinese in Beijing, China (EAS)\",\"CHS : Southern Han Chinese (EAS)\",\"CLM : Colombians from Medellin, Colombia (AMR)\",\"ESN : Esan in Nigeria (AFR)\",\"FIN : Finnish in Finland (EUR)\",\"GBR : British in England and Scotland (EUR)\",\"GIH : Gujarati Indian from Houston, Texas (SAS)\",\"GWD : Gambian in Western Divisions in the Gambia (AFR)\",\"IBS : Iberian Population in Spain (EUR)\",\"ITU : Indian Telugu from the UK (SAS)\",\"JPT : Japanese in Tokyo, Japan (EAS)\",\"KHV : Kinh in Ho Chi Minh City, Vietnam (EAS)\",\"LWK : Luhya in Webuye, Kenya (AFR)\",\"MSL : Mende in Sierra Leone (AFR)\",\"MXL : Mexican Ancestry from Los Angeles USA (AMR)\",\"PEL : Peruvians from Lima, Peru (AMR)\",\"PJL : Punjabi from Lahore, Pakistan (SAS)\",\"PUR : Puerto Ricans from Puerto Rico (AMR)\",\"STU : Sri Lankan Tamil from the UK (SAS)\",\"TSI : Toscani in Italia (EUR)\",\"YRI : Yoruba in Ibadan, Nigeria (AFR)\"],{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addAwesomeMarkers\",\"args\":[-45,-107,{\"icon\":\"info\",\"markerColor\":\"white\",\"iconColor\":\"white\",\"spin\":false,\"squareMarker\":false,\"iconRotate\":0,\"font\":\"monospace\",\"prefix\":\"fa\"},null,null,{\"interactive\":true,\"draggable\":false,\"keyboard\":true,\"title\":\"\",\"alt\":\"\",\"zIndexOffset\":0,\"opacity\":1,\"riseOnHover\":false,\"riseOffset\":250},\"Source: https://github.com/sinarueeger/map-1000genomes\",null,null,null,null,{\"interactive\":false,\"permanent\":false,\"direction\":\"auto\",\"opacity\":1,\"offset\":[0,0],\"textsize\":\"10px\",\"textOnly\":false,\"className\":\"\",\"sticky\":true},null]},{\"method\":\"addLegend\",\"args\":[{\"colors\":[\"#E50102\",\"#00A9DD\",\"#57BA1F\",\"#575757\",\"#FD8E00\"],\"labels\":[\"EUR\",\"AFR\",\"AMR\",\"EAS\",\"SAS\"],\"na_color\":null,\"na_label\":\"NA\",\"opacity\":1,\"position\":\"bottomright\",\"type\":\"unknown\",\"title\":null,\"extra\":null,\"layerId\":null,\"className\":\"info legend\",\"group\":null}]}],\"limits\":{\"lat\":[-45,60.16952],\"lng\":[-118.24368,139.69171]}},\"evals\":[],\"jsHooks\":[]}  Save the map ## save to png ## ------------ mapview::mapshot(m, file = \u0026quot;map-1000genomes-populations.png\u0026quot;) ## save to hmtl ## ------------- htmlwidgets::saveWidget(m, file=\u0026quot;map-1000genomes-populations.html\u0026quot;)  Reason for deviation from the original I mapped the populations according to the current location but coloured them according to ancestry.\n  This is a png and cannot be altered.↩\n Seen here: https://stackoverflow.com/questions/36175529/getting-over-query-limit-after-one-request-with-geocode.↩\n See https://stackoverflow.com/questions/36175529/getting-over-query-limit-after-one-request-with-geocode.↩\n   ","date":1543968000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543968000,"objectID":"5a079f2f8096ce6048f7aa7477164910","permalink":"https://sinarueeger.github.io/post/1kgmap/","publishdate":"2018-12-05T00:00:00Z","relpermalink":"/post/1kgmap/","section":"post","summary":"Goal Get started Data Add geographical coordinates Create leaflet Save the map Reason for deviation from the original   This post provides the R-Code to map the 26 populations of the 1000 Genomes project.\nGoal Create a map similar to the one1 on the front page of http://www.internationalgenome.org/ in a reproducible manner.\nVersion on internationalgenome.org\n  Get started Packages needed:","tags":["data visualisation","R","maps"],"title":"Create a map of the 1000 Genomes project reference populations","type":"post"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   ","date":1543917600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543917600,"objectID":"e2e2dbd61149f7b5ffc2983cea602799","permalink":"https://sinarueeger.github.io/talk/r-lunchs/","publishdate":"2017-01-01T00:00:00+02:00","relpermalink":"/talk/r-lunchs/","section":"talk","summary":" Click on the Slides button above to view the built-in slides feature.   ","tags":["workflow","best practices","R"],"title":"Workflow \u0026 best practices for projects","type":"talk"},{"authors":null,"categories":null,"content":"  What is a “project folder”? Why now? Why we tidy up: authority and incentive Challenges What I want The options Drake Getting started More Examples Resources  But wait: drake does not care about messy folders What is next? When is the right time to tidy Is it worth it?   Recently, I started to seriously1 think about the tidiness of data analysis project folders and the implications of tidying up.\nI was lucky enough to talk about what I have figured out so far at the Genève R User Group. While I am not done yet with reflecting on this2, I wanted to write down my thoughts that lead to my presentation3. So what follows is just “thinking out loud”.\nUpdate: In February 2019, Amanda Dobbyn gave a talk at R-Ladies NYC about drake. All material here.\nTrailer. Your browser does not support the video tag.   Presentation trailer made with Maëlle Salmon’s instructions.   What is a “project folder”? To me, a project folder is anything that contains the (R-)scripts necessary to run a data analysis and create the corresponding report. It is like a framed piece of work that you can take and place somewhere else. And probably it will take the form of the Figure from the R4DS book below:\n  Adapted from Figure in R4DS book.    Ideally, you should be able to take that folder as it is, run it on another computer and get the same results. Unfortunately, this is not always the case - at least with my project folders.\nI think that the tidiness of a project folder, how it is structured and how it tells the user to execute what and when, correlate strongly with the whole repeatability, replicability and reproducibility aspect.\n Why now? The reason I started to dig deeper into workflow management possibilities in R, is, that I was changing jobs, and I had to clean up my old project folders from almost five years of analysing genetic data 😱. And so I faced this gigantic mess a bit of a mess, spread over several servers and computers, some version controlled, others not, with implemented “best practices” from different waves of trying to improve. I tried to clean up as good as I could, but I told myself that this would not happen again. At my new job, I would use version control for everything, and I would use something make-like (e.g. remake) to indicate the “recipe” of a project and be in control of what is recomputed and what is not4.\n Why we tidy up: authority and incentive I have a long-time interest in tidiness in general, and from studying my behaviour I came up with the theory that tidiness is only present when a) somebody tells you to do it, or b) you are rewarded for it.\nHere are some examples:\n If you want to compile an R-package you have little to no freedom in how to name folders. You must have a given folder and file structure. Otherwise, it won’t compile. This dictated and unified folder structure makes it easy for R users to understand what is where in an R-package. No matter who coded it.    R package structure. Figure from http://r-pkgs.had.co.nz/package.html.     If you work on several different projects at the same time, it is beneficial to have structure, so that you can quickly dive back into a project.\n Following good practices also leaves you more time to do the fun stuff, like modelling and creating data visualisation.\n   Challenges I started by wondering why maintaining a tidy and coherent folder structure was so difficult for me to maintain. So I came up with a list (which is undoubtedly going to change over time):\n Having different places for computation (Laptop, Server1, Server2, …). Not using git consistently. Unclear separation of the folders data (raw input data), processed-data and output-data (results). Data deliveries: data hardly ever arrives in one tidy folder, but instead comes at different time points and so poses other challenges. Having many different best practices implemented: so each project would have its own set of folder names and file naming convention, leading to little overview of the analysis and its iteration steps → cleaning, modelling, visualisation, reports. Using similar code in many different R scripts → redundant code. Having no punishment for not cleaning up (and also not seeing the benefit).   What I want Then I asked myself what I want to achieve with implementing (and sticking to) something new.\nMaking it easy for colleagues at work to rerun (and understand) the project → “repeatability” Making it easy for others to rerun and to understand the project → “reproducibility”5 Making it easy for others to rerun the code with different data → “replicability”  Next, I looked for solutions. First, I would need to use coherent folder names. Second, I would need to have a file that indicates the recipe of an analysis. Third, I would implement most free floating and redundant code into functions. Fourth, I would minimise unnecessary computation by caching results. Fifth, I would start using unit tests6.\n The options There are many different ready-to-use software packages out there. I was thinking of going back to using make, that I used years ago. Then I came across {remake}, which seemed just what I needed. A colleague at work was using stu and was recommending it. But then the Swiss Institute of Bioinformatics offered a course on Make-like declarative workflows with R taught by Kirill Müller, which I could not attend. Luckily, thanks to the excellent online course material, I could learn it by myself.\n Drake The presentation Make-like declarative workflows with R presented the R-package {drake} (drake = Data Frames in R for Make7).\n{Drake} was created by Will Landau and reviewed by rOpenSci. On the github page it says that {drake} is a “general-purpose workflow manager for data-driven tasks”. Sounds perfect!\nThe way I understand it is, that it is based on make (and overlaps with the R-package {remake}). Therefore when making a change to an analysis and re-running it, it only re-compute the dependent parts. But compared to make, {drake} is much more convenient to use. Plus it is scalable to parallel computing. And it is intuitive to use, meaning, colleagues can learn it quickly.\nGetting started Best is, to run the mini example provided in the package, and then go from there. Drake has many other examples provided; you can check them by running drake::drake_examples().\ninstall.packages(\"drake\") Run drake::drake_example(\"main\") → this will download a folder called main. Go to the terminal. You can look at all the files contained in main by writing tree main (this works on MacOS)  main/ ├── COPYRIGHT.md ├── LICENSE.md ├── README.md ├── clean.R ├── make.R ├── raw_data.xlsx └── report.Rmd Next, open make.R. The key functions are drake_plan() and make(). Add the following bit before and after make(plan).  config \u0026lt;- drake_config(plan) vis_drake_graph(config)  Run all code for a first time. Change something (e.g. the plot function). Rerun and watch the colours change in vis_drake_graph(config). Use functions readd() and loadd() to work with the produced output. checkout .drake/ folder. This is where all the cached work is stored.  By running this example, you will see that drake_plan() is used to create a recipe of the analysis and make() is used to execute that recipe. make() will create objects, such as fit and hist in the example and store them in the folder .drake/.\nreadd() is used to return an object from cache. This is handy when we only want to display an object. loadd() on the other hand is used to load an object into our session (similarly to load).\n More To further checkout options I recommend - The slides from Christine Stawitz (presented at R-Ladies Seattle in June 2018). - The material by Amanda Dobbyn (presented at R-Ladies NYC in February 2019). (Update)\nBoth presentations provide a good overview of the options {drake} provides.\n Examples I also created some tiny examples that use genetic data. It has four folders:\nwild-west: this is how I was structuring folders till now (this example was used to introduce the analysis during the presentation). wild-west-pro: same as 1. but with an README.md. drake: implementing 1. into drake. drake-adv: implementing 1. into a more realistic, hierarchical folder structure.  The examples use genetic data that was originally used in the crowdAI openSNP height prediction challenge. The full openSNP data set was prepared by my colleague Olivier Naret and can be downloaded here. The examples use a small subset of the entire dataset that can be downloaded here.\n Resources Here are a bunch of resources that helped me understand {drake}:\n Github Repo This tutorial and cheatsheet by Kirill Müller. Overview of options: Make-like declarative workflows with R by Christine Stawitz. Best practices for drake projects. Lots of tutorials and examples.    But wait: drake does not care about messy folders True! I can have a make.R file anywhere and it will still work. But I believe that the shift in logic that you have to get used to with {drake} makes you care more about folder structure.\n What is next? I am currently reading the PlosCompBio paper Good enough practices in scientific computing - a great read, giving me lots of ideas!\nI want to use {drake} in a more complex setting. There are also other R-packages that help with project workflows. And I should invest some time to come up with a test suite for data analysis projects.\n When is the right time to tidy At the Genève RUG meetup, we were also discussing when we think is the right time to tidy up.\nProject folders evolve. Especially at the beginning of a project, we are busy figuring things out, wrangling data, fitting models, making plots and telling people what we found out. This can take some time. But at one point we are ready to write a report.\nIt is probably at that stage (when we write a report) that we can “frame” that project into something that is “stable” and “portable”.\nAlthough - I am not sure we have to wait that long. I think the benefits of {drake} (e.g. caching) already help us at an earlier stage.\n Is it worth it? I think there is a trade-off between dedicating days to tidying up and not caring about structure at all. Same with tooling. For example, if we use a tool, say make, but no one else but us knows how to use it, it is going to be hard for colleagues to understand and use project folders that use make. We have to keep that balance in mind.\n  Seriously, meaning, different from previous, half-hearted attempts.↩\n Just started reading Good enough practices in scientific computing - great paper!↩\n Thanks to Maëlle for pointing out that this is a good thing to do!↩\n And while at it, I would totally decrease my coffee consumption too and never procrastinate again 😉.↩\n The terminology is really confusing at times. I rely on this definition. ↩\n Thanks to my colleague for the idea!↩\n I am still wondering how “Data Frames in R for Make” adds up to “drake” 🤔.↩\n   ","date":1539043200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539043200,"objectID":"6afbc2ff36dc45b7457de9036b143829","permalink":"https://sinarueeger.github.io/post/drake/","publishdate":"2018-10-09T00:00:00Z","relpermalink":"/post/drake/","section":"post","summary":"What is a “project folder”? Why now? Why we tidy up: authority and incentive Challenges What I want The options Drake Getting started More Examples Resources  But wait: drake does not care about messy folders What is next? When is the right time to tidy Is it worth it?   Recently, I started to seriously1 think about the tidiness of data analysis project folders and the implications of tidying up.","tags":["drake","R","projects"],"title":"Tidying workflows in R","type":"post"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   ","date":1538668800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538668800,"objectID":"7c868d5e4adaad351525337d14504726","permalink":"https://sinarueeger.github.io/talk/geneve-rug/","publishdate":"2017-01-01T00:00:00+02:00","relpermalink":"/talk/geneve-rug/","section":"talk","summary":" Click on the Slides button above to view the built-in slides feature.   ","tags":["workflow","R"],"title":"Tidying workflows \u0026 R community","type":"talk"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   ","date":1536766200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536766200,"objectID":"fd0549ad004f2a2079c743f9f3a0a32d","permalink":"https://sinarueeger.github.io/talk/geek-girls-carrots/","publishdate":"2017-01-01T00:00:00+02:00","relpermalink":"/talk/geek-girls-carrots/","section":"talk","summary":" Click on the Slides button above to view the built-in slides feature.   ","tags":["R","introduction"],"title":"An introduction to (problem solving with) R","type":"talk"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;\n","date":1536440400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536440400,"objectID":"6a451186c775f5f0adb3a0416d0cb711","permalink":"https://sinarueeger.github.io/tutorial/example/","publishdate":"2018-09-09T00:00:00+03:00","relpermalink":"/tutorial/example/","section":"tutorial","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 \u0026hellip;\nTip 2 \u0026hellip;","tags":null,"title":"Example Page","type":"docs"},{"authors":["**R\u0026uuml;eger, S**","McDaid, A","Kutalik, Z"],"categories":null,"content":"","date":1535749200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535749200,"objectID":"b4edcfb7d66f21452231eaaa94f14806","permalink":"https://sinarueeger.github.io/publication/plosgen-2018-ssimp-application/","publishdate":"2018-09-01T00:00:00+03:00","relpermalink":"/publication/plosgen-2018-ssimp-application/","section":"publication","summary":"","tags":[""],"title":"Evaluation and application of summary statistic imputation to discover new height-associated loci","type":"publication"},{"authors":["**R\u0026uuml;eger, S**","McDaid, A","Kutalik, Z"],"categories":null,"content":"","date":1535749200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535749200,"objectID":"e364d4763049b6bb668db3250aa8d3c5","permalink":"https://sinarueeger.github.io/publication/biorxiv-2018-ssimp-method/","publishdate":"2018-09-01T00:00:00+03:00","relpermalink":"/publication/biorxiv-2018-ssimp-method/","section":"publication","summary":"","tags":[""],"title":"Improved imputation of summary statistics for admixed populations","type":"publication"},{"authors":null,"categories":null,"content":"   Goal Getting it done 1. Get summary statistics Visualising associations Identify genomic region with lowest P-value  2. Extracting annotation biomaRt Learning resources Using biomaRt Quick d-tour: assembly GRCh37 or GRCh38? Extracting gene name for one SNP Extracting gene names for genomic region  3. Combining summary statistics and annotation Wouldn’t it be nice… Source   In the world of genome-wide association studies (GWAS), we often get a list of genetic markers (SNPs) that seem for some reason relevant for a particular outcome. At the same time, we have little knowledge about these genetic variants that come in cryptic combinations of characters and numbers.\nFor example:\n We might get asked how frequent the SNP rs1421085 is in a range of populations.  We need to extract all known SNPs that are within 1 Mb of rs1421085.  A genomic region turns out to be highly relevant for some disease, and we want to know all genes contained in that genomic region.  Unless you sit on lots of genetic data, the list of SNPs come from a summarised form. Minimally, these summary statistic datasets contain the SNP identifier (SNPNAME), the effect size (beta) and the standard error (se). Sometimes the position (POS) and chromosome (CHR) is provided instead of the SNP identifier, and sometimes both are available. Then there is usually other information coming from the study data; for example, the allele frequency in the study  this is shown on the LHS in the table below. However, hardly ever do the datasets contain annotation, such as the gene where the SNP resides, the phenotypes it is associated with or the minor allele frequency (MAF) in a specific population  this is illustrated in the green coloured part in the table below.\n  GWAS summary statistics dataset   Annotation     CHR  POS  SNPNAME  REF  ALT  beta  se  Gene  Linked phenotype  Global MAF      7  75163169  rs1167827  A  G  0.028  0.0032  HIP1  BMI  0.4720450    12  122781897  rs11057405  G  A  -0.021  0.0037  CLIP1  BMI  0.0327476    10  114758349  rs7903146  C  T  -0.031  0.0023  TCF7L2  BMI  0.2278350    1  49589847  rs657452  A  G  -0.015  0.0025  AGBL4  BMI  0.4532750    1  49589847  rs657452  A  G  -0.015  0.0025  AC099788.1  BMI  0.4532750     This makes sense. Genetic data from cohorts will be used for years and won’t change, but the annotation will change over the years. Take the chromosomal position of SNPs that changes with every new human genome reference assembly.\nTo get our hands on annotation, we need to consult external databases that are - lucky us! - public. But first, let us define what we want to see at the end of the blog post.\nGoal For this blog post, we keep it simple and only focus on the genes contained in a genomic region, but this can be easily extended to any other annotation available in databases.\nIn the illustration below1, we want to know the gene starting and end positions around one particular gene region to create an informative locuszoom plot.\n  In this case, we want to visualise the GWAS P-value (-log10(P-value)) of a genomic region (with each point representing a SNP), with the corresponding genes at the bottom. This is similar to a plot done with LocusZoom, a tool that takes summary statistics as input and outputs a pretty graph for a desired genomic region, including gene annotation, LD information and more.     Getting it done There are currently more than 10 Mio SNPs known, and knowing their functions and genes by heart would equal to some superpower. Which is why we 2 public databases, such as dbSNP and ensembl3.\nWhat we like even more, is, to make our analyses reproducible and automate annotation and lookups.\n In this blog post, I will show how to zoom into GWAS results and annotate the plot based on the information about that genomic region using R. Here is the plan:\nGet summary statistics Extract annotation Combine summary statistics and annotation  Before tackling the first item, we want to have all R-packages installed \u0026amp; ready to use:\n## Packages needed to run code ## ---------------------------- # install.packages(\u0026quot;dplyr\u0026quot;) ## data manipulation # install.packages(\u0026quot;data.table\u0026quot;) ## read data, fast! # install.packages(\u0026quot;forcats\u0026quot;) ## dealing with factors # install.packages(\u0026quot;ggplot2\u0026quot;) ## dataviz # install.packages(\u0026quot;magrittr\u0026quot;) ## piping # install.packages(\u0026quot;metafolio\u0026quot;) ## colorpalette # install.packages(\u0026quot;skimr\u0026quot;) ## summarising data # install.packages(\u0026quot;qqman\u0026quot;) ## Manhattan plot # install.packages(\u0026quot;patchwork\u0026quot;) ## assembling plots # source(\u0026quot;https://bioconductor.org/biocLite.R\u0026quot;) # biocLite(\u0026quot;biomaRt\u0026quot;) ## annotation ## Optional packages for Rmd ## -------------------------- # install.packages(\u0026quot;kableExtra\u0026quot;) ## making pretty tables # install.packages(\u0026quot;devtools\u0026quot;) # devtools::install_github(\u0026quot;hadley/emo\u0026quot;) ## emojis # devtools::install_github(\u0026quot;ropenscilabs/icon\u0026quot;) ## icons  1. Get summary statistics First, we need some GWAS summary statistics.\nThere are lots of resources for publicly available GWAS summary statistics.\nWe will look at BMI, because it can be accessed easily4 and because it is relatively small for a genomic dataset. The data is from the Genetic Investigation of ANthropometric Traits (GIANT) consortium. You can download the dataset5 here or load it directly into R.\n## Data Source URL url \u0026lt;- \u0026quot;https://portals.broadinstitute.org/collaboration/giant/images/2/21/BMI_All_ancestry.fmt.gzip\u0026quot; #url \u0026lt;- \u0026quot;jenger.riken.jp/1analysisresult_qtl_download/All_2017_BMI_BBJ_autosome.txt.gz\u0026quot; ## Import BMI summary statistics dat.bmi \u0026lt;- read_tsv(file = url) ## ## taking too long, let\u0026#39;s use fread instead. dat.bmi \u0026lt;- data.table::fread(url, verbose = FALSE) I added verbose = FALSE because it will complain that there is an unexpected character in column 1, which appears to be numerical. This is because chromosome X will only appear towards the end of the dataset.\nNext, we rename some columns to something more conventional.\n## Rename some columns dat.bmi \u0026lt;- dat.bmi %\u0026gt;% rename(SNP = SNPNAME, P = Pvalue) Now, let’s look at the data with the skimr package.\nskimr::skim(dat.bmi) ## Skim summary statistics ## n obs: 246328 ## n variables: 10 ## ## ── Variable type:character ────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n min max empty n_unique ## ALT 0 246328 246328 1 1 0 4 ## CHR 0 246328 246328 1 2 0 23 ## ExAC_MAF 0 246328 246328 1 325 0 51423 ## GMAF 0 246328 246328 1 17 0 9964 ## REF 0 246328 246328 1 1 0 4 ## SNP 0 246328 246328 1 11 0 243206 ## ## ── Variable type:integer ──────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 p50 ## POS 0 246328 246328 7.6e+07 5.7e+07 11885 3.2e+07 6.1e+07 ## p75 p100 hist ## 1.1e+08 2.5e+08 ▇▇▅▅▃▂▁▁ ## ## ── Variable type:numeric ──────────────────────────────────────────────────────────────────────────────────── ## variable missing complete n mean sd p0 p25 ## beta 977 245351 246328 0.00051 0.13 -3.2 -0.032 ## P 977 245351 246328 0.48 0.3 8.6e-269 0.21 ## se 0 246328 246328 Inf NaN 0.002 0.023 ## p50 p75 p100 hist ## -0.00017 0.031 3.2 ▁▁▁▇▇▁▁▁ ## 0.47 0.73 1 ▇▇▆▆▆▆▆▆ ## 0.059 0.11 Inf ▇▁▁▁▁▁▁▁ The reference allele (REF), alternative allele (ALT), SNP identifier (SNP) and chromosome (CHR) are characters. There are four unique values for ALT and REF: A, C, G, T, and 23 unique values for CHR - seems about right. The two columns with the minor allele frequencies measured in GIANT and ExAC datasets (GMAF, ExAC_MAF) are characters too because the allele is concatenated. The chromosomal position (POS) is an integer. Then there is the actual association of each SNP with BMI (beta, se, P).\nThis dataset has 246328 rows and 10 columns. What if we want to visualise this all at once? In particular, we are interested if there are ANY associations between the genetic markers and BMI. The mini P distribution in the skimr output does not reveal much.\nVisualising associations Visualising all associations at once can be done with a Manhattan plot, where the x-axis represents chromosomeCHR and the chromosomal position POS, and the y-axis the -log10(P)-value. Let’s use the R-package qqman6 for that.\nqqman::manhattan(dat.bmi %\u0026gt;% mutate(CHR = as.numeric(as.character(fct_recode(CHR, \u0026quot;23\u0026quot; = \u0026quot;X\u0026quot;)))) %\u0026gt;% filter(-log10(P)\u0026gt;1), chr=\u0026quot;CHR\u0026quot;, bp=\u0026quot;POS\u0026quot;, snp=\u0026quot;SNP\u0026quot;, p=\u0026quot;P\u0026quot;, suggestiveline =FALSE, genomewideline = FALSE, chrlabs = c(1:22, \u0026quot;X\u0026quot;), cex = 0.4) We can spot immediately, that there are loads of SNPs with P-values smaller than \\(10^{-100}\\) (sample size was around 700K).\nOf course, there are many other solutions to spot real associations, but this is not the point of this blog post ;-).\n Identify genomic region with lowest P-value Now that we know that there are lots of genetic markers associated with BMI, we want to look at a specific genomic region and figure out what genes it contains. For illustrative purposes, we pick the genomic region with the lowest P-value.\ndat.bmi.sel \u0026lt;- dat.bmi %\u0026gt;% slice(which.min(P)) dat.bmi.sel ## CHR POS REF ALT SNP GMAF ExAC_MAF beta se P ## 1 16 53800954 T C rs1421085 C:0.2286 - 0.078 0.0022 8.6e-269 SNP identifier rs1421085 that has the lowest P-value (\\(P = 8.6\\times 10^{-269}\\)).\nNow we can visualise the summary statistics of that genomic region (\\(\\pm 500 \\cdot 10^{3}\\)).\nrange \u0026lt;- 5e+05 sel.chr \u0026lt;- dat.bmi.sel$CHR sel.pos \u0026lt;- dat.bmi.sel$POS dat.bmi.sel.region \u0026lt;- dat.bmi %\u0026gt;% filter(CHR == sel.chr, between(POS, sel.pos - range, sel.pos + range)) p1 \u0026lt;- ggplot(data = dat.bmi.sel.region) + geom_point(aes(POS, -log10(P)), shape = 1) + labs(title = \u0026quot;Locuszoomplot for BMI GWAS\u0026quot;, subtitle = paste(\u0026quot;Summary statistics for chromosome\u0026quot;, sel.chr, \u0026quot;from\u0026quot;, format((sel.pos - range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;to\u0026quot;, format((sel.pos + range), big.mark = \u0026quot;\u0026#39;\u0026quot;), \u0026quot;bp\u0026quot;), caption = paste(\u0026quot;Data source:\u0026quot;, url)) print(p1) Next, we want to know if rs1421085 is part of a gene, and if yes, which one.\n  2. Extracting annotation biomaRt Thankfully, there is an R-package called biomaRt that can do this for us.\nI had only come across this package a few weeks ago, so I apologise in advance that I won’t make full use of all the features that the package offers.\n biomaRt should not be confused with biomartr. biomartr is an rOpenSci package by Hajk-Georg Drost. I needed this tweet  to realise that these were two separate yet related R-packages.\nDid you ever want to reproducibly retrieve thousands of genomes across the tree of life using only one R command? Then have a look at the new version of biomartr which is on its way to CRAN! https://t.co/kWF5XCoGhj #bioinformatics #rstats #Genomics pic.twitter.com/EZHJP0n1f9 — Hajk-Georg Drost (@HajkDrost) June 28, 2018   So far, I did not fully grasp the benefits of biomartr compared to biomaRt for annotation of human data. But I will look more into it.\nI also got a tip from Marianna Foos to check out rsnps, an R-package dealing with SNP annotation.\n Learning resources I used mainly two sources to learn what I wanted to do:\n Vignette of biomaRt on Bioconductor7. This question on StackOverflow.   Using biomaRt First, we need to load the biomaRt package from Bioconductor.\nlibrary(biomaRt)  Next, we specify which database to use. You can use the functions listMart(), listEnsembl() and listDatasets() to select from the right biomart and dataset. We will need to extract SNPs, hence biomart = \"snp\".\nsnp.ensembl \u0026lt;- useEnsembl(biomart = \u0026quot;snp\u0026quot;, dataset = \u0026quot;hsapiens_snp\u0026quot;) class(snp.ensembl) ## [1] \u0026quot;Mart\u0026quot; ## attr(,\u0026quot;package\u0026quot;) ## [1] \u0026quot;biomaRt\u0026quot; # other ways of selecting the mart snp.mart \u0026lt;- useMart(biomart = # \u0026#39;ENSEMBL_MART_SNP\u0026#39;, dataset=\u0026#39;hsapiens_snp\u0026#39;) # gene.mart \u0026lt;- useMart(\u0026#39;ensembl\u0026#39;, dataset=\u0026#39;hsapiens_gene_ensembl\u0026#39;) Last, we extract the gene id to which our SNP belongs using the function getBM(). Along with that, we also extract other information, like the minor allele frequency minor_allele_freq. To check which attributes and filters are available, run listAttributes(snp.ensembl) and listFilters(snp.ensembl).\nout.bm \u0026lt;- getBM( attributes = c(\u0026quot;ensembl_gene_stable_id\u0026quot;, \u0026quot;refsnp_id\u0026quot;, \u0026quot;chr_name\u0026quot;, \u0026quot;chrom_start\u0026quot;, \u0026quot;chrom_end\u0026quot;, \u0026quot;minor_allele\u0026quot;, \u0026quot;minor_allele_freq\u0026quot;), # \u0026quot;ensembl_transcript_stable_id\u0026quot;, # \u0026quot;consequence_type_tv\u0026quot;), filters = \u0026quot;snp_filter\u0026quot;, values = \u0026quot;rs1421085\u0026quot;,#dat.bmi.sel$SNP, mart = snp.ensembl) This gives us - as chosen in attributes - the gene identifier, the SNP identifier, the chromosome name, chromosomal position (start + end), minor allele and minor allele frequency. The output in out.bm corresponds to this webpage entry on the ensembl webpage.\nout.bm ## ensembl_gene_stable_id refsnp_id chr_name chrom_start chrom_end ## 1 ENSG00000140718 rs1421085 16 53767042 53767042 ## minor_allele minor_allele_freq ## 1 C 0.228634  Quick d-tour: assembly GRCh37 or GRCh38? Before getting the gene names, we want to check if the positions in the dataset are from assembly GRCh37 or GRCh38. This is a handy thing because often only SNP identifiers are reported. Or SNP identifiers are reported, but with positions on a different assembly.\nifelse(sel.pos == out.bm$chrom_start, \u0026quot;\\u2713: same assembley (GRCh38)\u0026quot;, \u0026quot;\\u2717: not the same assembley\u0026quot;) ## [1] \u0026quot;✗: not the same assembley\u0026quot; The position is not matching, because the databases that we are looking at is based on the most recent human assembly GRCh38, but the BMI summary statistics dataset is based on human assembly GRCh37. The command listEnsemblArchives() will list you the URLs needed to get access to an archived assembly. So let’s pull out the archived GRCh37 version with the argument host = 'http://grch37.ensembl.org'.\nsnp.ensembl.grch37 \u0026lt;- useMart(host=\u0026#39;http://grch37.ensembl.org\u0026#39;, biomart=\u0026#39;ENSEMBL_MART_SNP\u0026#39;, dataset=\u0026#39;hsapiens_snp\u0026#39;) out.bm.grch37 \u0026lt;- getBM( attributes = c(\u0026#39;ensembl_gene_stable_id\u0026#39;, \u0026#39;refsnp_id\u0026#39;, \u0026#39;chr_name\u0026#39;, \u0026#39;chrom_start\u0026#39;, \u0026#39;chrom_end\u0026#39;, \u0026#39;minor_allele\u0026#39;, \u0026#39;minor_allele_freq\u0026#39;), filters = \u0026#39;snp_filter\u0026#39;, values = dat.bmi.sel$SNP, mart = snp.ensembl.grch37 ) out.bm.grch37 ## ensembl_gene_stable_id refsnp_id chr_name chrom_start chrom_end ## 1 ENSG00000140718 rs1421085 16 53800954 53800954 ## minor_allele minor_allele_freq ## 1 C 0.228634 Let’s check again.\nifelse(sel.pos == out.bm.grch37$chrom_start, \u0026quot;\\u2713: same assembley (grch37)\u0026quot;, \u0026quot;\\u2717: not the same assembley\u0026quot;) ## [1] \u0026quot;✓: same assembley (grch37)\u0026quot; Flavia Hodel pointed out that adding the argument GRCh = 37 works too! That’s a handy argument (but limited to GRCh versions 37 and 38).\nsnp.ensembl.grch37.alt \u0026lt;- useEnsembl(biomart = \u0026quot;snp\u0026quot;, dataset = \u0026quot;hsapiens_snp\u0026quot;, GRCh = 37) out.bm.grch37 \u0026lt;- getBM( attributes = c(\u0026#39;ensembl_gene_stable_id\u0026#39;, \u0026#39;refsnp_id\u0026#39;, \u0026#39;chr_name\u0026#39;, \u0026#39;chrom_start\u0026#39;, \u0026#39;chrom_end\u0026#39;, \u0026#39;minor_allele\u0026#39;, \u0026#39;minor_allele_freq\u0026#39;), filters = \u0026#39;snp_filter\u0026#39;, values = dat.bmi.sel$SNP, mart = snp.ensembl.grch37.alt ) out.bm.grch37 ## ensembl_gene_stable_id refsnp_id chr_name chrom_start chrom_end ## 1 ENSG00000140718 rs1421085 16 53800954 53800954 ## minor_allele minor_allele_freq ## 1 C 0.228634 ifelse(sel.pos == out.bm.grch37$chrom_start, \u0026quot;\\u2713: same assembley (grch37)\u0026quot;, \u0026quot;\\u2717: not the same assembley\u0026quot;) ## [1] \u0026quot;✓: same assembley (grch37)\u0026quot;  Extracting gene name for one SNP Next, we want to get the gene name where rs1421085 falls into.\nLet’s check which attributes contain the string gene.\nlistAttributes(snp.ensembl) %\u0026gt;% slice(str_which(name, \u0026quot;gene\u0026quot;))  ## name description page ## 1 associated_gene Associated gene with phenotype snp ## 2 ensembl_gene_stable_id Gene stable ID snp Now use ensembl_gene_stable_id and associated_gene as additional attributes.\n## extract gene ## ---------- out.bm.snp2gene \u0026lt;- getBM( attributes = c(\u0026#39;refsnp_id\u0026#39;, \u0026#39;allele\u0026#39;, \u0026#39;chrom_start\u0026#39;, \u0026#39;chr_name\u0026#39;, \u0026#39;ensembl_gene_stable_id\u0026#39;), filters = c(\u0026#39;snp_filter\u0026#39;), values = dat.bmi.sel$SNP, mart = snp.ensembl) out.bm.snp2gene ## refsnp_id allele chrom_start chr_name ensembl_gene_stable_id ## 1 rs1421085 T/C 53767042 16 ENSG00000140718 ## Attribute `associated_gene` is `Associated gene with phenotype`. ## Extract string ## ---------- gene.ensembl \u0026lt;- useEnsembl(biomart = \u0026quot;ensembl\u0026quot;, dataset = \u0026quot;hsapiens_gene_ensembl\u0026quot;, GRCh = 37) # we will need an additional mart for genes ## because we are using positions from GRCh = 37 in a next query, we need to pass that information on. out.bm.gene \u0026lt;- getBM(attributes = c(\u0026#39;external_gene_name\u0026#39;), filters = c(\u0026#39;ensembl_gene_id\u0026#39;), values = unique(out.bm.snp2gene$ensembl_gene_stable_id), mart = gene.ensembl) out.bm.gene ## external_gene_name ## 1 FTO The gene that contains SNP rs1421085 is called FTO.\n Extracting gene names for genomic region So now we know that rs1421085 is part of FTO. But where does FTO start and end? And what are the genes nearby? For this purpose, we want to visualise the summary statistics of the full genomic region (\\(\\pm 250 \\cdot 10^3\\) Mb). We just recycle the previous code, but instead of providing a SNP-id, we provide the chromosome, the start and the end position.\nout.bm.genes.region \u0026lt;- getBM( attributes = c(\u0026#39;start_position\u0026#39;,\u0026#39;end_position\u0026#39;,\u0026#39;ensembl_gene_id\u0026#39;,\u0026#39;external_gene_name\u0026#39;, \u0026#39;gene_biotype\u0026#39;), filters = c(\u0026#39;chromosome_name\u0026#39;,\u0026#39;start\u0026#39;,\u0026#39;end\u0026#39;), values = list(sel.chr, sel.pos - range, sel.pos + range), mart = gene.ensembl) head(out.bm.genes.region) ## start_position end_position ensembl_gene_id external_gene_name ## 1 53088945 53363062 ENSG00000177200 CHD9 ## 2 53332136 53333704 ENSG00000261056 RP11-454F8.2 ## 3 53368474 53368576 ENSG00000238645 snoU13 ## 4 53371365 53371483 ENSG00000202193 RNA5SP427 ## 5 53395931 53397590 ENSG00000259962 RP11-44F14.4 ## 6 53398894 53406995 ENSG00000260078 RP11-44F14.1 ## gene_biotype ## 1 protein_coding ## 2 pseudogene ## 3 snoRNA ## 4 rRNA ## 5 pseudogene ## 6 pseudogene We can plot out.bm.genes.region with a line range plot, where each horizontal line represents one gene.\n## rank gene names according to start position out.bm.genes.region \u0026lt;- out.bm.genes.region %\u0026gt;% mutate(external_gene_name = fct_reorder(external_gene_name, start_position, .desc = TRUE)) ## plot ggplot(data = out.bm.genes.region) + geom_linerange(aes(x = external_gene_name, ymin = start_position, ymax = end_position)) + coord_flip() + ylab(\u0026quot;\u0026quot;) Let’s try to make that pretty. We can group the genes by gene_biotype and colour them accordingly. And we move the protein-coding genes to the top row and colour it black.\n## define plot range for x-axis plot.range \u0026lt;- c(min(sel.pos - range, out.bm.genes.region$start_position), max(sel.pos + range, out.bm.genes.region$end_position)) ## rank gene_biotype label out.bm.genes.region \u0026lt;- out.bm.genes.region %\u0026gt;% mutate(gene_biotype_fac = fct_relevel(as.factor(gene_biotype), \u0026quot;protein_coding\u0026quot;), external_gene_name = fct_reorder2(external_gene_name, start_position, gene_biotype_fac, .desc = TRUE)) ## plot p2 \u0026lt;- ggplot(data = out.bm.genes.region) + geom_linerange(aes(x = external_gene_name, ymin = start_position, ymax = end_position, colour = gene_biotype_fac, group = gene_biotype_fac)) + coord_flip() + ylab(\u0026quot;\u0026quot;) + ylim(plot.range) + geom_text(aes(x = external_gene_name, y = start_position, label = external_gene_name, colour = gene_biotype_fac), fontface = 2, alpha = I(0.7), hjust = \u0026quot;right\u0026quot;, size= 2.5) + labs(title = \u0026quot;\u0026quot;, subtitle = paste0(\u0026quot;Genes\u0026quot;), caption = paste0(\u0026quot;Data source: \u0026quot;, gene.ensembl@host, \u0026quot; + Data set: \u0026quot;, gene.ensembl@dataset), color = \u0026quot;Gene Biotype\u0026quot;) + theme(axis.title.y=element_blank(), axis.text.y=element_blank(), axis.ticks.y=element_blank(), strip.text.y = element_text(angle = 0), legend.position=\u0026quot;bottom\u0026quot;, panel.grid.major.y = element_blank()) + expand_limits(y=c(-1, 1)) + scale_color_manual(values = c(\u0026quot;black\u0026quot;, metafolio::gg_color_hue(nlevels(out.bm.genes.region$gene_biotype_fac)-1))) ## hack to have 11 colors, but probably merging some gene biotype groups could make sense too. ## consider colorblindr::palette_OkabeIto_black print(p2) Some short genes are starting with AC and LINCO. We can check what they are and/or consult a biologist.\n  3. Combining summary statistics and annotation Now we are ready to combine plot p1 and p2.\nlibrary(patchwork) p1b \u0026lt;- p1 + xlab(\u0026quot;\u0026quot;) + theme(axis.title.x=element_blank(), axis.text.x=element_blank(), axis.ticks.x=element_blank()) + xlim(plot.range) p1b + p2 + plot_layout(ncol = 1, heights = c(6, 6)) Having both plots combined, we can spot that there are six protein-coding genes in the region. But the P-value peak is located in gene FTO.\nIf this were work done for a real project, this would now be the time to get back to domain experts, discuss the results and figure out what other annotation they need8.\n Wouldn’t it be nice…  … to polish that plot even more? For starters, the colour code is not ideal… … to add information about the correlation between SNPs, like in LocusZoom plots? … or have an interactive plot9: hovering over the SNP points would light up the corresponding genes or give some other information, like the allele frequency.  Totally 😉 Seems like great material for future blog posts!\n Source  The R Markdown file is here. Some more biomaRt code snippets are in a gist.  If you know another R-package to solve a similar problem or have feedback, you can comment below 👇.\n  Thanks to Maëlle Salmon you get a real plot here instead of a blurry hand drawing 😉 She gave lots of feedback on my first version - much appreciated!↩\n Awesome icons in R?      checkout these instructions by Mitchell O’Hara-Wild on the rOpenSci webpage.↩\n Thanks to my colleagues with biology background for explaining me the differences between these databases and what a biomart is!↩\n Initially, I wanted to use data from two recent studies by the Psychiatric Genomics Consortium (PGC) on schizophrenia (SCZ) and bipoloar disorder (BD), as well as Major depressive disorder (MDD). Like most consortia, PGC provides summary statistics that can be downloaded. However, before downloading anything, the user needs to acknowledge and agree to a list of conditions - which I think is an excellent approach! - therefore we cannot directly load it into R.↩\n Data source: Yengo et al. (2018).↩\n A guide to crafting Manhattan plots by Yan Holtz for the R graph gallery↩\n I recently listened to a podcast by Saskia Freytag and NJ Tierney where they talk about the differences between CRAN and Bioconductor (there are many!). The podcast is called Credibly Curious.↩\n +1 for reproducibility!↩\n Liza Darrous pointed out the interactive Manhattan R function manhanttanly.↩\n   ","date":1532908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1532908800,"objectID":"878cf620f59faea12ff979b4a1095038","permalink":"https://sinarueeger.github.io/post/locuszoomplot/","publishdate":"2018-07-30T00:00:00Z","relpermalink":"/post/locuszoomplot/","section":"post","summary":"Goal Getting it done 1. Get summary statistics Visualising associations Identify genomic region with lowest P-value  2. Extracting annotation biomaRt Learning resources Using biomaRt Quick d-tour: assembly GRCh37 or GRCh38? Extracting gene name for one SNP Extracting gene names for genomic region  3. Combining summary statistics and annotation Wouldn’t it be nice… Source   In the world of genome-wide association studies (GWAS), we often get a list of genetic markers (SNPs) that seem for some reason relevant for a particular outcome.","tags":["statistical genetics","R","data visualisation"],"title":"Locuszoom plot of GWAS summary statistics","type":"post"},{"authors":null,"categories":null,"content":" This is a brief write-up of my satRdays Cardiff experience.\nFirst - what is a satRday?\nIt is an awesome concept: attending an R conference organised by a local RUG on a Saturday.\nThe programme in Cardiff had parallel sessions - tough decision-making to pick between promising talks!\ndplyr workshop Kathrine Tansey kept us busy with a workshop on dplyr in the morning.\n💻 rstudio-cloud project\nPackaging workshop Heather Turner upgraded us on packaging.\n💡 useful tips and workflow! Eager to apply what I learned.\n💻 rstudio-cloud project\n➡️ Slides\nIntroduction to tidytext Textmining with Nujcharee (เป็ด), applied to #metoo twitter data:\nMy slide on Introduction to Tidytext https://t.co/XOiEiedNjN @satRdays_org #rstats\n\u0026mdash; Nujcharee (เป็ด) (@Nujcharee) June 23, 2018 Package reviews with rOpenSci Maëlle Salmon presented the review process of rOpenSci and then reviewed the review process using the force of textmining.\n💡 Rigorous, but friendly reviewing process at rOpenSci.\nMore info on submitting a package for review here.\n➡️ Slides\nAirtable \u0026amp; R Amy McDougall presented airtable (💡!) and the R interface airtabler.\nAmy wrote a blogpost on this topic too (which I found through Locke Data\u0026rsquo;s write up here).\n➡️ Slides\nLightening talks Counting and weighing Penguins Philipp Boersch-Supan telling us about the challenges of counting penguins (and apparently Rcpp helps).\nIntegrating command-line tools with R Erle Holgersen: embrace the system() function for command-line snippets in R.\nFor example:\nls.directory \u0026lt;- system(\u0026quot;ls -all\u0026quot;, intern = TRUE)  💡 intern = TRUE is my new friend!\nOther tips: - use the command line functionality in data.table::fread(\u0026quot;\u0026quot;) - make use of tempfile()\ngit in five Steph Locke gave git in a nutshell.\n💡 Recommendation: use GitKraken as a GUI client.\ntidy eval Nic Crane explaining the basics of tidy eval and when it is needed (💡 functions!).\nA simple Bayesian workflow Paul Robinson presented his Bioconductor package dealing with proteins.\n➡️ Slides\nR-Forwards and R-Ladies Remote Heather Turner presented two #rstats related initiatives.\nR Forwards is an initiative to widen the participation of under-represented groups ➡️ looking for volunteers that take on tasks.\nR-Ladies remote chapter: has monthly coffee breaks on slack!\nFinal remarks 👏 Kudos to the organisers! They paid lots of attention to details and did an awesome jexcellentaking everyone feel welcome!\n✍️ Mental note to self: I should probably learn how to take pics at the right time + live tweet 😉\nFurther information 📁 Slides repo.\n📘 Check out Maëlle\u0026rsquo;s blogpost on Sto rrrify #satRdayCDF 2018.\n📘 Loads of info from the Locke Data team members.\n#satRdayCDF on Twitter.\n⏭️ Next satRday is on September 1 2018 in Amsterdam.\n📅 Check all upcoming events.\nAlso 👇\nIf you would like a satRday near you, why not run one?\nHere\u0026#39;s what you need to know to decide if it\u0026#39;s right for you https://t.co/Szy7AMB3rF\n\u0026mdash; satRdays (@satRdays_org) June 26, 2018 ","date":1530662400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530662400,"objectID":"7af74600ae0107467cf199df6c223a76","permalink":"https://sinarueeger.github.io/post/satrdaycardiff/","publishdate":"2018-07-04T00:00:00Z","relpermalink":"/post/satrdaycardiff/","section":"post","summary":"This is a brief write-up of my satRdays Cardiff experience.\nFirst - what is a satRday?\nIt is an awesome concept: attending an R conference organised by a local RUG on a Saturday.\nThe programme in Cardiff had parallel sessions - tough decision-making to pick between promising talks!\ndplyr workshop Kathrine Tansey kept us busy with a workshop on dplyr in the morning.\n💻 rstudio-cloud project\nPackaging workshop Heather Turner upgraded us on packaging.","tags":["conference","R","satRday","rOpenSci","package","dplyr"],"title":"satRday Cardiff 2018","type":"post"},{"authors":null,"categories":null,"content":"I work (broadly speaking) in epidemiology. Within collaborations we often have to share sensitive data across institutions and are therefore likely to not share IT facilities. But the most often used options - bare e-mail or file hosting - are not secure, as they both work via a server that could potentially be exposed.\nThere are a handful of secure options around (ProtonMail or keybase.io, both working via encryption), but for those that trust open source projects the most and are familiar with a terminal I have written some basic instructions for asymmetric GPG encryption.\nI should say that I am not an encryption expert at all. But, although there is a lot of talk about data protection, I never came across a compact, easy-to-follow instruction for the sender of the document and the recipient, valid for all three operating systems (Linux, Mac, Windows). Therefore, I tried to write one, mainly for myself and collaborators. Feedback is appreciated!\n ","date":1523644859,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1523644859,"objectID":"dbca7dfcf69c05fd04ce13b346a6aeb2","permalink":"https://sinarueeger.github.io/post/2018-04-04-encryption-of-files/","publishdate":"2018-04-13T20:40:59+02:00","relpermalink":"/post/2018-04-04-encryption-of-files/","section":"post","summary":"I work (broadly speaking) in epidemiology. Within collaborations we often have to share sensitive data across institutions and are therefore likely to not share IT facilities. But the most often used options - bare e-mail or file hosting - are not secure, as they both work via a server that could potentially be exposed.\nThere are a handful of secure options around (ProtonMail or keybase.io, both working via encryption), but for those that trust open source projects the most and are familiar with a terminal I have written some basic instructions for asymmetric GPG encryption.","tags":["encryption","best practices","workflow"],"title":"Getting started with encryption of documents","type":"post"},{"authors":["**R\u0026uuml;eger, S**","Bochud, P-Y","Dufour, J-F","M\u0026uuml;llhaupt, B","Semela, D","Heim, M H","Moradpour, D","Cerny, A","Malinverni, R","Booth, D R","Suppiah, V","George, J","Argiro, L","Halfon, P","Bourli\u0026egrave;re, M","Talal, A H","Jacobson, I M","Patin, E","Nalpas, B","Poynard, T","Pol, S","Abel, L","Kutalik, Z","Negro, F"],"categories":null,"content":"","date":1441054800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441054800,"objectID":"8089d020d4b95ead47a18ea77aaa9b9a","permalink":"https://sinarueeger.github.io/publication/gut-2015-fibrosis/","publishdate":"2015-09-01T00:00:00+03:00","relpermalink":"/publication/gut-2015-fibrosis/","section":"publication","summary":"","tags":[],"title":"Impact of common risk factors of fibrosis progression in chronic hepatitis C","type":"publication"}]